---
title: Cilium 5ì£¼ì°¨ ì •ë¦¬
date: 2025-08-16 20:30:00 +0900
categories: [Cilium]
tags: [Cilium]
---

## **ğŸ”§ ì‹¤ìŠµ í™˜ê²½ êµ¬ì„±**

### **1. VirtualBox í˜¸í™˜ì„± ì´ìŠˆ**

```bash
curl -O https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/Vagrantfile
vagrant up --provider=virtualbox
```

**ğŸ“¢ ì˜¤ë¥˜ ë°œìƒ**

```bash
vagrant up --provider=virtualbox
The provider 'virtualbox' that was requested to back the machine
'k8s-ctr' is reporting that it isn't usable on this system. The
reason is shown below:
Vagrant has detected that you have a version of VirtualBox installed
that is not supported by this version of Vagrant. Please install one of
the supported versions listed below to use Vagrant:
4.0, 4.1, 4.2, 4.3, 5.0, 5.1, 5.2, 6.0, 6.1, 7.0, 7.1
A Vagrant update may also be available that adds support for the version
you specified. Please check www.vagrantup.com/downloads.html to download
the latest version.
```

- Vagrantê°€ ì§€ì›í•˜ëŠ” VirtualBoxëŠ” ìµœëŒ€ `7.1` ë²„ì „
- Arch Linux ë¡¤ë§ ì—…ë°ì´íŠ¸ë¡œ VirtualBox `7.2`ê°€ ì„¤ì¹˜ë˜ì–´ í˜¸í™˜ ë¶ˆê°€

### **2. í•´ê²°ê³¼ì •: libvirt ì „í™˜**

**(1) libvirt í™˜ê²½ ì„¤ì •**

```bash
sudo systemctl enable --now libvirtd
sudo usermod -a -G libvirt $USER
newgrp libvirt
```

**(2) libvirt í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜**

```bash
vagrant plugin install vagrant-libvirt
```

âœ…Â **ì¶œë ¥**

```bash
Installing the 'vagrant-libvirt' plugin. This can take a few minutes...
Fetching xml-simple-1.1.9.gem
Fetching racc-1.8.1.gem
Building native extensions. This could take a while...
Fetching nokogiri-1.18.9-x86_64-linux-gnu.gem
Fetching ruby-libvirt-0.8.4.gem
Building native extensions. This could take a while...
Fetching formatador-1.2.0.gem
Fetching fog-core-2.6.0.gem
Fetching fog-xml-0.1.5.gem
Fetching fog-json-1.2.0.gem
Fetching fog-libvirt-0.13.2.gem
Fetching diffy-3.4.4.gem
Fetching vagrant-libvirt-0.12.2.gem
Installed the plugin 'vagrant-libvirt (0.12.2)'!
```

**(3) ë„¤íŠ¸ì›Œí¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜**

```bash
sudo pacman -S dnsmasq bridge-utils iptables-nft
sudo systemctl restart libvirtd
```

**(4) ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ í™œì„±í™”**

```bash
sudo virsh net-start default
sudo virsh net-autostart default
sudo virsh net-list --all
```

âœ…Â **ì¶œë ¥**

```bash
Network default started
Network default marked as autostarted

Name      State    Autostart   Persistent
--------------------------------------------
default   active   yes         yes
```

**(5) Vagrantfile ìˆ˜ì •**

```bash
# Variables
K8SV = '1.33.2-1.1' # Kubernetes Version
CONTAINERDV = '1.7.27-1' # Containerd Version
CILIUMV = '1.18.0' # Cilium CNI Version
N = 1 # max number of worker nodes

# Base Image
BOX_IMAGE = "bento/ubuntu-24.04"
BOX_VERSION = "202508.03.0"

Vagrant.configure("2") do |config|
  #-ControlPlane Node
  config.vm.define "k8s-ctr" do |subconfig|
    subconfig.vm.box = BOX_IMAGE
    subconfig.vm.box_version = BOX_VERSION
    
    subconfig.vm.provider "libvirt" do |libvirt|
      libvirt.cpus = 2
      libvirt.memory = 2560
    end
    
    subconfig.vm.host_name = "k8s-ctr"
    subconfig.vm.network "private_network", ip: "192.168.10.100"
    subconfig.vm.network "forwarded_port", guest: 22, host: 60000, auto_correct: true, id: "ssh"
    subconfig.vm.synced_folder "./", "/vagrant", disabled: true
    subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/init_cfg.sh", args: [ K8SV, CONTAINERDV ]
    subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/k8s-ctr.sh", args: [ N, CILIUMV, K8SV ]
    subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/route-add1.sh"
  end

  #-Worker Nodes Subnet1
  (1..N).each do |i|
    config.vm.define "k8s-w#{i}" do |subconfig|
      subconfig.vm.box = BOX_IMAGE
      subconfig.vm.box_version = BOX_VERSION
      
      subconfig.vm.provider "libvirt" do |libvirt|
        libvirt.cpus = 2
        libvirt.memory = 1536
      end
      
      subconfig.vm.host_name = "k8s-w#{i}"
      subconfig.vm.network "private_network", ip: "192.168.10.10#{i}"
      subconfig.vm.network "forwarded_port", guest: 22, host: "6000#{i}", auto_correct: true, id: "ssh"
      subconfig.vm.synced_folder "./", "/vagrant", disabled: true
      subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/init_cfg.sh", args: [ K8SV, CONTAINERDV]
      subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/k8s-w.sh"
      subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/route-add1.sh"
    end
  end

  #-Router Node
  config.vm.define "router" do |subconfig|
    subconfig.vm.box = BOX_IMAGE
    subconfig.vm.box_version = BOX_VERSION
    
    subconfig.vm.provider "libvirt" do |libvirt|
      libvirt.cpus = 1
      libvirt.memory = 768
    end
    
    subconfig.vm.host_name = "router"
    subconfig.vm.network "private_network", ip: "192.168.10.200"
    subconfig.vm.network "forwarded_port", guest: 22, host: 60009, auto_correct: true, id: "ssh"
    subconfig.vm.network "private_network", ip: "192.168.20.200", auto_config: false
    subconfig.vm.synced_folder "./", "/vagrant", disabled: true
    subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/router.sh"
  end

  #-Worker Nodes Subnet2
  config.vm.define "k8s-w0" do |subconfig|
    subconfig.vm.box = BOX_IMAGE
    subconfig.vm.box_version = BOX_VERSION
    
    subconfig.vm.provider "libvirt" do |libvirt|
      libvirt.cpus = 2
      libvirt.memory = 1536
    end
    
    subconfig.vm.host_name = "k8s-w0"
    subconfig.vm.network "private_network", ip: "192.168.20.100"
    subconfig.vm.network "forwarded_port", guest: 22, host: 60010, auto_correct: true, id: "ssh"
    subconfig.vm.synced_folder "./", "/vagrant", disabled: true
    subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/init_cfg.sh", args: [ K8SV, CONTAINERDV]
    subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/k8s-w.sh"
    subconfig.vm.provision "shell", path: "https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/5w/route-add2.sh"
  end
end
```

- VirtualBox ê´€ë ¨ ì„¤ì • ì œê±° (`vb.customize`, `vb.name`, `vb.linked_clone`)
- providerë¥¼ `libvirt`ë¡œ ë³€ê²½

**(6) í´ëŸ¬ìŠ¤í„° ì‹¤í–‰**

```bash
vagrant up --provider=libvirt
```

âœ…Â **ì¶œë ¥**

```bash
...
==> k8s-ctr: Running provisioner: shell...
    k8s-ctr: Running: /tmp/vagrant-shell20250815-120434-uhtz9s.sh
    k8s-ctr: >>>> K8S Controlplane config Start <<<<
    k8s-ctr: [TASK 1] Initial Kubernetes
    k8s-w1: >>>> Initial Config End <<<<
==> k8s-w1: Running provisioner: shell...
    k8s-w1: Running: /tmp/vagrant-shell20250815-120434-susr2r.sh
    k8s-w1: >>>> K8S Node config Start <<<<
    k8s-w1: [TASK 1] K8S Controlplane Join
    k8s-w0: >>>> Initial Config End <<<<
==> k8s-w0: Running provisioner: shell...
    k8s-w0: Running: /tmp/vagrant-shell20250815-120434-gmyjn3.sh
    k8s-w0: >>>> K8S Node config Start <<<<
    k8s-w0: [TASK 1] K8S Controlplane Join
    k8s-ctr: [TASK 2] Setting kube config file
    k8s-ctr: [TASK 3] Source the completion
    k8s-ctr: [TASK 4] Alias kubectl to k
    k8s-ctr: [TASK 5] Install Kubectx & Kubens
    k8s-ctr: [TASK 6] Install Kubeps & Setting PS1
    k8s-ctr: [TASK 7] Install Cilium CNI
    k8s-ctr: [TASK 8] Install Cilium / Hubble CLI
    k8s-ctr: cilium
    k8s-w1: >>>> K8S Node config End <<<<
==> k8s-w1: Running provisioner: shell...
    k8s-w1: Running: /tmp/vagrant-shell20250815-120434-45fmm8.sh
    k8s-w1: >>>> Route Add Config Start <<<<
    k8s-w1: >>>> Route Add Config End <<<<
    k8s-ctr: hubble
    k8s-ctr: [TASK 9] Remove node taint
    k8s-ctr: node/k8s-ctr untainted
    k8s-ctr: [TASK 10] local DNS with hosts file
    k8s-ctr: [TASK 11] Dynamically provisioning persistent local storage with Kubernetes
    k8s-ctr: [TASK 13] Install Metrics-server
    k8s-ctr: [TASK 14] Install k9s
    k8s-ctr: >>>> K8S Controlplane Config End <<<<
==> k8s-ctr: Running provisioner: shell...
    k8s-ctr: Running: /tmp/vagrant-shell20250815-120434-5mw3lc.sh
    k8s-ctr: >>>> Route Add Config Start <<<<
    k8s-ctr: >>>> Route Add Config End <<<<
    k8s-w0: >>>> K8S Node config End <<<<
==> k8s-w0: Running provisioner: shell...
    k8s-w0: Running: /tmp/vagrant-shell20250815-120434-dtpox6.sh
    k8s-w0: >>>> Route Add Config Start <<<<
    k8s-w0: >>>> Route Add Config End <<<<
```

### **3. ë¼ìš°íŒ… ë° BGP ì‹¤ìŠµ**
![](https://velog.velcdn.com/images/tlsalswls123/post/5aabdc21-1d14-456a-9442-8b06a7320e69/image.png)
- [https://docs.frrouting.org/en/stable-10.4/about.html](https://docs.frrouting.org/en/stable-10.4/about.html)
- FRR(FRRouting) ì„¤ì¹˜ í›„ BGP ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ì‹¤ìŠµ êµ¬ì„±
- `bgpControlPlane.enabled=true` ì„¤ì •
- `autoDirectNodeRoutes=false` â†’ ê°™ì€ ë„¤íŠ¸ì›Œí¬ ë…¸ë“œì—ì„œ ë‹¤ë¥¸ ë…¸ë“œì˜ pod CIDR ì¶”ê°€í•˜ëŠ” ê²ƒ ë¹„í™œì„±í™”

```bash
cat <<EOT>> /etc/netplan/50-vagrant.yaml
      routes:
      - to: 192.168.20.0/24
        via: 192.168.10.200
      # - to: 172.20.0.0/16
      #   via: 192.168.10.200
EOT
```

```bash
cat <<EOT>> /etc/netplan/50-vagrant.yaml
      routes:
      - to: 192.168.10.0/24
        via: 192.168.20.200
      # - to: 172.20.0.0/16
      #   via: 192.168.20.200
EOT
```

- ë‚´ë¶€ë§ ìµœì†Œ ë¼ìš°íŒ… ê·œì¹™ë§Œ ì¶”ê°€
- BGPë¥¼ í†µí•´ ë¼ìš°íŒ… ì •ë³´ êµí™˜ ë° ê´‘ê³  ì„¤ì •

```bash
echo "[TASK 7] Configure FRR"
apt install frr -y >/dev/null 2>&1
sed -i "s/^bgpd=no/bgpd=yes/g" /etc/frr/daemons

NODEIP=$(ip -4 addr show eth1 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
cat << EOF >> /etc/frr/frr.conf
!
router bgp 65000
  bgp router-id $NODEIP
  bgp graceful-restart
  no bgp ebgp-requires-policy
  bgp bestpath as-path multipath-relax
  maximum-paths 4
  network 10.10.1.0/24
EOF

systemctl daemon-reexec >/dev/null 2>&1
systemctl restart frr >/dev/null 2>&1
systemctl enable frr >/dev/null 2>&1
```

- ë¼ìš°í„° VMì—ë„ FRR ì„¤ì¹˜ ë° `bgpd=yes` ì ìš© í›„ BGP ë¼ìš°í„°ë¡œ ë™ì‘

---

## **ğŸ–¥ï¸ [k8s-ctr] ì ‘ì† í›„ ê¸°ë³¸ ì •ë³´ í™•ì¸**

### **1. ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ì—ì„œ ê° ë…¸ë“œ(`k8s-w0`, `k8s-w1`, `router`)ì— SSH ì ‘ê·¼ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# for i in k8s-w0 k8s-w1 router ; do echo ">> node : $i <<"; sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@$i hostname; echo; done
```

âœ…Â **ì¶œë ¥**

```bash

>> node : k8s-w0 <<
Warning: Permanently added 'k8s-w0' (ED25519) to the list of known hosts.
k8s-w0

>> node : k8s-w1 <<
Warning: Permanently added 'k8s-w1' (ED25519) to the list of known hosts.
k8s-w1

>> node : router <<
Warning: Permanently added 'router' (ED25519) to the list of known hosts.
router
```

### **2. ë…¸ë“œ Join ë¬¸ì œ í™•ì¸**

**(1) ë¬¸ì œ ìƒí™©**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get node -owide
```

âœ…Â **ì¶œë ¥**

```bash
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-ctr   Ready    control-plane   4m25s   v1.33.2   192.168.10.100   <none>        Ubuntu 24.04.2 LTS   6.8.0-64-generic   containerd://1.7.27
k8s-w1    Ready    <none>          4m11s   v1.33.2   192.168.10.101   <none>        Ubuntu 24.04.2 LTS   6.8.0-64-generic   containerd://1.7.27
```

- `kubectl get node` ê²°ê³¼ì—ì„œ `k8s-w0` ë…¸ë“œê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ

**(2) ì›ì¸ íŒŒì•…**

```bash
vagrant ssh k8s-w0

root@k8s-w0:~# cat /root/kubeadm-join-worker-config.yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: JoinConfiguration
discovery:
  bootstrapToken:
    token: "123456.1234567890123456"
    apiServerEndpoint: "192.168.10.100:6443"
    unsafeSkipCAVerification: true
nodeRegistration:
  criSocket: "unix:///run/containerd/containerd.sock"
  kubeletExtraArgs:
    - name: node-ip
      value: "192.168.20.100"
```

- ì„¤ì •íŒŒì¼ì— ë”ë¯¸í† í°ì´ í•˜ë“œì½”ë”© ë˜ì–´ìˆì–´ì„œ join ì‹¤íŒ¨

**(3) í•´ê²° ê³¼ì •**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubeadm token create --ttl=72h
9llwjd.azxcrk0wd8lkrh45
```

- ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ì—ì„œ ìƒˆ í† í° ìƒì„±

```bash
root@k8s-w0:~# sudo sed -i 's/123456.1234567890123456/9llwjd.azxcrk0wd8lkrh45/g' /root/kubeadm-join-worker-config.yaml
root@k8s-w0:~# cat /root/kubeadm-join-worker-config.yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: JoinConfiguration
discovery:
  bootstrapToken:
    token: "9llwjd.azxcrk0wd8lkrh45"
    apiServerEndpoint: "192.168.10.100:6443"
    unsafeSkipCAVerification: true
nodeRegistration:
  criSocket: "unix:///run/containerd/containerd.sock"
  kubeletExtraArgs:
    - name: node-ip
      value: "192.168.20.100"
```

- ì›Œì»¤ë…¸ë“œ0 ì„¤ì • íŒŒì¼ ë‚´ í† í° ê°’ êµì²´

```bash
root@k8s-w0:~# sudo kubeadm reset -f
root@k8s-w0:~# sudo kubeadm join --config="/root/kubeadm-join-worker-config.yaml"
```

âœ…Â **ì¶œë ¥**

```bash
[preflight] Running pre-flight checks
W0815 22:41:14.831416    4224 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Deleted contents of the etcd data directory: /var/lib/etcd
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

The reset process does not perform cleanup of CNI plugin configuration,
network filtering rules and kubeconfig files.

For information on how to perform this cleanup manually, please see:
    https://k8s.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/
    
[preflight] Running pre-flight checks
[preflight] Reading configuration from the "kubeadm-config" ConfigMap in namespace "kube-system"...
[preflight] Use 'kubeadm init phase upload-config --config your-config-file' to re-upload it.
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.002299091s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.    
```

- `kubeadm reset -f` í›„ `kubeadm join` ì¬ì‹¤í–‰

**(4) ê²°ê³¼**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get nodes -owide
```

âœ…Â **ì¶œë ¥**

```bash
NAME      STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-ctr   Ready    control-plane   10m   v1.33.2   192.168.10.100   <none>        Ubuntu 24.04.2 LTS   6.8.0-64-generic   containerd://1.7.27
k8s-w0    Ready    <none>          53s   v1.33.2   192.168.20.100   <none>        Ubuntu 24.04.2 LTS   6.8.0-64-generic   containerd://1.7.27
k8s-w1    Ready    <none>          10m   v1.33.2   192.168.10.101   <none>        Ubuntu 24.04.2 LTS   6.8.0-64-generic   containerd://1.7.27
```

- ëª¨ë“  ë…¸ë“œ(`k8s-ctr`, `k8s-w0`, `k8s-w1`) ì •ìƒ ì—°ê²°

---

## **âš™ï¸ [k8s-ctr] cilium ì„¤ì • í™•ì¸**

**Ciliumì—ì„œ BGP Control Plane ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium config view | grep -i bgp

```

âœ…Â **ì¶œë ¥**

```bash
bgp-router-id-allocation-ip-pool                  
bgp-router-id-allocation-mode                     default
bgp-secrets-namespace                             kube-system
enable-bgp-control-plane                          true
enable-bgp-control-plane-status-report            true
```

- `enable-bgp-control-plane = true` ìƒíƒœ í™•ì¸ ì™„ë£Œ

## **ğŸŒ ë„¤íŠ¸ì›Œí¬ ì •ë³´ í™•ì¸**

### **1. router ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router ip -br -c -4 addr
```

âœ…Â **ì¶œë ¥**

```bash
lo               UNKNOWN        127.0.0.1/8 
eth0             UP             192.168.121.180/24 metric 100 
eth1             UP             192.168.10.200/24 
eth2             UP             192.168.20.200/24 
loop1            UNKNOWN        10.10.1.200/24 
loop2            UNKNOWN        10.10.2.200/24
```

### **2. ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# ip -c -4 addr show dev eth1
```

âœ…Â **ì¶œë ¥**

```bash
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    altname enp0s6
    altname ens6
    inet 192.168.10.100/24 brd 192.168.10.255 scope global eth1
       valid_lft forever preferred_lft forever
```

### **3. ì›Œì»¤ë…¸ë“œ ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# for i in w1 w0 ; do echo ">> node : k8s-$i <<"; sshpass -p 'vagrant' ssh vagrant@k8s-$i ip -c -4 addr show dev eth1; echo; done
```

âœ…Â **ì¶œë ¥**

```bash
>> node : k8s-w1 <<
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    altname enp0s6
    altname ens6
    inet 192.168.10.101/24 brd 192.168.10.255 scope global eth1
       valid_lft forever preferred_lft forever

>> node : k8s-w0 <<
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    altname enp0s6
    altname ens6
    inet 192.168.20.100/24 brd 192.168.20.255 scope global eth1
       valid_lft forever preferred_lft forever
```

- `k8s-w1`: `192.168.10.101/24` â†’ ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ê³¼ ê°™ì€ ëŒ€ì—­
- `k8s-w0`: `192.168.20.100/24`

### **4. router ë¼ìš°íŒ… ì •ë³´ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router ip -c route
```

âœ…Â **ì¶œë ¥**

```bash
default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.25 metric 100 
10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200 
192.168.20.0/24 dev eth2 proto kernel scope link src 192.168.20.200 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.25 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.25 metric 100 
```

- `192.168.10.0/24` ì™€ `192.168.20.0/24` ë¼ìš°íŒ… ì²˜ë¦¬

### **5. ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ë¼ìš°íŒ… ì •ë³´ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# ip -c route
```

âœ…Â **ì¶œë ¥**

```bash
default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.70 metric 100 
172.20.0.0/24 via 172.20.0.230 dev cilium_host proto kernel src 172.20.0.230 
172.20.0.230 dev cilium_host proto kernel scope link 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.100 
192.168.20.0/24 via 192.168.10.200 dev eth1 proto static 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.70 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.70 metric 100 
```

- `autoDirectNodeRoutes=false` ì„¤ì •ìœ¼ë¡œ ì¸í•´ Pod CIDR ìë™ ê²½ë¡œ ë“±ë¡ì´ ë¹„í™œì„±í™”ë¨
- ë”°ë¼ì„œ ê°™ì€ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­ì— ìˆë”ë¼ë„ ìƒëŒ€ë°© Pod CIDRê°€ ë¼ìš°íŒ… í…Œì´ë¸”ì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ
- ì»¨íŠ¸ë¡¤ í”Œë ˆì¸(`k8s-ctr`)ì€ ìì‹ ì˜ PodCIDR(`172.20.0.0/24`)ë§Œ ê°€ì§€ê³  ìˆìŒ

### **6. ì›Œì»¤ë…¸ë“œ ë¼ìš°íŒ… ì •ë³´ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# for i in w1 w0 ; do echo ">> node : k8s-$i <<"; sshpass -p 'vagrant' ssh vagrant@k8s-$i ip -c route; echo; done
```

âœ…Â **ì¶œë ¥**

```bash
>> node : k8s-w1 <<
default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.62 metric 100 
172.20.1.0/24 via 172.20.1.4 dev cilium_host proto kernel src 172.20.1.4 
172.20.1.4 dev cilium_host proto kernel scope link 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.101 
192.168.20.0/24 via 192.168.10.200 dev eth1 proto static 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.62 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.62 metric 100 

>> node : k8s-w0 <<
default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.122 metric 100 
172.20.2.0/24 via 172.20.2.89 dev cilium_host proto kernel src 172.20.2.89 
172.20.2.89 dev cilium_host proto kernel scope link 
192.168.10.0/24 via 192.168.20.200 dev eth1 proto static 
192.168.20.0/24 dev eth1 proto kernel scope link src 192.168.20.100 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.122 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.122 metric 100 
```

- ê° ì›Œì»¤ ë…¸ë“œ(`k8s-w1`, `k8s-w0`)ë„ ìì‹ ì˜ PodCIDRë§Œ ë“±ë¡ë˜ì–´ ìˆìŒ
- `autoDirectNodeRoutes=false` ë•Œë¬¸ì— ë‹¤ë¥¸ ë…¸ë“œì˜ Pod CIDRì€ ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ì§€ ì•ŠìŒ

---

## **ğŸ“¦ ìƒ˜í”Œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ë° í†µì‹  ë¬¸ì œ í™•ì¸**

### **1. ìƒ˜í”Œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF

# ê²°ê³¼
deployment.apps/webpod created
service/webpod created
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# ê²°ê³¼
pod/curl-pod created
```

### **2. Pod ìŠ¤ì¼€ì¤„ë§ ë° ì„œë¹„ìŠ¤ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get deploy,svc,ep webpod -owide
```

âœ…Â **ì¶œë ¥**

```bash
Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES           SELECTOR
deployment.apps/webpod   3/3     3            3           2m23s   webpod       traefik/whoami   app=webpod

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/webpod   ClusterIP   10.96.54.159   <none>        80/TCP    2m23s   app=webpod

NAME               ENDPOINTS                                        AGE
endpoints/webpod   172.20.0.158:80,172.20.1.65:80,172.20.2.204:80   2m23s
```

- `podAntiAffinity` ì„¤ì •ìœ¼ë¡œ ì¸í•´ íŒŒë“œê°€ ë…¸ë“œë³„ë¡œ ë¶„ì‚° ë°°ì¹˜ë¨
- Service: ClusterIP `10.96.54.159` í• ë‹¹
- Endpoints: 3ê°œ íŒŒë“œ ê°ê° ë‹¤ë¥¸ PodCIDR(`172.20.0.x`, `172.20.1.x`, `172.20.2.x`)ì— ë¶„ì‚°

### **3. Cilium Endpoint í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ciliumendpoints
```

âœ…Â **ì¶œë ¥**

```bash
NAME                      SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6
curl-pod                  64126               ready            172.20.0.64    
webpod-697b545f57-fbtbj   38082               ready            172.20.0.158   
webpod-697b545f57-pxhvr   38082               ready            172.20.1.65    
webpod-697b545f57-rpblf   38082               ready            172.20.2.204 
```

- `curl-pod` â†’ `172.20.0.64` (ì»¨íŠ¸ë¡¤ í”Œë ˆì¸)
- `webpod` 3ê°œ íŒŒë“œ â†’ `172.20.0.158`, `172.20.1.65`, `172.20.2.204`

### **4. í†µì‹  í…ŒìŠ¤íŠ¸ ì§„í–‰**

`curl-pod`ì—ì„œ `webpod` ì„œë¹„ìŠ¤ë¡œ ë°˜ë³µ ìš”ì²­ ìˆ˜í–‰

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl exec -it curl-pod -- sh -c 'while true; do curl -s --connect-timeout 1 webpod | grep Hostname; echo "---" ; sleep 1; done'
```

âœ…Â **ì¶œë ¥**

```bash
---
---
---
---
---
Hostname: webpod-697b545f57-fbtbj
---
---
---
Hostname: webpod-697b545f57-fbtbj
---
---
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-fbtbj
---
---
...
```

- `--connect-timeout 1` ì˜µì…˜ì„ ì¤˜ì„œ 1ì´ˆ ì´ë‚´ ì‘ë‹µ ì—†ìœ¼ë©´ ì—°ê²° ì¢…ë£Œ
- `k8s-ctr`ì— ë°°í¬ëœ `webpod` íŒŒë“œ(`172.20.0.158`)ë§Œ ì‘ë‹µ
- ë‹¤ë¥¸ ë…¸ë“œ(`k8s-w1`, `k8s-w0`)ì— ìœ„ì¹˜í•œ `webpod` íŒŒë“œë“¤ì€ ì‘ë‹µí•˜ì§€ ëª»í•¨

---

## **ğŸ“¡ Cilium BGP Control Plane**

- [https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-v2/](https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-v2/)
- **BGPÂ ì„¤ì •** í›„ í†µì‹  í™•ì¸ : Ciliumì˜ BGPëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì™¸ë¶€ ê²½ë¡œë¥¼ ì»¤ë„ ë¼ìš°íŒ… í…Œì´ë¸”ì— ì£¼ì…í•˜ì§€ ì•ŠìŒ
![](https://velog.velcdn.com/images/tlsalswls123/post/4c195f04-e63f-4966-9df5-a04d0dd8f7a3/image.png)

### **1. FRR í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router
root@router:~# ss -tnlp | grep -iE 'zebra|bgpd'
```

âœ…Â **ì¶œë ¥**

```bash
LISTEN 0      3          127.0.0.1:2601      0.0.0.0:*    users:(("zebra",pid=3827,fd=23))                                                     
LISTEN 0      3          127.0.0.1:2605      0.0.0.0:*    users:(("bgpd",pid=3832,fd=18))                                                      
LISTEN 0      4096         0.0.0.0:179       0.0.0.0:*    users:(("bgpd",pid=3832,fd=22))                                                      
LISTEN 0      4096            [::]:179          [::]:*    users:(("bgpd",pid=3832,fd=23))
```

- BGP ê´€ë ¨ í¬íŠ¸(`179`) ë¦¬ìŠ¨ ì¤‘

```bash
root@router:~# ps -ef |grep frr
```

âœ…Â **ì¶œë ¥**

```bash
root        3814       1  0 22:30 ?        00:00:00 /usr/lib/frr/watchfrr -d -F traditional zebra bgpd staticd
frr         3827       1  0 22:30 ?        00:00:00 /usr/lib/frr/zebra -d -F traditional -A 127.0.0.1 -s 90000000
frr         3832       1  0 22:30 ?        00:00:00 /usr/lib/frr/bgpd -d -F traditional -A 127.0.0.1
frr         3839       1  0 22:30 ?        00:00:00 /usr/lib/frr/staticd -d -F traditional -A 127.0.0.1
root        4417    4399  0 23:02 pts/1    00:00:00 grep --color=auto frr
```

- `watchfrr`, `zebra`, `bgpd`, `staticd` í”„ë¡œì„¸ìŠ¤ êµ¬ë™ í™•ì¸

### **2. FRR ì„¤ì • í™•ì¸ (vtysh)**

```bash
root@router:~# vtysh -c 'show running'
```

âœ…Â **ì¶œë ¥**

```bash
Building configuration...

Current configuration:
!
frr version 8.4.4
frr defaults traditional
hostname router
log syslog informational
no ipv6 forwarding
service integrated-vtysh-config
!
router bgp 65000
 bgp router-id 192.168.10.200
 no bgp ebgp-requires-policy
 bgp graceful-restart
 bgp bestpath as-path multipath-relax
 !
 address-family ipv4 unicast
  network 10.10.1.0/24
  maximum-paths 4
 exit-address-family
exit
!
end
```

- Router BGP AS ë²ˆí˜¸ëŠ” `65000`
- Loopback ë„¤íŠ¸ì›Œí¬ `10.10.1.0/24`ë¥¼ ê´‘ê³ í•˜ë„ë¡ ì„¤ì •ë¨
- ë©€í‹°íŒ¨ìŠ¤(`maximum-paths 4`) í—ˆìš© ì„¤ì • ì ìš©

### **3. FRR ì„¤ì • íŒŒì¼ í™•ì¸**

```bash
root@router:~# cat /etc/frr/frr.conf 
```

âœ…Â **ì¶œë ¥**

```bash
# default to using syslog. /etc/rsyslog.d/45-frr.conf places the log in
# /var/log/frr/frr.log
#
# Note:
# FRR's configuration shell, vtysh, dynamically edits the live, in-memory
# configuration while FRR is running. When instructed, vtysh will persist the
# live configuration to this file, overwriting its contents. If you want to
# avoid this, you can edit this file manually before starting FRR, or instruct
# vtysh to write configuration to a different file.
log syslog informational
!
router bgp 65000
  bgp router-id 192.168.10.200
  bgp graceful-restart
  no bgp ebgp-requires-policy
  bgp bestpath as-path multipath-relax
  maximum-paths 4
  network 10.10.1.0/24
```

- `/etc/frr/frr.conf` íŒŒì¼ì—ì„œ ë™ì¼í•œ ì„¤ì • í™•ì¸ ê°€ëŠ¥
- `network 10.10.1.0/24` ê´‘ê³  ì¤‘

### **4. BGP ìƒíƒœ í™•ì¸**

```bash
root@router:~# vtysh -c 'show ip bgp summary'
```

âœ…Â **ì¶œë ¥**

```bash
% No BGP neighbors found in VRF default
```

- ì•„ì§ Neighbor ì—†ìŒ

```bash
root@router:~# vtysh -c 'show ip bgp'
```

âœ…Â **ì¶œë ¥**

```bash
BGP table version is 1, local router ID is 192.168.10.200, vrf id 0
Default local pref 100, local AS 65000
Status codes:  s suppressed, d damped, h history, * valid, > best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, < announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
*> 10.10.1.0/24     0.0.0.0                  0         32768 i

Displayed  1 routes and 1 total paths
```

- ìì‹ ì´ ë³´ìœ í•œ `10.10.1.0/24` ë„¤íŠ¸ì›Œí¬ë§Œ ê´‘ê³  ì¤‘
- ì™¸ë¶€ ë…¸ë“œì™€ ì—°ê²°ë˜ì§€ ì•Šì•„ BGP í…Œì´ë¸”ì€ ë‹¨ì¼ ì—”íŠ¸ë¦¬ë§Œ ì¡´ì¬

### **5. router ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ í™•ì¸**

```bash
root@router:~# ip -c addr
```

âœ…Â **ì¶œë ¥**

```bash
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:97:4b:c4 brd ff:ff:ff:ff:ff:ff
    altname enp0s5
    altname ens5
    inet 192.168.121.25/24 metric 100 brd 192.168.121.255 scope global dynamic eth0
       valid_lft 2869sec preferred_lft 2869sec
    inet6 fe80::5054:ff:fe97:4bc4/64 scope link 
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:24:2d:30 brd ff:ff:ff:ff:ff:ff
    altname enp0s6
    altname ens6
    inet 192.168.10.200/24 brd 192.168.10.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fe24:2d30/64 scope link 
       valid_lft forever preferred_lft forever
4: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 52:54:00:50:33:eb brd ff:ff:ff:ff:ff:ff
    altname enp0s7
    altname ens7
    inet 192.168.20.200/24 brd 192.168.20.255 scope global eth2
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fe50:33eb/64 scope link 
       valid_lft forever preferred_lft forever
5: loop1: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 22:63:e6:d9:f6:95 brd ff:ff:ff:ff:ff:ff
    inet 10.10.1.200/24 scope global loop1
       valid_lft forever preferred_lft forever
    inet6 fe80::2063:e6ff:fed9:f695/64 scope link 
       valid_lft forever preferred_lft forever
6: loop2: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether 6e:08:a4:e5:88:c0 brd ff:ff:ff:ff:ff:ff
    inet 10.10.2.200/24 scope global loop2
       valid_lft forever preferred_lft forever
    inet6 fe80::6c08:a4ff:fee5:88c0/64 scope link 
       valid_lft forever preferred_lft forever
```

- `loop1`: `10.10.1.200/24`

### **6. router ë¼ìš°íŒ… í…Œì´ë¸” í™•ì¸**

```bash
root@router:~# ip -c route
```

âœ…Â **ì¶œë ¥**

```bash
default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.25 metric 100 
10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200 
192.168.20.0/24 dev eth2 proto kernel scope link src 192.168.20.200 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.25 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.25 metric 100 
```

### **7. BGP Neighbor ì„¤ì • ì¶”ê°€**

routerì—ì„œ Cilium ë…¸ë“œ(`k8s-ctr`, `k8s-w1`, `k8s-w0`)ë¥¼ Neighborë¡œ ë“±ë¡

```bash
root@router:~#  cat << EOF >> /etc/frr/frr.conf
  neighbor CILIUM peer-group
  neighbor CILIUM remote-as external
  neighbor 192.168.10.100 peer-group CILIUM
  neighbor 192.168.10.101 peer-group CILIUM
  neighbor 192.168.20.100 peer-group CILIUM 
EOF
```

- `peer-group` CILIUMìœ¼ë¡œ ë¬¶ì–´ ê´€ë¦¬ ë‹¨ìˆœí™”
- `remote-as external` ì˜µì…˜ìœ¼ë¡œ ë‹¤ë¥¸ AS ìë™ ìˆ˜ìš©

```bash
root@router:~# cat /etc/frr/frr.conf
```

âœ…Â **ì¶œë ¥**

```bash
# default to using syslog. /etc/rsyslog.d/45-frr.conf places the log in
# /var/log/frr/frr.log
#
# Note:
# FRR's configuration shell, vtysh, dynamically edits the live, in-memory
# configuration while FRR is running. When instructed, vtysh will persist the
# live configuration to this file, overwriting its contents. If you want to
# avoid this, you can edit this file manually before starting FRR, or instruct
# vtysh to write configuration to a different file.
log syslog informational
!
router bgp 65000
  bgp router-id 192.168.10.200
  bgp graceful-restart
  no bgp ebgp-requires-policy
  bgp bestpath as-path multipath-relax
  maximum-paths 4
  network 10.10.1.0/24
  neighbor CILIUM peer-group
  neighbor CILIUM remote-as external
  neighbor 192.168.10.100 peer-group CILIUM
  neighbor 192.168.10.101 peer-group CILIUM
  neighbor 192.168.20.100 peer-group CILIUM 
```

- router AS: `65000`, ë…¸ë“œ AS: `65001`

### **8. FRR ì„œë¹„ìŠ¤ ì¬ì‹œì‘ ë° ìƒíƒœ í™•ì¸**

```bash
root@router:~# systemctl daemon-reexec && systemctl restart frr
root@router:~# systemctl status frr --no-pager --full
```

âœ…Â **ì¶œë ¥**

```bash
â— frr.service - FRRouting
     Loaded: loaded (/usr/lib/systemd/system/frr.service; enabled; preset: enabled)
     Active: active (running) since Fri 2025-08-15 23:20:38 KST; 17s ago
       Docs: https://frrouting.readthedocs.io/en/latest/setup.html
    Process: 4540 ExecStart=/usr/lib/frr/frrinit.sh start (code=exited, status=0/SUCCESS)
   Main PID: 4550 (watchfrr)
     Status: "FRR Operational"
      Tasks: 13 (limit: 757)
     Memory: 19.5M (peak: 27.4M)
        CPU: 321ms
     CGroup: /system.slice/frr.service
             â”œâ”€4550 /usr/lib/frr/watchfrr -d -F traditional zebra bgpd staticd
             â”œâ”€4563 /usr/lib/frr/zebra -d -F traditional -A 127.0.0.1 -s 90000000
             â”œâ”€4568 /usr/lib/frr/bgpd -d -F traditional -A 127.0.0.1
             â””â”€4575 /usr/lib/frr/staticd -d -F traditional -A 127.0.0.1

Aug 15 23:20:38 router watchfrr[4550]: [YFT0P-5Q5YX] Forked background command [pid 4551]: /usr/lib/frr/watchfrr.sh restart all
Aug 15 23:20:38 router zebra[4563]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router bgpd[4568]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router staticd[4575]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] zebra state -> up : connect succeeded
Aug 15 23:20:38 router frrinit.sh[4540]:  * Started watchfrr
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] bgpd state -> up : connect succeeded
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] staticd state -> up : connect succeeded
Aug 15 23:20:38 router watchfrr[4550]: [KWE5Q-QNGFC] all daemons up, doing startup-complete notify
Aug 15 23:20:38 router systemd[1]: Started frr.service - FRRouting.
```

### **9. ëª¨ë‹ˆí„°ë§**

```bash
root@router:~# journalctl -u frr -f
```

âœ…Â **ì¶œë ¥**

```bash
Aug 15 23:20:38 router watchfrr[4550]: [YFT0P-5Q5YX] Forked background command [pid 4551]: /usr/lib/frr/watchfrr.sh restart all
Aug 15 23:20:38 router zebra[4563]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router bgpd[4568]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router staticd[4575]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] zebra state -> up : connect succeeded
Aug 15 23:20:38 router frrinit.sh[4540]:  * Started watchfrr
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] bgpd state -> up : connect succeeded
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] staticd state -> up : connect succeeded
Aug 15 23:20:38 router watchfrr[4550]: [KWE5Q-QNGFC] all daemons up, doing startup-complete notify
Aug 15 23:20:38 router systemd[1]: Started frr.service - FRRouting.
```

## **ğŸ›°ï¸ Ciliumì— BGP ì„¤ì •**

### **1. BGP í™œì„±í™” ë…¸ë“œ ë¼ë²¨ ì„¤ì •**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl label nodes k8s-ctr k8s-w0 k8s-w1 enable-bgp=true

# ê²°ê³¼
node/k8s-ctr labeled
node/k8s-w0 labeled
node/k8s-w1 labeled
```

- Cilium BGPë¥¼ ì‹¤í–‰í•  ë…¸ë“œì— `enable-bgp=true` ë¼ë²¨ ë¶€ì—¬

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get node -l enable-bgp=true
```

âœ…Â **ì¶œë ¥**

```bash
NAME      STATUS   ROLES           AGE   VERSION
k8s-ctr   Ready    control-plane   53m   v1.33.2
k8s-w0    Ready    <none>          43m   v1.33.2
k8s-w1    Ready    <none>          53m   v1.33.2
```

- 3ê°œ ë…¸ë“œ ë¼ë²¨ë§ í™•ì¸

### **2. Cilium BGP ë¦¬ì†ŒìŠ¤ ì„¤ì •**

**Ciliumì—ì„œ BGP ë™ì‘ì„ ì •ì˜í•˜ê¸° ìœ„í•´ 3ê°€ì§€ CRD ìƒì„±**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cat << EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisements
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "PodCIDR"
---
apiVersion: cilium.io/v2
kind: CiliumBGPPeerConfig
metadata:
  name: cilium-peer
spec:
  timers:
    holdTimeSeconds: 9
    keepAliveTimeSeconds: 3
  ebgpMultihop: 2
  gracefulRestart:
    enabled: true
    restartTimeSeconds: 15
  families:
    - afi: ipv4
      safi: unicast
      advertisements:
        matchLabels:
          advertise: "bgp"
---
apiVersion: cilium.io/v2
kind: CiliumBGPClusterConfig
metadata:
  name: cilium-bgp
spec:
  nodeSelector:
    matchLabels:
      "enable-bgp": "true"
  bgpInstances:
  - name: "instance-65001"
    localASN: 65001
    peers:
    - name: "tor-switch"
      peerASN: 65000
      peerAddress: 192.168.10.200  # router ip address
      peerConfigRef:
        name: "cilium-peer"
EOF

ciliumbgpadvertisement.cilium.io/bgp-advertisements created
ciliumbgppeerconfig.cilium.io/cilium-peer created
ciliumbgpclusterconfig.cilium.io/cilium-bgp created
```

- `CiliumBGPAdvertisement`
    - `advertisementType: PodCIDR` ì„¤ì • â†’ ê° ë…¸ë“œì˜ PodCIDRë¥¼ BGPë¡œ ê´‘ê³ 
- `CiliumBGPPeerConfig`
    - `advertisements.matchLabels: advertise=bgp` ë¡œ í•„í„°ë§
- `CiliumBGPClusterConfig`
    - ë¼ë²¨(`enable-bgp=true`)ì´ ì§€ì •ëœ ë…¸ë“œë§Œ BGP ë™ì‘ ëŒ€ìƒ
    - localASN: **65001**, peerASN: **65000**
    - peerAddress: **`192.168.10.200` (Router IP)**
    - Peer ì„¤ì •ì€ `cilium-peer` ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì°¸ì¡°

```bash
Aug 15 23:20:38 router watchfrr[4550]: [YFT0P-5Q5YX] Forked background command [pid 4551]: /usr/lib/frr/watchfrr.sh restart all
Aug 15 23:20:38 router zebra[4563]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router bgpd[4568]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router staticd[4575]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] zebra state -> up : connect succeeded
Aug 15 23:20:38 router frrinit.sh[4540]:  * Started watchfrr
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] bgpd state -> up : connect succeeded
Aug 15 23:20:38 router watchfrr[4550]: [QDG3Y-BY5TN] staticd state -> up : connect succeeded
Aug 15 23:20:38 router watchfrr[4550]: [KWE5Q-QNGFC] all daemons up, doing startup-complete notify
Aug 15 23:20:38 router systemd[1]: Started frr.service - FRRouting.
Aug 15 23:27:15 router bgpd[4568]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.20.100 in vrf default
Aug 15 23:27:15 router bgpd[4568]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.100 in vrf default
Aug 15 23:27:15 router bgpd[4568]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.101 in vrf default
```

## **ğŸ” í†µì‹  í™•ì¸**

### **1. BGP ì„¸ì…˜ ì—°ê²° í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# ss -tnlp | grep 179
(âˆ|HomeLab:N/A) root@k8s-ctr:~# ss -tnp | grep 179
ESTAB 0      0               192.168.10.100:35791          192.168.10.200:179   users:(("cilium-agent",pid=5170,fd=50))              
ESTAB 0      0      [::ffff:192.168.10.100]:6443    [::ffff:172.20.0.179]:46928 users:(("kube-apiserver",pid=3868,fd=105))   
```

- Ciliumì€ BGP Listenerê°€ ì•„ë‹ˆë¼ Initiatorë¡œ ë™ì‘í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì¥ë¹„(FRR ë¼ìš°í„°)ì™€ TCP 179 í¬íŠ¸ë¡œ ì—°ê²°ì„ ìˆ˜ë¦½í•¨
- ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ë…¸ë“œ(`192.168.10.100:35791`)ê°€ ë¼ìš°í„°(`192.168.10.200:179`)ì™€ ì„¸ì…˜ì„ ë§ºì€ ìƒíƒœ í™•ì¸ë¨

### **2. Cilium BGP Peer ì—°ê²° ìƒíƒœ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium bgp peers
Node      Local AS   Peer AS   Peer Address     Session State   Uptime   Family         Received   Advertised
k8s-ctr   65001      65000     192.168.10.200   established     10m59s   ipv4/unicast   4          2    
k8s-w0    65001      65000     192.168.10.200   established     10m59s   ipv4/unicast   4          2    
k8s-w1    65001      65000     192.168.10.200   established     10m59s   ipv4/unicast   4          2    
```

- 3ê°œ ë…¸ë“œ(`k8s-ctr`, `k8s-w0`, `k8s-w1`) ëª¨ë‘ ë¼ìš°í„°ì™€ **established** ìƒíƒœ í™•ì¸
- Local ASNì€ 65001, Peer ASNì€ 65000ìœ¼ë¡œ ì •ìƒì ìœ¼ë¡œ ë§¤ì¹­

### **3. PodCIDR ê´‘ê³  ìƒíƒœ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium bgp routes available ipv4 unicast
```

âœ…Â **ì¶œë ¥**

```bash
Node      VRouter   Prefix          NextHop   Age      Attrs
k8s-ctr   65001     172.20.0.0/24   0.0.0.0   12m43s   [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w0    65001     172.20.2.0/24   0.0.0.0   12m42s   [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w1    65001     172.20.1.0/24   0.0.0.0   12m43s   [{Origin: i} {Nexthop: 0.0.0.0}]  
```

- ê° ë…¸ë“œì˜ PodCIDR ê´‘ê³  í™•ì¸ë¨

### **4. Cilium BGP ë¦¬ì†ŒìŠ¤ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ciliumbgpadvertisements,ciliumbgppeerconfigs,ciliumbgpclusterconfigs
```

âœ…Â **ì¶œë ¥**

```bash
NAME                                                  AGE
ciliumbgpadvertisement.cilium.io/bgp-advertisements   13m

NAME                                        AGE
ciliumbgppeerconfig.cilium.io/cilium-peer   13m

NAME                                          AGE
ciliumbgpclusterconfig.cilium.io/cilium-bgp   13m
```

**ê° ë…¸ë“œë³„ `CiliumBGPNodeConfig` ë¦¬ì†ŒìŠ¤ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ciliumbgpnodeconfigs -o yaml | yq
```

âœ…Â **ì¶œë ¥**

```bash
{
  "apiVersion": "v1",
  "items": [
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T14:27:12Z",
        "generation": 1,
        "name": "k8s-ctr",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "7578",
        "uid": "a72d5068-f106-4b37-a0a7-2ad0e72e8f9d"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "establishedTime": "2025-08-15T14:27:14Z",
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peeringState": "established",
                "routeCount": [
                  {
                    "advertised": 2,
                    "afi": "ipv4",
                    "received": 1,
                    "safi": "unicast"
                  }
                ],
                "timers": {
                  "appliedHoldTimeSeconds": 9,
                  "appliedKeepaliveSeconds": 3
                }
              }
            ]
          }
        ]
      }
    },
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T14:27:12Z",
        "generation": 1,
        "name": "k8s-w0",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "7575",
        "uid": "395cc9e2-0f3e-47f3-bce0-169110494292"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "establishedTime": "2025-08-15T14:27:14Z",
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peeringState": "established",
                "routeCount": [
                  {
                    "advertised": 2,
                    "afi": "ipv4",
                    "received": 1,
                    "safi": "unicast"
                  }
                ],
                "timers": {
                  "appliedHoldTimeSeconds": 9,
                  "appliedKeepaliveSeconds": 3
                }
              }
            ]
          }
        ]
      }
    },
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T14:27:12Z",
        "generation": 1,
        "name": "k8s-w1",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "7581",
        "uid": "d98cdab1-5d96-4ecf-ae47-1cc3c80a3071"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "establishedTime": "2025-08-15T14:27:14Z",
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peeringState": "established",
                "routeCount": [
                  {
                    "advertised": 2,
                    "afi": "ipv4",
                    "received": 1,
                    "safi": "unicast"
                  }
                ],
                "timers": {
                  "appliedHoldTimeSeconds": 9,
                  "appliedKeepaliveSeconds": 3
                }
              }
            ]
          }
        ]
      }
    }
  ],
  "kind": "List",
  "metadata": {
    "resourceVersion": ""
  }
}
```

- Local ASN 65001
- Peer Address `192.168.10.200`, Peer ASN 65000
- Peering State `established`
- RouteCount: `advertised 2`, `received 1`

### **5. ë¼ìš°í„° ì»¤ë„ ë¼ìš°íŒ… í…Œì´ë¸” í™•ì¸**

```bash
root@router:~# ip -c route | grep bgp
172.20.0.0/24 nhid 29 via 192.168.10.100 dev eth1 proto bgp metric 20 
172.20.1.0/24 nhid 30 via 192.168.10.101 dev eth1 proto bgp metric 20 
172.20.2.0/24 nhid 28 via 192.168.20.100 dev eth2 proto bgp metric 20 
```

- FRR ë¼ìš°í„° ì»¤ë„ ë¼ìš°íŒ… í…Œì´ë¸”ì— **Pod CIDR** ê²½ë¡œê°€ BGP í”„ë¡œí† ì½œì„ í†µí•´ í•™ìŠµë˜ì–´ ë“±ë¡ë¨
- íŠ¹ì • Pod ëŒ€ì—­ê³¼ í†µì‹ í•˜ë ¤ë©´ ë°˜ë“œì‹œ í•´ë‹¹ ë…¸ë“œë¡œ ì „ë‹¬ë˜ë„ë¡ ë¼ìš°íŒ… ê²½ë¡œê°€ ì¡í˜

### **6. BGP Neighbor ê´€ê³„ í™•ì¸**

```bash
root@router:~# vtysh -c 'show ip bgp summary'
```

âœ…Â **ì¶œë ¥**

```bash
IPv4 Unicast Summary (VRF default):
BGP router identifier 192.168.10.200, local AS number 65000 vrf-id 0
BGP table version 4
RIB entries 7, using 1344 bytes of memory
Peers 3, using 2172 KiB of memory
Peer groups 1, using 64 bytes of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc
192.168.10.100  4      65001       353       356        0    0    0 00:17:29            1        4 N/A
192.168.10.101  4      65001       353       356        0    0    0 00:17:29            1        4 N/A
192.168.20.100  4      65001       353       356        0    0    0 00:17:28            1        4 N/A

Total number of neighbors 3
```

- FRR ë¼ìš°í„°(AS 65000)ëŠ” Ciliumì´ êµ¬ë™ ì¤‘ì¸ 3ê°œ ë…¸ë“œ(AS 65001)ì™€ BGP neighborë¥¼ ë§ºìŒ
- ëª¨ë“  neighborê°€ `Established` ìƒíƒœë¡œ ì •ìƒ ì—°ê²°ë¨

### **7. BGP ê´‘ê³  ê²½ë¡œ í™•ì¸**

```bash
root@router:~# vtysh -c 'show ip bgp'
BGP table version is 4, local router ID is 192.168.10.200, vrf id 0
Default local pref 100, local AS 65000
Status codes:  s suppressed, d damped, h history, * valid, > best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, < announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
*> 10.10.1.0/24     0.0.0.0                  0         32768 i
*> 172.20.0.0/24    192.168.10.100                         0 65001 i
*> 172.20.1.0/24    192.168.10.101                         0 65001 i
*> 172.20.2.0/24    192.168.20.100                         0 65001 i

Displayed  4 routes and 4 total paths
```

- Pod CIDR (`172.20.0.0/24`, `172.20.1.0/24`, `172.20.2.0/24`)ê°€ ëª¨ë‘ ìˆ˜ì‹ ë¨
- nexthopì€ ê°ê° ë…¸ë“œì˜ ë‚´ë¶€ IPë¡œ í‘œì‹œë˜ë©°, BGPë¥¼ í†µí•´ ì˜¬ë°”ë¥´ê²Œ ê´‘ê³  ì „íŒŒê°€ ì´ë¤„ì§„ ê²ƒ í™•ì¸

### **8. BGP Neighbor ì„¤ì • í›„ í†µì‹  ë¶ˆê°€ í˜„ìƒ**

```bash
Hostname: webpod-697b545f57-fbtbj
---
---
---
---
---
---
---
---
---
---
---
Hostname: webpod-697b545f57-fbtbj
---
---
---
---
---
...
```

- Ciliumì˜ íŠ¹ì„±ìƒ BGP ì„¸ì…˜ì„ ë§ºë”ë¼ë„ ê¸°ë³¸ì ìœ¼ë¡œ ì»¤ë„ ë¼ìš°íŒ… í…Œì´ë¸”ì— ê²½ë¡œë¥¼ ì£¼ì…í•˜ì§€ ì•ŠìŒ (`disable-fib` ìƒíƒœ)

## **ğŸ”€ BGP ì •ë³´ ì „ë‹¬ í™•ì¸**

### **1. ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ tcpdump ì§„í–‰**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# tcpdump -i eth1 tcp port 179 -w /tmp/bgp.pcap

# ê²°ê³¼
tcpdump: listening on eth1, link-type EN10MB (Ethernet), snapshot length 262144 bytes
```

### **2. FRR ì¬ì‹œì‘ ë° BGP ì„¸ì…˜ í™•ì¸**

```bash
root@router:~# systemctl restart frr && journalctl -u frr -f
```

âœ…Â **ì¶œë ¥**

```bash
Aug 16 00:00:33 router watchfrr.sh[4856]: Cannot stop zebra: pid file not found
Aug 16 00:00:33 router zebra[4858]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 16 00:00:33 router bgpd[4863]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 16 00:00:33 router staticd[4870]: [VTVCM-Y2NW3] Configuration Read in Took: 00:00:00
Aug 16 00:00:33 router frrinit.sh[4835]:  * Started watchfrr
Aug 16 00:00:33 router systemd[1]: Started frr.service - FRRouting.
Aug 16 00:00:33 router watchfrr[4845]: [QDG3Y-BY5TN] zebra state -> up : connect succeeded
Aug 16 00:00:33 router watchfrr[4845]: [QDG3Y-BY5TN] bgpd state -> up : connect succeeded
Aug 16 00:00:33 router watchfrr[4845]: [QDG3Y-BY5TN] staticd state -> up : connect succeeded
Aug 16 00:00:33 router watchfrr[4845]: [KWE5Q-QNGFC] all daemons up, doing startup-complete notify
Aug 16 00:00:39 router bgpd[4863]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.100 in vrf default
Aug 16 00:00:39 router bgpd[4863]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.101 in vrf default
Aug 16 00:00:40 router bgpd[4863]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.20.100 in vrf default
```

### **3. Termshark í™•ì¸**

**(1) BGP íŒ¨í‚· ìº¡ì²˜ íŒŒì¼ ë¶„ì„**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# termshark -r /tmp/bgp.pcap
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/37112528-b622-401e-93dc-dc51a7326d51/image.png)

**(2) íŒ¨í‚· ë¶„ì„ ì¤‘, ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ì—°ê²°ì´ ëŠê¹€**

```bash
vagrant halt k8s-ctr --force
```

- ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ VM ê°•ì œ ì¢…ë£Œ

```bash
sudo virsh start 5w_k8s-ctr

# ê²°ê³¼
Domain '5w_k8s-ctr' started
```

- ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ VM ì¬ê¸°ë™

---

## **ğŸ› ï¸ ë¬¸ì œ í•´ê²° í›„ í†µì‹  í™•ì¸**

### **1. Cilium BGP ë™ì‘ íŠ¹ì„± í™•ì¸**

- Ciliumì˜ BGPëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **ì™¸ë¶€ ê²½ë¡œë¥¼ ì»¤ë„ ë¼ìš°íŒ… í…Œì´ë¸”ì— ì£¼ì…í•˜ì§€ ì•ŠìŒ**
- `disable-fib` ì˜µì…˜ìœ¼ë¡œ ë¹Œë“œë˜ì–´ ìˆìŒ â†’ ì»¤ë„ ë¼ìš°íŒ… í…Œì´ë¸”(FIB)ì— BGP ê²½ë¡œë¥¼ ë°˜ì˜í•˜ì§€ ì•Šê² ë‹¤ëŠ” ì˜ë¯¸
- ë”°ë¼ì„œ BGP í”¼ì–´ë¡œë¶€í„° ê²½ë¡œëŠ” ìˆ˜ì‹ í–ˆì§€ë§Œ, `ip route` ì¶œë ¥ì—ëŠ” í•´ë‹¹ CIDR ëŒ€ì—­ì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

### **2. BGP ê²½ë¡œ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# ip -c route
```

âœ…Â **ì¶œë ¥**

```bash
default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.70 metric 100 
172.20.0.0/24 via 172.20.0.230 dev cilium_host proto kernel src 172.20.0.230 
172.20.0.230 dev cilium_host proto kernel scope link 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.100 
192.168.20.0/24 via 192.168.10.200 dev eth1 proto static 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.70 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.70 metric 100
```

- `172.20.1.0/24`, `172.20.2.0/24` ëŒ€ì—­ì´ ëˆ„ë½ë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium bgp routes
```

âœ…Â **ì¶œë ¥**

```bash
(Defaulting to `available ipv4 unicast` routes, please see help for more options)

Node      VRouter   Prefix          NextHop   Age      Attrs
k8s-ctr   65001     172.20.0.0/24   0.0.0.0   9m26s    [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w0    65001     172.20.2.0/24   0.0.0.0   57m18s   [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w1    65001     172.20.1.0/24   0.0.0.0   57m18s   [{Origin: i} {Nexthop: 0.0.0.0}] 
```

- ê° ë…¸ë“œ CIDR ê²½ë¡œê°€ BGPë¡œ ì •ìƒ ìˆ˜ì‹ ëœ ìƒíƒœ í™•ì¸ ê°€ëŠ¥

ì¦‰, **BGP ìˆ˜ì‹ ì€ ì •ìƒì ì´ë‚˜, ì»¤ë„ ë¼ìš°íŒ… ë°˜ì˜ì´ ì•ˆ ë˜ì–´ í†µì‹  ë¶ˆê°€ ìƒíƒœ**ì„

```bash
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-fbtbj
---
---
---
---
---
---
Hostname: webpod-697b545f57-fbtbj
---
---
...
```

### **3. k8s íŒŒë“œ ëŒ€ì—­ í†µì‹ ì„ eth1 ê²½ë¡œë¡œ ì§€ì •**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# ip route add 172.20.0.0/16 via 192.168.10.200
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@k8s-w1 sudo ip route add 172.20.0.0/16 via 192.168.10.200
sshpass -p 'vagrant' ssh vagrant@k8s-w0 sudo ip route add 172.20.0.0/16 via 192.168.20.200
```

- eth0 â†’ ì¸í„°ë„· í†µì‹  ì „ìš©
- eth1 â†’ Kubernetes íŒŒë“œ í†µì‹  ì „ìš©
- ë”°ë¼ì„œ íŒŒë“œ ëŒ€ì—­(`172.20.0.0/16`)ì„ eth1ì„ í†µí•´ ë¼ìš°íŒ…í•˜ë„ë¡ ëª…ì‹œì  ê²½ë¡œ ì¶”ê°€

```bash
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-rpblf
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-rpblf
---
Hostname: webpod-697b545f57-rpblf
---
Hostname: webpod-697b545f57-rpblf
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-pxhvr
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-rpblf
---
Hostname: webpod-697b545f57-fbtbj
---
Hostname: webpod-697b545f57-pxhvr
---
...
```

- ê²½ë¡œ ì¶”ê°€ í›„ íŒŒë“œ ê°„ í†µì‹  ì •ìƒ ë™ì‘ í™•ì¸

---

## **â¸ï¸ ë…¸ë“œ(k8s-w0) ìœ ì§€ë³´ìˆ˜ ìƒí™©**
- [https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-operation/#shutting-down-a-node](https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-operation/#shutting-down-a-node)

### **1. ë…¸ë“œ ìœ ì§€ë³´ìˆ˜ ì‹œì‘ (k8s-w0 Drain)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl drain k8s-w0 --ignore-daemonsets

# ê²°ê³¼
node/k8s-w0 cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/cilium-envoy-8tgrn, kube-system/cilium-wszbk, kube-system/kube-proxy-xhjtq
evicting pod default/webpod-697b545f57-rpblf
pod/webpod-697b545f57-rpblf evicted
node/k8s-w0 drained
```

- `kubectl drain` ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ `k8s-w0` ë…¸ë“œì˜ íŒŒë“œë¥¼ ì•ˆì „í•˜ê²Œ ì œê±°í•˜ê³  ìŠ¤ì¼€ì¤„ë§ì„ ë§‰ìŒ

### **2. BGP ê¸°ëŠ¥ ë¹„í™œì„±í™” (enable-bgp=false)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl label nodes k8s-w0 enable-bgp=false --overwrite

# ê²°ê³¼
node/k8s-w0 labeled
```

- ìœ ì§€ë³´ìˆ˜ë¥¼ ìœ„í•´ `k8s-w0` ë…¸ë“œì˜ `enable-bgp` ë¼ë²¨ì„ `false`ë¡œ ë³€ê²½
- ì´ë¡œ ì¸í•´ Ciliumì˜ BGP ë°ëª¬ì´ í•´ë‹¹ ë…¸ë“œì—ì„œëŠ” ë™ì‘í•˜ì§€ ì•Šê²Œ ë¨

### **3. ë…¸ë“œ ìƒíƒœ í™•ì¸ (SchedulingDisabled)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get node
```

âœ…Â **ì¶œë ¥**

```bash
NAME      STATUS                     ROLES           AGE    VERSION
k8s-ctr   Ready                      control-plane   124m   v1.33.2
k8s-w0    Ready,SchedulingDisabled   <none>          115m   v1.33.2
k8s-w1    Ready                      <none>          124m   v1.33.2
```

- `k8s-w0` ë…¸ë“œ ìƒíƒœê°€ `Ready,SchedulingDisabled` ë¡œ í‘œì‹œë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ciliumbgpnodeconfigs
NAME      AGE
k8s-ctr   70m
k8s-w1    70m
```

- `k8s-w0` ë…¸ë“œëŠ” BGP NodeConfig ëª©ë¡ì—ì„œë„ ì œì™¸ë¨

### **4. BGP ë¼ìš°íŠ¸/í”¼ì–´ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium bgp routes
```

âœ…Â **ì¶œë ¥**

```bash
(Defaulting to `available ipv4 unicast` routes, please see help for more options)

Node      VRouter   Prefix          NextHop   Age        Attrs
k8s-ctr   65001     172.20.0.0/24   0.0.0.0   23m21s     [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w1    65001     172.20.1.0/24   0.0.0.0   1h11m13s   [{Origin: i} {Nexthop: 0.0.0.0}]  
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium bgp peers
```

âœ…Â **ì¶œë ¥**

```bash
Node      Local AS   Peer AS   Peer Address     Session State   Uptime   Family         Received   Advertised
k8s-ctr   65001      65000     192.168.10.200   established     15m58s   ipv4/unicast   3          2    
k8s-w1    65001      65000     192.168.10.200   established     15m58s   ipv4/unicast   3          2  
```

- ì¶œë ¥ì—ì„œ `k8s-w0` ë¼ìš°íŠ¸ ì •ë³´ê°€ ì‚¬ë¼ì§

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp summary'"
```

âœ…Â **ì¶œë ¥**

```bash
IPv4 Unicast Summary (VRF default):
BGP router identifier 192.168.10.200, local AS number 65000 vrf-id 0
BGP table version 5
RIB entries 5, using 960 bytes of memory
Peers 3, using 2172 KiB of memory
Peer groups 1, using 64 bytes of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc
192.168.10.100  4      65001       400       404        0    0    0 00:19:48            1        3 N/A
192.168.10.101  4      65001       400       404        0    0    0 00:19:49            1        3 N/A
192.168.20.100  4      65001       266       267        0    0    0 00:06:48       Active        0 N/A

Total number of neighbors 3
```

- FRR ë¼ìš°í„°ì—ì„œë„ ìƒíƒœê°€ **Active** ë¡œ í‘œì‹œë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip route bgp'"
```

âœ…Â **ì¶œë ¥**

```bash
Codes: K - kernel route, C - connected, S - static, R - RIP,
       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,
       T - Table, v - VNC, V - VNC-Direct, A - Babel, F - PBR,
       f - OpenFabric,
       > - selected route, * - FIB route, q - queued, r - rejected, b - backup
       t - trapped, o - offload failure

B>* 172.20.0.0/24 [20/0] via 192.168.10.100, eth1, weight 1, 00:18:28
B>* 172.20.1.0/24 [20/0] via 192.168.10.101, eth1, weight 1, 00:18:28
```

- ë¼ìš°íŒ… í…Œì´ë¸”ì—ì„œë„ `k8s-w0`ì˜ CIDRì´ ì œê±°ë¨

### **5. ë…¸ë“œ ë³µêµ¬ (enable-bgp=true & uncordon)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl label nodes k8s-w0 enable-bgp=true --overwrite

# ê²°ê³¼
node/k8s-w0 labeled
```

- ìœ ì§€ë³´ìˆ˜ê°€ ëë‚œ í›„ `k8s-w0` ë¼ë²¨ì„ ë‹¤ì‹œ `enable-bgp=true` ë¡œ ì›ë³µ

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl uncordon k8s-w0

# ê²°ê³¼
node/k8s-w0 uncordoned
```

- `kubectl uncordon` ëª…ë ¹ì–´ë¡œ ìŠ¤ì¼€ì¤„ë§ì„ í—ˆìš©í•˜ì—¬ ì •ìƒ ìƒíƒœë¡œ ë³µêµ¬

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get node
kubectl get ciliumbgpnodeconfigs
cilium bgp routes
cilium bgp peers
```

âœ…Â **ì¶œë ¥**

```bash
NAME      STATUS   ROLES           AGE    VERSION
k8s-ctr   Ready    control-plane   132m   v1.33.2
k8s-w0    Ready    <none>          123m   v1.33.2
k8s-w1    Ready    <none>          132m   v1.33.2

NAME      AGE
k8s-ctr   77m
k8s-w0    47s
k8s-w1    77m

(Defaulting to `available ipv4 unicast` routes, please see help for more options)

Node      VRouter   Prefix          NextHop   Age        Attrs
k8s-ctr   65001     172.20.0.0/24   0.0.0.0   29m55s     [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w0    65001     172.20.2.0/24   0.0.0.0   48s        [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w1    65001     172.20.1.0/24   0.0.0.0   1h17m47s   [{Origin: i} {Nexthop: 0.0.0.0}]   

Node      Local AS   Peer AS   Peer Address     Session State   Uptime   Family         Received   Advertised
k8s-ctr   65001      65000     192.168.10.200   established     22m2s    ipv4/unicast   4          2    
k8s-w0    65001      65000     192.168.10.200   established     46s      ipv4/unicast   4          2    
k8s-w1    65001      65000     192.168.10.200   established     22m2s    ipv4/unicast   4          2
```

### **6. ë…¸ë“œë³„ íŒŒë“œ ë¶„ë°° ì‹¤í–‰**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get pod -owide
```

âœ…Â **ì¶œë ¥**

```bash
NAME                      READY   STATUS    RESTARTS      AGE    IP            NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   1 (31m ago)   117m   172.20.0.35   k8s-ctr   <none>           <none>
webpod-697b545f57-fbtbj   1/1     Running   1 (31m ago)   119m   172.20.0.6    k8s-ctr   <none>           <none>
webpod-697b545f57-lzxbc   1/1     Running   0             10m    172.20.1.98   k8s-w1    <none>           <none>
webpod-697b545f57-pxhvr   1/1     Running   0             119m   172.20.1.65   k8s-w1    <none>           <none>
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl scale deployment webpod --replicas 0
# ê²°ê³¼
deployment.apps/webpod scaled

(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl scale deployment webpod --replicas 3
# ê²°ê³¼
deployment.apps/webpod scaled
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get pod -owide
```

âœ…Â **ì¶œë ¥**

```bash
NAME                      READY   STATUS    RESTARTS      AGE    IP             NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   1 (31m ago)   117m   172.20.0.35    k8s-ctr   <none>           <none>
webpod-697b545f57-5twrq   1/1     Running   0             7s     172.20.1.119   k8s-w1    <none>           <none>
webpod-697b545f57-cp7xq   1/1     Running   0             7s     172.20.0.15    k8s-ctr   <none>           <none>
webpod-697b545f57-xtmdx   1/1     Running   0             7s     172.20.2.35    k8s-w0    <none>           <none>
```

- `kubectl scale` ëª…ë ¹ì–´ë¡œ `webpod` ë°°í¬ë¥¼ ì¤„ì˜€ë‹¤ê°€ ë‹¤ì‹œ í™•ì¥í•˜ì—¬ íŒŒë“œê°€ `k8s-w0`ì—ë„ ì •ìƒ ë°°ì¹˜ë¨ì„ í™•ì¸

---

## **ğŸš« CRD ìƒíƒœ ë³´ê³  ë¹„í™œì„±í™”**
- [https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-operation/#disabling-crd-status-report](https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-operation/#disabling-crd-status-report)
- Cilium BGPëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `CiliumBGPNodeConfig` ë¦¬ì†ŒìŠ¤ì˜ `status` í•„ë“œì— í”¼ì–´ ìƒíƒœ, ì„¸ì…˜, ë¼ìš°íŠ¸ ì •ë³´ë¥¼ ê¸°ë¡í•¨
- ê·¸ëŸ¬ë‚˜ ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„°ì—ì„œëŠ” ìƒíƒœ ë³´ê³ ê°€ ë¹ˆë²ˆíˆ ë°œìƒí•´ **Kubernetes API ì„œë²„ì— ë¶€í•˜**ë¥¼ ì¤„ ìˆ˜ ìˆìŒ
- ë”°ë¼ì„œ ê³µì‹ ë¬¸ì„œì—ì„œë„ `bgp status reporting off` ì˜µì…˜ ì‚¬ìš©ì„ ê¶Œì¥í•¨

### **1. í˜„ì¬ BGP ìƒíƒœ í™•ì¸ (Status Report í™œì„±í™” ìƒíƒœ)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ciliumbgpnodeconfigs -o yaml | yq
```

âœ…Â **ì¶œë ¥**

```bash
{
  "apiVersion": "v1",
  "items": [
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T14:27:12Z",
        "generation": 1,
        "name": "k8s-ctr",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "15080",
        "uid": "a72d5068-f106-4b37-a0a7-2ad0e72e8f9d"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "establishedTime": "2025-08-15T15:22:57Z",
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peeringState": "established",
                "routeCount": [
                  {
                    "advertised": 2,
                    "afi": "ipv4",
                    "received": 3,
                    "safi": "unicast"
                  }
                ],
                "timers": {
                  "appliedHoldTimeSeconds": 9,
                  "appliedKeepaliveSeconds": 3
                }
              }
            ]
          }
        ]
      }
    },
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T15:44:11Z",
        "generation": 1,
        "name": "k8s-w0",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "16068",
        "uid": "fd222576-cc33-4f4f-b7cd-c8157fbc8009"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "establishedTime": "2025-08-15T15:44:13Z",
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peeringState": "established",
                "routeCount": [
                  {
                    "advertised": 2,
                    "afi": "ipv4",
                    "received": 3,
                    "safi": "unicast"
                  }
                ],
                "timers": {
                  "appliedHoldTimeSeconds": 9,
                  "appliedKeepaliveSeconds": 3
                }
              }
            ]
          }
        ]
      }
    },
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T14:27:12Z",
        "generation": 1,
        "name": "k8s-w1",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "15076",
        "uid": "d98cdab1-5d96-4ecf-ae47-1cc3c80a3071"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "establishedTime": "2025-08-15T15:22:57Z",
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peeringState": "established",
                "routeCount": [
                  {
                    "advertised": 2,
                    "afi": "ipv4",
                    "received": 3,
                    "safi": "unicast"
                  }
                ],
                "timers": {
                  "appliedHoldTimeSeconds": 9,
                  "appliedKeepaliveSeconds": 3
                }
              }
            ]
          }
        ]
      }
    }
  ],
  "kind": "List",
  "metadata": {
    "resourceVersion": ""
  }
}
```

- ê° ë…¸ë“œ(`k8s-ctr`, `k8s-w0`, `k8s-w1`)ì˜ `status`ì— **í”¼ì–´ ì—°ê²° ìƒíƒœ (`established`), Advertised / Received Routes, Keepalive / HoldTime ê°’** ë“±ì˜ ì •ë³´ê°€ ìƒì„¸íˆ ê¸°ë¡ë¨

### **2. Helm ì—…ê·¸ë ˆì´ë“œë¡œ ìƒíƒœ ë³´ê³  ë¹„í™œì„±í™”**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# helm upgrade cilium cilium/cilium --version 1.18.0 --namespace kube-system --reuse-values \
  --set bgpControlPlane.statusReport.enabled=false
  
# ê²°ê³¼  
Release "cilium" has been upgraded. Happy Helming!
NAME: cilium
LAST DEPLOYED: Sat Aug 16 00:51:38 2025
NAMESPACE: kube-system
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
You have successfully installed Cilium with Hubble Relay and Hubble UI.

Your release version is 1.18.0.

For any further help, visit https://docs.cilium.io/en/v1.18/gettinghelp
```

- ì„±ê³µì ìœ¼ë¡œ `cilium` ì°¨íŠ¸ê°€ ì¬ë°°í¬ë˜ë©°, ì´í›„ BGP ìƒíƒœ ë³´ê³ ê°€ ì¤‘ë‹¨ë¨

### **3. Cilium DaemonSet ë¡¤ë§ ì¬ì‹œì‘**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl -n kube-system rollout restart ds/cilium

# ê²°ê³¼
daemonset.apps/cilium restarted
```

### **4. ê²°ê³¼ í™•ì¸ (Status Report ì œê±°ë¨)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ciliumbgpnodeconfigs -o yaml | yq
```

âœ…Â **ì¶œë ¥**

```bash
{
  "apiVersion": "v1",
  "items": [
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T14:27:12Z",
        "generation": 1,
        "name": "k8s-ctr",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "17327",
        "uid": "a72d5068-f106-4b37-a0a7-2ad0e72e8f9d"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {}
    },
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T15:44:11Z",
        "generation": 1,
        "name": "k8s-w0",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "17231",
        "uid": "fd222576-cc33-4f4f-b7cd-c8157fbc8009"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {}
    },
    {
      "apiVersion": "cilium.io/v2",
      "kind": "CiliumBGPNodeConfig",
      "metadata": {
        "creationTimestamp": "2025-08-15T14:27:12Z",
        "generation": 1,
        "name": "k8s-w1",
        "ownerReferences": [
          {
            "apiVersion": "cilium.io/v2",
            "controller": true,
            "kind": "CiliumBGPClusterConfig",
            "name": "cilium-bgp",
            "uid": "e1f4b328-d375-4a7c-a99b-ed2658602a14"
          }
        ],
        "resourceVersion": "17229",
        "uid": "d98cdab1-5d96-4ecf-ae47-1cc3c80a3071"
      },
      "spec": {
        "bgpInstances": [
          {
            "localASN": 65001,
            "name": "instance-65001",
            "peers": [
              {
                "name": "tor-switch",
                "peerASN": 65000,
                "peerAddress": "192.168.10.200",
                "peerConfigRef": {
                  "name": "cilium-peer"
                }
              }
            ]
          }
        ]
      },
      "status": {}
    }
  ],
  "kind": "List",
  "metadata": {
    "resourceVersion": ""
  }
}
```

- `status` í•„ë“œê°€ **ë¹„ì–´ ìˆìŒ (`status: {}`)**
- ë” ì´ìƒ CRDë¥¼ í†µí•´ BGP ìƒíƒœê°€ ê¸°ë¡ë˜ì§€ ì•ŠìŒ â†’ API ì„œë²„ ë¶€í•˜ ê°ì†Œ íš¨ê³¼

---

## **ğŸ·ï¸ LoadBalancer  External IPë¥¼ BGPë¡œ ê´‘ê³ **

- [https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-v2/#service-virtual-ips](https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-v2/#service-virtual-ips)
- Kubernetes Service íƒ€ì…ì„ `LoadBalancer`ë¡œ ë³€ê²½í•˜ì—¬ External IPë¥¼ í• ë‹¹ë°›ìŒ
- L2 Announcementì™€ ë‹¬ë¦¬ BGPëŠ” ë¼ìš°íŒ… ê¸°ë°˜ì´ë¼, External IPê°€ **ë…¸ë“œ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­ê³¼ ë‹¬ë¼ë„ ê´‘ê³  ê°€ëŠ¥**

### **1. Cilium LoadBalancer IP Pool ìƒì„±**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cat << EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumLoadBalancerIPPool
metadata:
  name: "cilium-pool"
spec:
  allowFirstLastIPs: "No"
  blocks:
  - cidr: "172.16.1.0/24"
EOF

# ê²°ê³¼
ciliumloadbalancerippool.cilium.io/cilium-pool created
```

- `CiliumLoadBalancerIPPool` CRDë¥¼ ì´ìš©í•´ `172.16.1.0/24` ëŒ€ì—­ì„ í’€ë¡œ ë“±ë¡

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ippool
NAME          DISABLED   CONFLICTING   IPS AVAILABLE   AGE
cilium-pool   false      False         254             3m12s
```

- `/24` ëŒ€ì—­(`172.16.1.0/24`) â†’ **254ê°œ IP í• ë‹¹ ê°€ëŠ¥**

### **2. ì„œë¹„ìŠ¤ íƒ€ì… LoadBalancer ì ìš© ë° External IP í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl patch svc webpod -p '{"spec": {"type": "LoadBalancer"}}'

# ê²°ê³¼
service/webpod patched
```

- ê¸°ì¡´ `webpod` ì„œë¹„ìŠ¤ë¥¼ `LoadBalancer` íƒ€ì…ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ External IPë¥¼ í• ë‹¹ë°›ìŒ

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get svc webpod 
```

âœ…Â **ì¶œë ¥**

```bash
NAME     TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
webpod   LoadBalancer   10.96.54.159   172.16.1.1    80:32567/TCP   15h
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get ippool
```

âœ…Â **ì¶œë ¥**

```bash
NAME          DISABLED   CONFLICTING   IPS AVAILABLE   AGE
cilium-pool   false      False         253             4m50s
```

- `cilium-pool`ì—ì„œ 1ê°œ IPê°€ ì†Œëª¨ë˜ì–´ `172.16.1.1`ì´ ì™¸ë¶€ ì ‘ê·¼ìš© ì£¼ì†Œë¡œ ë¶€ì—¬ë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl describe svc webpod | grep 'Traffic Policy'
```

âœ…Â **ì¶œë ¥**

```bash
External Traffic Policy:  Cluster
Internal Traffic Policy:  Cluster
```

- Traffic PolicyëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **Cluster** ëª¨ë“œë¡œ ì„¤ì •ë˜ì–´, ëª¨ë“  ë…¸ë“œë¡œ íŠ¸ë˜í”½ì´ ë¶„ì‚°ë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg service list
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend                Service Type   Backend                                 
1    10.96.12.94:80/TCP      ClusterIP      1 => 172.20.0.232:4245/TCP (active)     
2    0.0.0.0:30003/TCP       NodePort       1 => 172.20.0.13:8081/TCP (active)      
5    10.96.33.159:80/TCP     ClusterIP      1 => 172.20.0.13:8081/TCP (active)      
6    10.96.198.41:443/TCP    ClusterIP      1 => 172.20.0.122:10250/TCP (active)    
7    10.96.0.1:443/TCP       ClusterIP      1 => 192.168.10.100:6443/TCP (active)   
8    10.96.137.113:443/TCP   ClusterIP      1 => 192.168.10.101:4244/TCP (active)   
9    10.96.0.10:53/TCP       ClusterIP      1 => 172.20.0.82:53/TCP (active)        
                                            2 => 172.20.0.104:53/TCP (active)       
10   10.96.0.10:53/UDP       ClusterIP      1 => 172.20.0.82:53/UDP (active)        
                                            2 => 172.20.0.104:53/UDP (active)       
11   10.96.0.10:9153/TCP     ClusterIP      1 => 172.20.0.82:9153/TCP (active)      
                                            2 => 172.20.0.104:9153/TCP (active)     
12   10.96.54.159:80/TCP     ClusterIP      1 => 172.20.0.15:80/TCP (active)        
                                            2 => 172.20.1.119:80/TCP (active)       
                                            3 => 172.20.2.35:80/TCP (active)        
14   0.0.0.0:32567/TCP       NodePort       1 => 172.20.0.15:80/TCP (active)        
                                            2 => 172.20.1.119:80/TCP (active)       
                                            3 => 172.20.2.35:80/TCP (active)        
17   172.16.1.1:80/TCP       LoadBalancer   1 => 172.20.0.15:80/TCP (active)        
                                            2 => 172.20.1.119:80/TCP (active)       
                                            3 => 172.20.2.35:80/TCP (active)   
```

- `172.16.1.1:80/TCP` LoadBalancer í”„ë¡ íŠ¸ì—”ë“œê°€ íŒŒë“œ 3ê°œ(`172.20.x.x`) ë°±ì—”ë“œì™€ ë§¤í•‘ë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get svc webpod -o jsonpath='{.status.loadBalancer.ingress[0].ip}'
LBIP=$(kubectl get svc webpod -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
curl -s $LBIP
```

âœ…Â **ì¶œë ¥**

```bash
172.16.1.1Hostname: webpod-697b545f57-5twrq
IP: 127.0.0.1
IP: ::1
IP: 172.20.1.119
IP: fe80::dcab:bcff:fee2:3765
RemoteAddr: 192.168.10.100:59608
GET / HTTP/1.1
Host: 172.16.1.1
User-Agent: curl/8.5.0
Accept: */*
```

- í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì—ì„œ `curl 172.16.1.1` í…ŒìŠ¤íŠ¸ ì‹œ ì •ìƒì ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì‘ë‹µ í™•ì¸ë¨

### **3. router ë¼ìš°íŒ… í…Œì´ë¸” ëª¨ë‹ˆí„°ë§**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# watch "sshpass -p 'vagrant' ssh vagrant@router ip -c route"
```

âœ…Â **ì¶œë ¥**

```bash
Every 2.0s: sshpass -p 'vagrant' ssh vagrant@router ip -c route   k8s-ctr: Sat Aug 16 14:04:23 2025

default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.25 metric 100
10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
172.20.0.0/24 nhid 92 via 192.168.10.100 dev eth1 proto bgp metric 20
172.20.1.0/24 nhid 88 via 192.168.10.101 dev eth1 proto bgp metric 20
172.20.2.0/24 nhid 94 via 192.168.20.100 dev eth2 proto bgp metric 20
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200 
192.168.20.0/24 dev eth2 proto kernel scope link src 192.168.20.200 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.25 metric 100
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.25 metric 100
```

- í˜„ì¬ëŠ” Cilium BGPë¥¼ í†µí•´ **Pod CIDR ëŒ€ì—­(`172.20.0.0/24`, `172.20.1.0/24`, `172.20.2.0/24`)** ì´ ê´‘ê³ ëœ ìƒíƒœ

### **4. Cilium BGP Advertisement í™•ì¸ (Pod CIDR)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get CiliumBGPAdvertisement
```

âœ…Â **ì¶œë ¥**

```bash
NAME                 AGE
bgp-advertisements   14h
```

- í˜„ì¬ëŠ” **Pod CIDRë¥¼ ê´‘ê³ **í•˜ë„ë¡ ì„¤ì •ëœ ì •ì±…ë§Œ ì¡´ì¬

### **5. ìƒˆë¡œìš´ BGP Advertisement ìƒì„±**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# k get svc
```

âœ…Â **ì¶œë ¥**

```bash
NAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1      <none>        443/TCP        15h
webpod       LoadBalancer   10.96.54.159   172.16.1.1    80:32567/TCP   15h
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cat << EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisements-lb-exip-webpod
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "Service"
      service:
        addresses:
          - LoadBalancerIP
      selector:             
        matchExpressions:
          - { key: app, operator: In, values: [ webpod ] }
EOF

# ê²°ê³¼
ciliumbgpadvertisement.cilium.io/bgp-advertisements-lb-exip-webpod created
```

- `advertisementType: Service`ë¡œ ì§€ì •í•˜ì—¬ **Serviceì˜ LoadBalancer External IPë§Œ ê´‘ê³ **í•˜ë„ë¡ ì„¤ì •
- íŠ¹ì • ì„œë¹„ìŠ¤(`app=webpod`)ì˜ LoadBalancer IP(`172.16.1.1`)ê°€ ê´‘ê³  ëŒ€ìƒì´ ë¨

### **6. routerì—ì„œ LoadBalancer External IP ë¼ìš°íŠ¸ ë°˜ì˜ í™•ì¸**

```bash
Every 2.0s: sshpass -p 'vagrant' ssh vagrant@router ip -c route   k8s-ctr: Sat Aug 16 14:13:23 2025

default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.25 metric 100 
10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
172.16.1.1 nhid 105 proto bgp metric 20 
	nexthop via 192.168.10.101 dev eth1 weight 1 
	nexthop via 192.168.10.100 dev eth1 weight 1 
	nexthop via 192.168.20.100 dev eth2 weight 1 
172.20.0.0/24 nhid 92 via 192.168.10.100 dev eth1 proto bgp metric 20 
172.20.1.0/24 nhid 88 via 192.168.10.101 dev eth1 proto bgp metric 20 
172.20.2.0/24 nhid 94 via 192.168.20.100 dev eth2 proto bgp metric 20 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200 
192.168.20.0/24 dev eth2 proto kernel scope link src 192.168.20.200 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.25 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.25 metric 100
```

- ìƒˆë¡œìš´ `CiliumBGPAdvertisement` ìƒì„± í›„, ë¼ìš°í„° ë¼ìš°íŒ… í…Œì´ë¸”ì— `172.16.1.1/32` ê²½ë¡œê°€ ì¶”ê°€ë¨
- Nexthopì€ í´ëŸ¬ìŠ¤í„° ë‚´ 3ê°œ ë…¸ë“œ (`192.168.10.100`, `192.168.10.101`, `192.168.20.100`)
- ë™ì¼ ìš°ì„ ìˆœìœ„ë¡œ multipath ë¼ìš°íŒ…ì´ êµ¬ì„±ë¨

### **7. Cilium BGP Advertisement ë° ì •ì±… í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get CiliumBGPAdvertisement
```

âœ…Â **ì¶œë ¥**

```bash
NAME                                AGE
bgp-advertisements                  14h
bgp-advertisements-lb-exip-webpod   2m58s
```

- `CiliumBGPAdvertisement`ì— ìƒˆë¡œìš´ ì •ì±…(`bgp-advertisements-lb-exip-webpod`)ì´ ì¶”ê°€ë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl exec -it -n kube-system ds/cilium -- cilium-dbg bgp route-policies
VRouter   Policy Name                                             Type     Match Peers         Match Families   Match Prefixes (Min..Max Len)   RIB Action   Path Actions
65001     allow-local                                             import                                                                        accept       
65001     tor-switch-ipv4-PodCIDR                                 export   192.168.10.200/32                    172.20.1.0/24 (24..24)          accept       
65001     tor-switch-ipv4-Service-webpod-default-LoadBalancerIP   export   192.168.10.200/32                    172.16.1.1/32 (32..32)          accept
```

- ì •ì±… ì´ë¦„ì€ ì„œë¹„ìŠ¤ë³„ë¡œ ìƒì„±ë˜ë©°, ì´ë²ˆì—ëŠ” `tor-switch-ipv4-Service-webpod-default-LoadBalancerIP`
- ê²°ê³¼ì ìœ¼ë¡œ **Pod CIDR + Service LoadBalancer IP(172.16.1.1/32)** ê°€ ë™ì‹œì— export ë¨

### **8. Cilium BGP Routes í™•ì¸ (ë…¸ë“œë³„ ê´‘ê³  ìƒíƒœ)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium bgp routes available ipv4 unicast
```

âœ…Â **ì¶œë ¥**

```bash
Node      VRouter   Prefix          NextHop   Age         Attrs
k8s-ctr   65001     172.16.1.1/32   0.0.0.0   5m6s        [{Origin: i} {Nexthop: 0.0.0.0}]   
          65001     172.20.0.0/24   0.0.0.0   13h23m31s   [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w0    65001     172.16.1.1/32   0.0.0.0   5m6s        [{Origin: i} {Nexthop: 0.0.0.0}]   
          65001     172.20.2.0/24   0.0.0.0   13h23m43s   [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w1    65001     172.16.1.1/32   0.0.0.0   5m6s        [{Origin: i} {Nexthop: 0.0.0.0}]   
          65001     172.20.1.0/24   0.0.0.0   13h23m44s   [{Origin: i} {Nexthop: 0.0.0.0}] 
```

- í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  ë…¸ë“œ(k8s-ctr, k8s-w0, k8s-w1)ê°€ **`172.16.1.1/32`** ê²½ë¡œë¥¼ BGPë¡œ ê´‘ê³ 
- ë¼ìš°í„°ëŠ” ì´ë¥¼ ë°›ì•„ ëª¨ë“  ë…¸ë“œë¡œ íŠ¸ë˜í”½ì„ ë³´ë‚¼ ìˆ˜ ìˆê²Œ ë¨

### **9. router BGP í…Œì´ë¸”ì—ì„œ Multipath ë°˜ì˜ í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip route bgp'"
```

âœ…Â **ì¶œë ¥**

```bash
Codes: K - kernel route, C - connected, S - static, R - RIP,
       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,
       T - Table, v - VNC, V - VNC-Direct, A - Babel, F - PBR,
       f - OpenFabric,
       > - selected route, * - FIB route, q - queued, r - rejected, b - backup
       t - trapped, o - offload failure

B>* 172.16.1.1/32 [20/0] via 192.168.10.100, eth1, weight 1, 00:07:20
  *                      via 192.168.10.101, eth1, weight 1, 00:07:20
  *                      via 192.168.20.100, eth2, weight 1, 00:07:20
B>* 172.20.0.0/24 [20/0] via 192.168.10.100, eth1, weight 1, 00:29:43
B>* 172.20.1.0/24 [20/0] via 192.168.10.101, eth1, weight 1, 00:29:42
B>* 172.20.2.0/24 [20/0] via 192.168.20.100, eth2, weight 1, 00:29:42
```

- ë¼ìš°í„° BGP í…Œì´ë¸”ì—ì„œ **`172.16.1.1/32` ê²½ë¡œ**ëŠ” multipath ìƒíƒœë¡œ ê¸°ë¡ë¨
- `192.168.10.100`, `192.168.10.101`, `192.168.20.100` ì„¸ ë…¸ë“œê°€ ë™ì¼ í”„ë¦¬í”½ìŠ¤ë¥¼ ê´‘ê³ í–ˆê¸° ë•Œë¬¸ì— ë¼ìš°í„°ëŠ” ì„¸ ê²½ë¡œë¥¼ ëª¨ë‘ ìœ íš¨(`* valid`)ë¡œ ì¸ì‹
- BGP ê²½ë¡œ ìš°ì„ ìˆœìœ„ê°€ ë™ì¼ â†’ ì»¤ë„ ë¼ìš°íŒ… í…Œì´ë¸”ì— multipathë¡œ ë™ì‹œì— ë°˜ì˜ë¨

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp summary'"
```

âœ…Â **ì¶œë ¥**

```bash
IPv4 Unicast Summary (VRF default):
BGP router identifier 192.168.10.200, local AS number 65000 vrf-id 0
BGP table version 10
RIB entries 9, using 1728 bytes of memory
Peers 3, using 2172 KiB of memory
Peer groups 1, using 64 bytes of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc
192.168.10.100  4      65001      1284      1294        0    0    0 00:31:06            2        5 N/A
192.168.10.101  4      65001      1284      1294        0    0    0 00:31:05            2        5 N/A
192.168.20.100  4      65001      1125      1132        0    0    0 00:31:05            2        5 N/A

Total number of neighbors 3
```

- `show ip bgp summary` ì¶œë ¥ì—ì„œë„ 3ê°œì˜ í”¼ì–´ê°€ ëª¨ë‘ ë™ì¼ í”„ë¦¬í”½ìŠ¤ë¥¼ ê´‘ê³ í–ˆìŒì„ í™•ì¸

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp'"
```

âœ…Â **ì¶œë ¥**

```bash
BGP table version is 10, local router ID is 192.168.10.200, vrf id 0
Default local pref 100, local AS 65000
Status codes:  s suppressed, d damped, h history, * valid, > best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, < announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
*> 10.10.1.0/24     0.0.0.0                  0         32768 i
*= 172.16.1.1/32    192.168.20.100                         0 65001 i
*=                  192.168.10.101                         0 65001 i
*>                  192.168.10.100                         0 65001 i
*> 172.20.0.0/24    192.168.10.100                         0 65001 i
*> 172.20.1.0/24    192.168.10.101                         0 65001 i
*> 172.20.2.0/24    192.168.20.100                         0 65001 i

Displayed  5 routes and 7 total paths
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# sshpass -p 'vagrant' ssh vagrant@router "sudo vtysh -c 'show ip bgp 172.16.1.1/32'"
BGP routing table entry for 172.16.1.1/32, version 10
Paths: (3 available, best #3, table default)
  Advertised to non peer-group peers:
  192.168.10.100 192.168.10.101 192.168.20.100
  65001
    192.168.20.100 from 192.168.20.100 (192.168.20.100)
      Origin IGP, valid, external, multipath
      Last update: Sat Aug 16 14:10:56 2025
  65001
    192.168.10.101 from 192.168.10.101 (192.168.10.101)
      Origin IGP, valid, external, multipath
      Last update: Sat Aug 16 14:10:56 2025
  65001
    192.168.10.100 from 192.168.10.100 (192.168.10.100)
      Origin IGP, valid, external, multipath, best (Router ID)
      Last update: Sat Aug 16 14:10:56 2025
```

- íŠ¹ì • ê²½ë¡œ(`172.16.1.1/32`)ë§Œ ì¡°íšŒí•´ë„ multipath í•­ëª©ì´ ëª¨ë‘ í‘œì‹œë˜ë©°, `best` ê²½ë¡œëŠ” Router ID ê¸°ì¤€ìœ¼ë¡œ ì„ íƒë¨

## **ğŸ›œ routerì—ì„œ LB EX-IP í˜¸ì¶œ í™•ì¸**

### **1. routerì—ì„œ LoadBalancer External IP í˜¸ì¶œ í…ŒìŠ¤íŠ¸**

```bash
root@router:~# LBIP=172.16.1.1
curl -s $LBIP
```

âœ…Â **ì¶œë ¥**

```bash
Hostname: webpod-697b545f57-cp7xq
IP: 127.0.0.1
IP: ::1
IP: 172.20.0.15
IP: fe80::4870:31ff:fe42:a8a6
RemoteAddr: 192.168.10.200:43094
GET / HTTP/1.1
Host: 172.16.1.1
User-Agent: curl/8.5.0
Accept: */*
```

- ë¼ìš°í„°ì—ì„œ `curl` ëª…ë ¹ìœ¼ë¡œ LB External IP(`172.16.1.1`) í˜¸ì¶œ

### **2. ë¼ìš°í„°ì—ì„œ ë¶€í•˜ë¶„ì‚° ê²°ê³¼ í™•ì¸**

```bash
root@router:~# for i in {1..100};  do curl -s $LBIP | grep Hostname; done | sort | uniq -c | sort -nr
```

âœ…Â **ì¶œë ¥**

```bash
     36 Hostname: webpod-697b545f57-xtmdx
     36 Hostname: webpod-697b545f57-5twrq
     28 Hostname: webpod-697b545f57-cp7xq
```

- 100íšŒ ë°˜ë³µ í˜¸ì¶œ ì‹œ íŒŒë“œ 3ê°œë¡œ íŠ¸ë˜í”½ì´ ë¶„ì‚°ë˜ëŠ” ê²ƒì„ í™•ì¸
- ì´ëŠ” Cilium LBê°€ multipathë¥¼ í†µí•´ ì •ìƒì ìœ¼ë¡œ ë¶€í•˜ë¶„ì‚°ì„ ìˆ˜í–‰í•˜ê³  ìˆìŒì„ ì˜ë¯¸

### **3. ì‹¤ì‹œê°„ í˜¸ì¶œ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë…¸ë“œ ë¶„ì‚° í™•ì¸**

```bash
root@router:~# while true; do curl -s $LBIP | egrep 'Hostname|RemoteAddr' ; sleep 0.1; done
```

âœ…Â **ì¶œë ¥**

```bash
Hostname: webpod-697b545f57-xtmdx
RemoteAddr: 192.168.10.100:34884
Hostname: webpod-697b545f57-xtmdx
RemoteAddr: 192.168.10.100:34900
Hostname: webpod-697b545f57-5twrq
RemoteAddr: 192.168.10.100:34916
Hostname: webpod-697b545f57-5twrq
RemoteAddr: 192.168.10.100:34924
Hostname: webpod-697b545f57-cp7xq
RemoteAddr: 192.168.10.200:34940
Hostname: webpod-697b545f57-cp7xq
RemoteAddr: 192.168.10.200:34946
Hostname: webpod-697b545f57-xtmdx
RemoteAddr: 192.168.10.100:34948
Hostname: webpod-697b545f57-cp7xq
RemoteAddr: 192.168.10.200:34954
Hostname: webpod-697b545f57-5twrq
RemoteAddr: 192.168.10.100:34964
Hostname: webpod-697b545f57-xtmdx
RemoteAddr: 192.168.10.100:34966
Hostname: webpod-697b545f57-cp7xq
RemoteAddr: 192.168.10.200:34974
Hostname: webpod-697b545f57-xtmdx
RemoteAddr: 192.168.10.100:34986
Hostname: webpod-697b545f57-cp7xq
RemoteAddr: 192.168.10.200:35002
...
```

- `curl` ë°˜ë³µ í˜¸ì¶œ ì‹œ `RemoteAddr` í™•ì¸ ê²°ê³¼, ë™ì¼ LB IP ìš”ì²­ì´ ì—¬ëŸ¬ ë…¸ë“œ(`192.168.10.100`, `192.168.10.200`)ë¡œ ë¶„ì‚°ë¨
- ì¦‰, ì™¸ë¶€ì—ì„œ LB IPë¡œ ì ‘ê·¼ ì‹œ ë¼ìš°í„°ëŠ” multipath ë¼ìš°íŒ…ì„ í†µí•´ ë‹¤ìˆ˜ ë…¸ë“œë¡œ íŠ¸ë˜í”½ì„ ì „ë‹¬í•¨

### **4. Pod ìˆ˜ ì¶•ì†Œ í›„ì—ë„ ëª¨ë“  ë…¸ë“œê°€ ê´‘ê³ ë˜ëŠ” ë¬¸ì œ**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl scale deployment webpod --replicas 2
deployment.apps/webpod scaled
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get pod -owide
```

âœ…Â **ì¶œë ¥**

```bash
NAME                      READY   STATUS    RESTARTS      AGE   IP             NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   1 (14h ago)   15h   172.20.0.35    k8s-ctr   <none>           <none>
webpod-697b545f57-5twrq   1/1     Running   0             13h   172.20.1.119   k8s-w1    <none>           <none>
webpod-697b545f57-xtmdx   1/1     Running   0             13h   172.20.2.35    k8s-w0    <none>           <none>
```

- `webpod`ë¥¼ replicas=2ë¡œ ì¤„ì—¬ íŒŒë“œê°€ `k8s-w0`, `k8s-w1`ì—ë§Œ ì¡´ì¬

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# cilium bgp routes
(Defaulting to `available ipv4 unicast` routes, please see help for more options)

Node      VRouter   Prefix          NextHop   Age         Attrs
k8s-ctr   65001     172.16.1.1/32   0.0.0.0   17m5s       [{Origin: i} {Nexthop: 0.0.0.0}]   
          65001     172.20.0.0/24   0.0.0.0   13h35m30s   [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w0    65001     172.16.1.1/32   0.0.0.0   17m5s       [{Origin: i} {Nexthop: 0.0.0.0}]   
          65001     172.20.2.0/24   0.0.0.0   13h35m42s   [{Origin: i} {Nexthop: 0.0.0.0}]   
k8s-w1    65001     172.16.1.1/32   0.0.0.0   17m5s       [{Origin: i} {Nexthop: 0.0.0.0}]   
          65001     172.20.1.0/24   0.0.0.0   13h35m43s   [{Origin: i} {Nexthop: 0.0.0.0}] 
```

- ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ ëª¨ë“  ë…¸ë“œ(`k8s-ctr`, `k8s-w0`, `k8s-w1`)ê°€ `172.16.1.1/32`ë¥¼ ê´‘ê³ 
- ê·¸ ê²°ê³¼, ë¼ìš°í„°ì—ì„œ ë¶ˆí•„ìš”í•œ ë…¸ë“œ(k8s-ctr)ë¡œë„ íŠ¸ë˜í”½ì´ ì „ë‹¬ë¨ â†’ ë¹„íš¨ìœ¨ì ì¸ ê²½ë¡œ ë°œìƒ

![](https://velog.velcdn.com/images/tlsalswls123/post/5f513b38-2f22-4d1e-87f7-a202ba48b7d3/image.png)

### **5. Pod ë¶€ì¬ì—ë„ ë¼ìš°í„° ê²½ë¡œê°€ ìœ ì§€ë˜ëŠ” ë¬¸ì œ**

- í˜„ì¬ Podê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ë¼ìš°í„°ì—ëŠ” ì—¬ì „íˆ `172.16.1.1/32` ê²½ë¡œê°€ ìœ ì§€ë¨
- ì´ëŠ” ëª¨ë“  ë…¸ë“œê°€ External IPë¥¼ ê´‘ê³ í•˜ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” í˜„ìƒì„

### **6. Tcpdumpë¡œ í™•ì¸í•œ ë¶ˆí•„ìš” ê²½ë¡œ íë¦„**

```bash
tcpdump -i eth1 -A -s 0 -nn 'tcp port 80'
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/94369809-355b-4c39-9d28-7dabf5b65ccb/image.png)
- ì‹ ê·œ í„°ë¯¸ë„(k8s-w1, k8s-w2, k8s-w0)ì—ì„œ tcpdump ìˆ˜í–‰

```bash
root@router:~# LBIP=172.16.1.1
curl -s $LBIP
```
âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/8ebf4d33-d3e6-4b53-968e-7a52288b8edf/image.png)
- `172.16.1.1` í˜¸ì¶œ ì‹œ, ìš”ì²­ì´ **k8s-ctr â†’ k8s-w0**ë¡œ ì´ë™í•˜ëŠ” íŒ¨í‚·ì´ ë™ì‹œì— ê´€ì°°ë¨
- ì¦‰, Podê°€ ì—†ëŠ” ë…¸ë“œë¡œë„ íŠ¸ë˜í”½ì´ ìœ ì…ë˜ì–´ ë¶ˆí•„ìš”í•œ ê²½ë¡œê°€ ì™„ì„±ë¨

---

## **ğŸ“ ExternalTrafficPolicy(Local) ì ìš© ë° ECMP í•´ì‹œ ì •ì±… ë³€ê²½**

### **1. ExternalTrafficPolicy(Local) ì ìš©**

```bash
# ëª¨ë‹ˆí„°ë§
(âˆ|HomeLab:N/A) root@k8s-ctr:~# watch "sshpass -p 'vagrant' ssh vagrant@router ip -c route"

# k8s-ctr
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl patch service webpod -p '{"spec":{"externalTrafficPolicy":"Local"}}'
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/31e27509-04a1-422c-b8f7-1b1c5713e12e/image.png)
- `kubectl patch` ëª…ë ¹ìœ¼ë¡œ Serviceì˜ `externalTrafficPolicy`ë¥¼ **Local**ë¡œ ë³€ê²½

```bash
root@router:~# vtysh -c 'show ip bgp'
```

âœ…Â **ì¶œë ¥**

```bash
BGP table version is 11, local router ID is 192.168.10.200, vrf id 0
Default local pref 100, local AS 65000
Status codes:  s suppressed, d damped, h history, * valid, > best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, < announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found

   Network          Next Hop            Metric LocPrf Weight Path
*> 10.10.1.0/24     0.0.0.0                  0         32768 i
*= 172.16.1.1/32    192.168.20.100                         0 65001 i
*>                  192.168.10.101                         0 65001 i
*> 172.20.0.0/24    192.168.10.100                         0 65001 i
*> 172.20.1.0/24    192.168.10.101                         0 65001 i
*> 172.20.2.0/24    192.168.20.100                         0 65001 i

Displayed  5 routes and 6 total paths
```

- ë³€ê²½ ì „(`Cluster`)ì—ëŠ” **ëª¨ë“  ë…¸ë“œê°€ BGP ê´‘ê³ **ë¥¼ ìˆ˜í–‰ â†’ podê°€ ì—†ëŠ” ë…¸ë“œê¹Œì§€ ê²½ë¡œì— í¬í•¨ë¨
- ë³€ê²½ í›„(`Local`)ì—ëŠ” **podê°€ ì¡´ì¬í•˜ëŠ” ë…¸ë“œë§Œ BGP ê´‘ê³ ** â†’ ë¶ˆí•„ìš”í•œ ë¼ìš°íŒ… ì œê±°
- í˜„ì¬ëŠ” `w0`, `w1` ë…¸ë“œë§Œ ê´‘ê³ 

### **2. Linux ECMP ê¸°ë³¸ í•´ì‹œ ì •ì±…**

```bash
root@router:~# LBIP=172.16.1.1
root@router:~# for i in {1..100};  do curl -s $LBIP | grep Hostname; done | sort | uniq -c | sort -nr
    100 Hostname: webpod-697b545f57-xtmdx
```

- ë¦¬ëˆ…ìŠ¤ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **L3 ê¸°ë°˜ í•´ì‹œ ì •ì±…** ì‚¬ìš©
- ì†ŒìŠ¤/ëª©ì ì§€ IPê°€ ë™ì¼í•  ê²½ìš°, ì—¬ëŸ¬ ê²½ë¡œê°€ ìˆì–´ë„ **í•˜ë‚˜ì˜ ê²½ë¡œë§Œ ì‚¬ìš©**
- `curl` í…ŒìŠ¤íŠ¸ ì‹œ íŠ¹ì • Podë§Œ 100% ì‘ë‹µ

### **3. ECMP Hash Policy ë³€ê²½**

```bash
root@router:~# sudo sysctl -w net.ipv4.fib_multipath_hash_policy=1
echo "net.ipv4.fib_multipath_hash_policy=1" >> /etc/sysctl.conf

# ê²°ê³¼
net.ipv4.fib_multipath_hash_policy = 1
```

- `net.ipv4.fib_multipath_hash_policy=1` ì„¤ì •ìœ¼ë¡œ **L4 í¬íŠ¸ ê¸°ë°˜ í•´ì‹œ** ì ìš©
- ì†ŒìŠ¤ í¬íŠ¸ê°€ ë‹¬ë¼ì§ˆ ê²½ìš° ë‹¤ë¥¸ ê²½ë¡œë¥¼ í™œìš© ê°€ëŠ¥ â†’ **ë¶€í•˜ë¶„ì‚° ê°œì„ **

```bash
root@router:~# for i in {1..100};  do curl -s $LBIP | grep Hostname; done | sort | uniq -c | sort -nr
     54 Hostname: webpod-697b545f57-5twrq
     46 Hostname: webpod-697b545f57-xtmdx
```

### **4. Deployment í™•ì¥**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl scale deployment webpod --replicas 3

# ê²°ê³¼
deployment.apps/webpod scaled
```

- `kubectl scale` ëª…ë ¹ìœ¼ë¡œ webpodë¥¼ 3ê°œ Replicaë¡œ í™•ì¥
- ìƒˆë¡œìš´ Podê°€ ìƒì„±ë˜ë©´ í•´ë‹¹ ë…¸ë“œì—ì„œ ì¦‰ì‹œ BGP ê²½ë¡œ ê´‘ê³  ë°˜ì˜

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl get pod -owide
```

âœ…Â **ì¶œë ¥**

```bash
NAME                      READY   STATUS    RESTARTS      AGE   IP             NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   1 (14h ago)   16h   172.20.0.35    k8s-ctr   <none>           <none>
webpod-697b545f57-5twrq   1/1     Running   0             14h   172.20.1.119   k8s-w1    <none>           <none>
webpod-697b545f57-npkj5   1/1     Running   0             8s    172.20.0.159   k8s-ctr   <none>           <none>
webpod-697b545f57-xtmdx   1/1     Running   0             14h   172.20.2.35    k8s-w0    <none>           <none>
```

- k8s-ctr ë…¸ë“œì—ë„ ìƒˆë¡œìš´ Pod(`webpod-697b545f57-npkj5`)ê°€ ë°°ì¹˜ë˜ì–´ 3ê°œ Podê°€ ëª¨ë‘ ë‹¤ë¥¸ ë…¸ë“œì— ìœ„ì¹˜

### **5. ë¼ìš°íŒ… ê²½ë¡œ í™•ì¸**

```bash
root@router:~# ip -c route
```

âœ…Â **ì¶œë ¥**

```bash
default via 192.168.121.1 dev eth0 proto dhcp src 192.168.121.25 metric 100 
10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200 
10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200 
172.16.1.1 nhid 114 proto bgp metric 20 
	nexthop via 192.168.10.101 dev eth1 weight 1 
	nexthop via 192.168.10.100 dev eth1 weight 1 
	nexthop via 192.168.20.100 dev eth2 weight 1 
172.20.0.0/24 nhid 92 via 192.168.10.100 dev eth1 proto bgp metric 20 
172.20.1.0/24 nhid 88 via 192.168.10.101 dev eth1 proto bgp metric 20 
172.20.2.0/24 nhid 94 via 192.168.20.100 dev eth2 proto bgp metric 20 
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200 
192.168.20.0/24 dev eth2 proto kernel scope link src 192.168.20.200 
192.168.121.0/24 dev eth0 proto kernel scope link src 192.168.121.25 metric 100 
192.168.121.1 dev eth0 proto dhcp scope link src 192.168.121.25 metric 100 
```

- k8s-ctr ë°˜ì˜

### **6. ExternalTrafficPolicy íš¨ê³¼**

- **Cluster ëª¨ë“œ**: Podê°€ ì—†ëŠ” ë…¸ë“œë„ ê²½ìœ  ê°€ëŠ¥ â†’ ë¹„íš¨ìœ¨ì ì¸ ë¼ìš°íŒ… ë°œìƒ
- **Local ëª¨ë“œ**: ìš”ì²­ì´ ë„ë‹¬í•œ ë…¸ë“œì˜ Podì—ì„œ ì§ì ‘ ì‘ë‹µ â†’ ë¶ˆí•„ìš”í•œ hop ì œê±°

### **7. íŠ¸ë˜í”½ ë¶„ì‚° ê²€ì¦**

```bash
root@router:~# for i in {1..100};  do curl -s $LBIP | grep Hostname; done | sort | uniq -c | sort -nr
     35 Hostname: webpod-697b545f57-npkj5
     34 Hostname: webpod-697b545f57-5twrq
     31 Hostname: webpod-697b545f57-xtmdx
```

- `curl` ë°˜ë³µ í…ŒìŠ¤íŠ¸ë¡œ 100ë²ˆ ìš”ì²­ ì‹œ, **3ê°œ Podë¡œ ê· ë“±í•˜ê²Œ ë¶„ì‚°**
- ExternalTrafficPolicy(Local) + ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì¡°í•©ìœ¼ë¡œ **ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ë¶€í•˜ë¶„ì‚° ë‹¬ì„±**

**BGP + SNAT + Random â†’ ê¶Œì¥ ë°©ì‹**

- BGP(ECMP) ê¸°ë°˜ ë¼ìš°íŒ…ê³¼ K8S Service(LB EX-IP, ExternalTrafficPolicy: Local)ë¥¼ ì¡°í•©
- SNAT ì ìš© + Random ì•Œê³ ë¦¬ì¦˜ â†’ ê°€ì¥ ë‹¨ìˆœí•˜ê³  ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥ë˜ëŠ” ë°©ì‹

---

## **ğŸ§® DSR + Maglev ë°©ì‹ â†’ ê·¸ë‚˜ë§ˆ ê´œì°®ì€ ë°©ì‹**

- BGP(ECMP) + Service(LB EX-IP, ExternalTrafficPolicy: Cluster) + DSR + Maglev
- Geneve í—¤ë” encapsulation í•„ìš”

### **1. DSR ë„ì… ë°°ê²½**
- ê¸°ì¡´ L4 ë¡œë“œë°¸ëŸ°ì„œëŠ” ì¥ë¹„ ê°€ê²©ì´ ë¹„ìŒˆ
- ëŒ€ì•ˆìœ¼ë¡œ **Direct Server Return(DSR)** ê°œë… ë“±ì¥ â†’ ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ë¡œ ì§ì ‘ ì‘ë‹µì„ ë¦¬í„´
![](https://velog.velcdn.com/images/tlsalswls123/post/10186acf-e2ca-42c4-89cb-a980f0cdd5be/image.png)

### **2. Cilium ì„¤ì • í™•ì¸**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl exec -it -n kube-system ds/cilium -- cilium status --verbose
```

âœ…Â **ì¶œë ¥**

```bash
...
KubeProxyReplacement Details:
  Status:               True
  Socket LB:            Enabled
  Socket LB Tracing:    Enabled
  Socket LB Coverage:   Full
  Devices:              eth0   fe80::5054:ff:fea7:8e7a 192.168.121.62, eth1   192.168.10.101 fe80::5054:ff:fefb:b52e (Direct Routing)
  Mode:                 SNAT
  Backend Selection:    Random
  Session Affinity:     Enabled
  NAT46/64 Support:     Disabled
  XDP Acceleration:     Disabled
  Services:
  - ClusterIP:      Enabled
  - NodePort:       Enabled (Range: 30000-32767) 
  - LoadBalancer:   Enabled 
  - externalIPs:    Enabled 
  - HostPort:       Enabled
  Annotations:
  - service.cilium.io/node
  - service.cilium.io/node-selector
  - service.cilium.io/proxy-delegation
  - service.cilium.io/src-ranges-policy
  - service.cilium.io/type
...
```

- Mode: SNAT (ì´ˆê¸° ìƒíƒœ)
- Backend Selection: Random

### **3. ëª¨ë“ˆ ì¤€ë¹„ (Geneve)**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# modprobe geneve
(âˆ|HomeLab:N/A) root@k8s-ctr:~# for i in w1 w0 ; do echo ">> node : k8s-$i <<"; sshpass -p 'vagrant' ssh vagrant@k8s-$i sudo modprobe geneve ; echo; done
>> node : k8s-w1 <<

>> node : k8s-w0 <<
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# lsmod | grep -E 'vxlan|geneve'
geneve                 49152  0
ip6_udp_tunnel         16384  1 geneve
udp_tunnel             32768  1 geneve

(âˆ|HomeLab:N/A) root@k8s-ctr:~# for i in w1 w0 ; do echo ">> node : k8s-$i <<"; sshpass -p 'vagrant' ssh vagrant@k8s-$i sudo lsmod | grep -E 'vxlan|geneve' ; echo; done
>> node : k8s-w1 <<
geneve                 49152  0
ip6_udp_tunnel         16384  1 geneve
udp_tunnel             32768  1 geneve

>> node : k8s-w0 <<
geneve                 49152  0
ip6_udp_tunnel         16384  1 geneve
udp_tunnel             32768  1 geneve
```

- ëª¨ë“  ë…¸ë“œì— geneve ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ
- geneveëŠ” **ë…¸ë“œ ê°„ íŒŒë“œ í†µì‹  ì „ì²´**ì— ì“°ëŠ” ê²Œ ì•„ë‹ˆë¼, **DSR ì „ì†¡ ê²½ìœ  ì‹œ**ì—ë§Œ ì‚¬ìš©ë¨

### **4. Helmìœ¼ë¡œ Cilium DSR ëª¨ë“œ ì ìš©**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# helm upgrade cilium cilium/cilium --version 1.18.0 --namespace kube-system --reuse-values \
  --set tunnelProtocol=geneve --set loadBalancer.mode=dsr --set loadBalancer.dsrDispatch=geneve \
  --set loadBalancer.algorithm=maglev

# ê²°ê³¼
Release "cilium" has been upgraded. Happy Helming!
NAME: cilium
LAST DEPLOYED: Sat Aug 16 17:29:45 2025
NAMESPACE: kube-system
STATUS: deployed
REVISION: 3
TEST SUITE: None
NOTES:
You have successfully installed Cilium with Hubble Relay and Hubble UI.

Your release version is 1.18.0.

For any further help, visit https://docs.cilium.io/en/v1.18/gettinghelp
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl -n kube-system rollout restart ds/cilium

# ê²°ê³¼
daemonset.apps/cilium restarted
```

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl exec -it -n kube-system ds/cilium -- cilium status --verbose
```

âœ…Â **ì¶œë ¥**

```bash
...
KubeProxyReplacement Details:
  Status:                True
  Socket LB:             Enabled
  Socket LB Tracing:     Enabled
  Socket LB Coverage:    Full
  Devices:               eth0   192.168.121.62 fe80::5054:ff:fea7:8e7a, eth1   192.168.10.101 fe80::5054:ff:fefb:b52e (Direct Routing)
  Mode:                  DSR
    DSR Dispatch Mode:   Geneve
  Backend Selection:     Maglev (Table Size: 16381)
  Session Affinity:      Enabled
  NAT46/64 Support:      Disabled
  XDP Acceleration:      Disabled
  Services:
  - ClusterIP:      Enabled
  - NodePort:       Enabled (Range: 30000-32767) 
  - LoadBalancer:   Enabled 
  - externalIPs:    Enabled 
  - HostPort:       Enabled
  Annotations:
  - service.cilium.io/node
  - service.cilium.io/node-selector
  - service.cilium.io/proxy-delegation
  - service.cilium.io/src-ranges-policy
  - service.cilium.io/type
...
```

- Mode: DSR
- DSR Dispatch Mode: Geneve
- Backend Selection: Maglev

### **5. ExternalTrafficPolicy Clusterë¡œ ì›ë³µ**

```bash
(âˆ|HomeLab:N/A) root@k8s-ctr:~# kubectl patch svc webpod -p '{"spec":{"externalTrafficPolicy":"Cluster"}}'

# ê²°ê³¼
service/webpod patched
```

- ë‹¤ì‹œ Cluster ëª¨ë“œë¡œ ë³µì›

### **6. DSR íŒ¨í‚· ìº¡ì²˜**

**(1) íŒ¨í‚· ìº¡ì²˜ ì¤€ë¹„**

```bash
tcpdump -i eth1 -w /tmp/dsr.pcap
```

- k8s-ctr, k8s-w1, k8s-w0 ëª¨ë‘ tcpdump ì‹¤í–‰

**(2) ì™¸ë¶€ ì ‘ê·¼ í…ŒìŠ¤íŠ¸**

```bash
root@router:~#  curl -s $LBIP
root@router:~#  curl -s $LBIP
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/560f3d4e-061f-411d-a4c5-227ca42a0309/image.png)
- ì™¸ë¶€ ë¼ìš°í„°ì—ì„œ LBIPë¡œ curl ìš”ì²­

**(3) ìº¡ì²˜ íŒŒì¼ í™•ì¸**

```bash
vagrant plugin install vagrant-scp

# ê²°ê³¼
Installing the 'vagrant-scp' plugin. This can take a few minutes...
Building native extensions. This could take a while...
Building native extensions. This could take a while...
Fetching vagrant-scp-0.5.9.gem
Installed the plugin 'vagrant-scp (0.5.9)'!
```

- Vagrant scp í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜

```bash
vagrant scp k8s-ctr:/tmp/dsr.pcap .

# ê²°ê³¼
[fog][WARNING] Unrecognized arguments: libvirt_ip_command
Warning: Permanently added '192.168.121.70' (ED25519) to the list of known hosts.
dsr.pcap                                                         100%   94KB  46.4MB/s   00:00 
```

- pcap íŒŒì¼ì„ Hostë¡œ ì „ì†¡ í›„ Termsharkë¡œ ì—´ì–´ ë¶„ì„

```bash
termshark -r dsr.pcap
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/329e29ed-a637-4d51-8fd0-7a432ab04828/image.png)
- ì¶œë°œì§€ IP: **192.168.10.100**
- ëª©ì ì§€ IP: **192.168.20.100 (k8s-w0)**
- ì™¸ë¶€ ì ‘ê·¼ì´ ì»¨íŠ¸ë¡¤í”Œë ˆì¸ìœ¼ë¡œ ë“¤ì–´ì˜¨ ë’¤ ì›Œì»¤ ë…¸ë“œë¡œ ì „ë‹¬ëœ ê²ƒ í™•ì¸
- Geneve encapsulation í—¤ë” ì¡´ì¬

![](https://velog.velcdn.com/images/tlsalswls123/post/6db2b427-b0aa-4e8f-883d-802623af5b5c/image.png)
- ì»¨íŠ¸ë¡¤í”Œë ˆì¸(k8s-ctr)ì—ì„œ **ì™¸ë¶€ í´ë¼ì´ì–¸íŠ¸(router)** ë¡œ ë°”ë¡œ ë¦¬í„´í•˜ëŠ” íŒ¨í‚· ì¡´ì¬
- **D.IP / D.Port = ìµœì´ˆ ìš”ì²­ ì‹œ S.IP / S.Port** ê·¸ëŒ€ë¡œ ìœ ì§€ë¨
- ì¦‰, SNATì´ ì•„ë‹Œ **DSR ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ**ì´ ì „ë‹¬ë¨

---

## **ğŸ§© ClusterMesh í™˜ê²½ ì¤€ë¹„**

### **1. West í´ëŸ¬ìŠ¤í„° ìƒì„±**

```bash
kind create cluster --name west --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000 # sample apps
    hostPort: 30000
  - containerPort: 30001 # hubble ui
    hostPort: 30001
- role: worker
  extraPortMappings:
  - containerPort: 30002 # sample apps
    hostPort: 30002
networking:
  podSubnet: "10.0.0.0/16"
  serviceSubnet: "10.2.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# ê²°ê³¼
Creating cluster "west" ...
 âœ“ Ensuring node image (kindest/node:v1.33.2) ğŸ–¼ 
 âœ“ Preparing nodes ğŸ“¦ ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing StorageClass ğŸ’¾ 
 âœ“ Joining worker nodes ğŸšœ 
Set kubectl context to "kind-west"
You can now use your cluster with:

kubectl cluster-info --context kind-west

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community ğŸ™‚
```

### **2. East í´ëŸ¬ìŠ¤í„° ìƒì„±**

```bash
kind create cluster --name east --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 31000 # sample apps
    hostPort: 31000
  - containerPort: 31001 # hubble ui
    hostPort: 31001
- role: worker
  extraPortMappings:
  - containerPort: 31002 # sample apps
    hostPort: 31002
networking:
  podSubnet: "10.1.0.0/16"
  serviceSubnet: "10.3.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# ê²°ê³¼
Creating cluster "east" ...
 âœ“ Ensuring node image (kindest/node:v1.33.2) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦ ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing StorageClass ğŸ’¾ 
 âœ“ Joining worker nodes ğŸšœ 
Set kubectl context to "kind-east"
You can now use your cluster with:

kubectl cluster-info --context kind-east

Not sure what to do next? ğŸ˜…  Check out https://kind.sigs.k8s.io/docs/user/quick-start/
```

### **3. ì»¨í…ìŠ¤íŠ¸ í™•ì¸**

```bash
kubectl config get-contexts 
```

âœ…Â **ì¶œë ¥**

```bash
CURRENT   NAME        CLUSTER     AUTHINFO    NAMESPACE
*         kind-east   kind-east   kind-east   
          kind-west   kind-west   kind-west   
```

- ë‘ í´ëŸ¬ìŠ¤í„°ê°€ kubeconfigì— ì •ìƒ ë“±ë¡ë¨ í™•ì¸

### **4. ë…¸ë“œ ê¸°ë³¸ íˆ´ ì„¤ì¹˜**

```bash
docker exec -it west-control-plane sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'
docker exec -it west-worker sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'
docker exec -it east-control-plane sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'
docker exec -it east-worker sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'
```

### **5. Context ì „í™˜ ë° ë…¸ë“œ ì¡°íšŒ**

```bash
kubectl config set-context kind-east

# ê²°ê³¼
Context "kind-east" modified.
```

- ê¸°ë³¸ contextë¥¼ eastë¡œ ë³€ê²½

```bash
kubectl get node -v=6 --context kind-east
```

âœ…Â **ì¶œë ¥**

```bash
I0816 18:18:58.547416  180043 loader.go:402] Config loaded from file:  /home/devshin/.kube/config
I0816 18:18:58.547878  180043 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0816 18:18:58.547890  180043 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0816 18:18:58.547898  180043 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0816 18:18:58.547901  180043 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0816 18:18:58.561481  180043 round_trippers.go:560] GET https://127.0.0.1:38631/api?timeout=32s 200 OK in 13 milliseconds
I0816 18:18:58.564716  180043 round_trippers.go:560] GET https://127.0.0.1:38631/apis?timeout=32s 200 OK in 1 milliseconds
I0816 18:18:58.598067  180043 round_trippers.go:560] GET https://127.0.0.1:38631/api/v1/nodes?limit=500 200 OK in 14 milliseconds
NAME                 STATUS     ROLES           AGE     VERSION
east-control-plane   NotReady   control-plane   7m31s   v1.33.2
east-worker          NotReady   <none>          7m20s   v1.33.2
```

```bash
kubectl get node -v=6
```

âœ…Â **ì¶œë ¥**

```bash
I0816 18:19:58.446920  180152 loader.go:402] Config loaded from file:  /home/devshin/.kube/config
I0816 18:19:58.449078  180152 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0816 18:19:58.449176  180152 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0816 18:19:58.449218  180152 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0816 18:19:58.449263  180152 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0816 18:19:58.465153  180152 round_trippers.go:560] GET https://127.0.0.1:38631/api/v1/nodes?limit=500 200 OK in 7 milliseconds
NAME                 STATUS     ROLES           AGE     VERSION
east-control-plane   NotReady   control-plane   8m31s   v1.33.2
east-worker          NotReady   <none>          8m20s   v1.33.2
```

- east í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ì •ë³´ ì¶œë ¥

```bash
kubectl get node -v=6 --context kind-west
```

âœ…Â **ì¶œë ¥**

```bash
I0816 18:21:10.714221  180273 loader.go:402] Config loaded from file:  /home/devshin/.kube/config
I0816 18:21:10.719211  180273 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0816 18:21:10.719796  180273 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0816 18:21:10.719902  180273 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0816 18:21:10.719986  180273 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0816 18:21:10.741286  180273 round_trippers.go:560] GET https://127.0.0.1:43999/api?timeout=32s 200 OK in 19 milliseconds
I0816 18:21:10.743285  180273 round_trippers.go:560] GET https://127.0.0.1:43999/apis?timeout=32s 200 OK in 1 milliseconds
I0816 18:21:10.751821  180273 round_trippers.go:560] GET https://127.0.0.1:43999/api/v1/nodes?limit=500 200 OK in 4 milliseconds
NAME                 STATUS     ROLES           AGE   VERSION
west-control-plane   NotReady   control-plane   10m   v1.33.2
west-worker          NotReady   <none>          10m   v1.33.2
```

- `--context` ì˜µì…˜ì„ ì£¼ë©´ west í´ëŸ¬ìŠ¤í„° ì •ë³´ë„ í™•ì¸ ê°€ëŠ¥

### **6. ë…¸ë“œ ìƒíƒœ í™•ì¸**

```bash
kubectl get pod -A
```

âœ…Â **ì¶œë ¥**

```bash
NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
kube-system          coredns-674b8bbfcf-2tm2l                     0/1     Pending   0          10m
kube-system          coredns-674b8bbfcf-c9qsg                     0/1     Pending   0          10m
kube-system          etcd-east-control-plane                      1/1     Running   0          10m
kube-system          kube-apiserver-east-control-plane            1/1     Running   0          10m
kube-system          kube-controller-manager-east-control-plane   1/1     Running   0          10m
kube-system          kube-scheduler-east-control-plane            1/1     Running   0          10m
local-path-storage   local-path-provisioner-7dc846544d-mwfdc      0/1     Pending   0          10m
```

- í˜„ì¬ kube-proxyì™€ CNIë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— **ëª¨ë“  ë…¸ë“œ ìƒíƒœëŠ” NotReady**
- System Pod(coredns, local-path-provisioner ë“±)ë„ **Pending ìƒíƒœ**

### **7. Context Alias ì„¤ì •**

```bash
alias kwest='kubectl --context kind-west'
alias keast='kubectl --context kind-east'
```

### **8. ë…¸ë“œ ìƒì„¸ ì •ë³´**

```bash
kwest get node -owide

NAME                 STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME
west-control-plane   NotReady   control-plane   14m   v1.33.2   172.18.0.2    <none>        Debian GNU/Linux 12 (bookworm)   6.16.0-arch2-1   containerd://2.1.3
west-worker          NotReady   <none>          14m   v1.33.2   172.18.0.3    <none>        Debian GNU/Linux 12 (bookworm)   6.16.0-arch2-1   containerd://2.1.3
```

```bash
keast get node -owide

NAME                 STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME
east-control-plane   NotReady   control-plane   13m   v1.33.2   172.18.0.4    <none>        Debian GNU/Linux 12 (bookworm)   6.16.0-arch2-1   containerd://2.1.3
east-worker          NotReady   <none>          13m   v1.33.2   172.18.0.5    <none>        Debian GNU/Linux 12 (bookworm)   6.16.0-arch2-1   containerd://2.1.3
```

---

## **ğŸ•¸ï¸ Cilium CNI ë°°í¬ : ClusterMesh**
- [https://docs.cilium.io/en/stable/network/clustermesh/clustermesh/](https://docs.cilium.io/en/stable/network/clustermesh/clustermesh/)

### **1. Cilium CLI ì„¤ì¹˜**

```bash
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi

curl -L --fail --remote-name-all \
  https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}  

sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
```

âœ…Â **ì¶œë ¥**

```bash
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 56.6M  100 56.6M    0     0  16.1M      0  0:00:03  0:00:03 --:--:-- 18.2M

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100    92  100    92    0     0    338      0 --:--:-- --:--:-- --:--:--   338

cilium-linux-amd64.tar.gz: OK
cilium
```

- Host OSì— `cilium` CLI ì„¤ì¹˜

### **2. West í´ëŸ¬ìŠ¤í„°ì— Cilium ì„¤ì¹˜**

```bash
cilium install --version 1.17.6 --set ipam.mode=kubernetes \
--set kubeProxyReplacement=true --set bpf.masquerade=true \
--set endpointHealthChecking.enabled=false --set healthChecking=false \
--set operator.replicas=1 --set debug.enabled=true \
--set routingMode=native --set autoDirectNodeRoutes=true --set ipv4NativeRoutingCIDR=10.0.0.0/16 \
--set ipMasqAgent.enabled=true --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.1.0.0/16}' \
--set cluster.name=west --set cluster.id=1 \
--context kind-west

# ê²°ê³¼   
ğŸ”® Auto-detected Kubernetes kind: kind
â„¹ï¸  Using Cilium version 1.17.6
â„¹ï¸  Using cluster name "west"
â„¹ï¸  Detecting real Kubernetes API server addr and port on Kind
ğŸ”® Auto-detected kube-proxy has not been installed
â„¹ï¸  Cilium will fully replace all functionalities of kube-proxy
```

- `cilium install` ëª…ë ¹ì–´ë¡œ **1.17.6 ë²„ì „** ì„¤ì¹˜
- `routingMode=native`, `autoDirectNodeRoutes=true`, `ipv4NativeRoutingCIDR=10.0.0.0/16`
- `cluster.name=west`, `cluster.id=1`

### **3. East í´ëŸ¬ìŠ¤í„°ì— Cilium ì„¤ì¹˜**

```bash
cilium install --version 1.17.6 --set ipam.mode=kubernetes \
--set kubeProxyReplacement=true --set bpf.masquerade=true \
--set endpointHealthChecking.enabled=false --set healthChecking=false \
--set operator.replicas=1 --set debug.enabled=true \
--set routingMode=native --set autoDirectNodeRoutes=true --set ipv4NativeRoutingCIDR=10.1.0.0/16 \
--set ipMasqAgent.enabled=true --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.0.0.0/16}' \
--set cluster.name=east --set cluster.id=2 \
--context kind-east

# ê²°ê³¼
ğŸ”® Auto-detected Kubernetes kind: kind
â„¹ï¸  Using Cilium version 1.17.6
â„¹ï¸  Using cluster name "east"
â„¹ï¸  Detecting real Kubernetes API server addr and port on Kind
ğŸ”® Auto-detected kube-proxy has not been installed
â„¹ï¸  Cilium will fully replace all functionalities of kube-proxy
```

- `routingMode=native`, `autoDirectNodeRoutes=true`, `ipv4NativeRoutingCIDR=10.1.0.0/16`
- `cluster.name=east`, `cluster.id=2`

### **4. ì„¤ì¹˜ í™•ì¸ (West/East Pod ìƒíƒœ)**

```bash
kwest get pod -A && keast get pod -A
```

âœ…Â **ì¶œë ¥**

```bash
NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
kube-system          cilium-9jlht                                 1/1     Running   0          2m22s
kube-system          cilium-envoy-5gpxx                           1/1     Running   0          2m22s
kube-system          cilium-envoy-skv7b                           1/1     Running   0          2m22s
kube-system          cilium-operator-7dbb574d5b-drtg2             1/1     Running   0          2m22s
kube-system          cilium-qvpkv                                 1/1     Running   0          2m22s
kube-system          coredns-674b8bbfcf-kwxv5                     1/1     Running   0          34m
kube-system          coredns-674b8bbfcf-nb96t                     1/1     Running   0          34m
kube-system          etcd-west-control-plane                      1/1     Running   0          34m
kube-system          kube-apiserver-west-control-plane            1/1     Running   0          34m
kube-system          kube-controller-manager-west-control-plane   1/1     Running   0          34m
kube-system          kube-scheduler-west-control-plane            1/1     Running   0          34m
local-path-storage   local-path-provisioner-7dc846544d-jrdw8      1/1     Running   0          34m

NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
kube-system          cilium-envoy-mrzw8                           1/1     Running   0          94s
kube-system          cilium-envoy-vq5r7                           1/1     Running   0          94s
kube-system          cilium-kxddr                                 1/1     Running   0          94s
kube-system          cilium-operator-867f8dc978-44zqb             1/1     Running   0          94s
kube-system          cilium-qn52j                                 1/1     Running   0          94s
kube-system          coredns-674b8bbfcf-2tm2l                     1/1     Running   0          33m
kube-system          coredns-674b8bbfcf-c9qsg                     1/1     Running   0          33m
kube-system          etcd-east-control-plane                      1/1     Running   0          33m
kube-system          kube-apiserver-east-control-plane            1/1     Running   0          33m
kube-system          kube-controller-manager-east-control-plane   1/1     Running   0          33m
kube-system          kube-scheduler-east-control-plane            1/1     Running   0          33m
local-path-storage   local-path-provisioner-7dc846544d-mwfdc      1/1     Running   0          33m
```

- **Cilium DaemonSet, Envoy, Operator** ì •ìƒ ì‹¤í–‰ (Running)
- CoreDNS, etcd, apiserver, controller, scheduler, local-path-provisioner ëª¨ë‘ ì •ìƒ

```bash
cilium status --context kind-west
```

âœ…Â **ì¶œë ¥**

```bash
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    OK
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium                   Desired: 2, Ready: 2/2, Available: 2/2
DaemonSet              cilium-envoy             Desired: 2, Ready: 2/2, Available: 2/2
Deployment             cilium-operator          Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium                   Running: 2
                       cilium-envoy             Running: 2
                       cilium-operator          Running: 1
                       clustermesh-apiserver    
                       hubble-relay             
Cluster Pods:          3/3 managed by Cilium
Helm chart version:    1.17.6
Image versions         cilium             quay.io/cilium/cilium:v1.17.6@sha256:544de3d4fed7acba72758413812780a4972d47c39035f2a06d6145d8644a3353: 2
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.33.4-1752151664-7c2edb0b44cf95f326d628b837fcdd845102ba68@sha256:318eff387835ca2717baab42a84f35a83a5f9e7d519253df87269f80b9ff0171: 2
                       cilium-operator    quay.io/cilium/operator-generic:v1.17.6@sha256:91ac3bf7be7bed30e90218f219d4f3062a63377689ee7246062fa0cc3839d096: 1
```

```bash
cilium status --context kind-east
```

âœ…Â **ì¶œë ¥**

```bash
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    OK
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium                   Desired: 2, Ready: 2/2, Available: 2/2
DaemonSet              cilium-envoy             Desired: 2, Ready: 2/2, Available: 2/2
Deployment             cilium-operator          Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium                   Running: 2
                       cilium-envoy             Running: 2
                       cilium-operator          Running: 1
                       clustermesh-apiserver    
                       hubble-relay             
Cluster Pods:          3/3 managed by Cilium
Helm chart version:    1.17.6
Image versions         cilium             quay.io/cilium/cilium:v1.17.6@sha256:544de3d4fed7acba72758413812780a4972d47c39035f2a06d6145d8644a3353: 2
                       cilium-envoy       quay.io/cilium/cilium-envoy:v1.33.4-1752151664-7c2edb0b44cf95f326d628b837fcdd845102ba68@sha256:318eff387835ca2717baab42a84f35a83a5f9e7d519253df87269f80b9ff0171: 2
                       cilium-operator    quay.io/cilium/operator-generic:v1.17.6@sha256:91ac3bf7be7bed30e90218f219d4f3062a63377689ee7246062fa0cc3839d096: 1
```

- `DaemonSet`, `Envoy`, `Operator` â†’ Desiredì™€ Ready ìˆ˜ ì¼ì¹˜ (ì •ìƒ ë°°í¬ ì™„ë£Œ)
- ClusterMesh, Hubble Relay â†’ ì•„ì§ **disabled ìƒíƒœ**

### **5. IP Masquerading ì„¤ì • í™•ì¸**

```bash
kwest -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg bpf ipmasq list
```

âœ…Â **ì¶œë ¥**

```bash
IP PREFIX/ADDRESS   
10.1.0.0/16              
169.254.0.0/16  
```

```bash
keast -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg bpf ipmasq list
```

âœ…Â **ì¶œë ¥**

```bash
IP PREFIX/ADDRESS   
10.0.0.0/16              
169.254.0.0/16           
```

- ì„œë¡œ ìƒëŒ€ í´ëŸ¬ìŠ¤í„°ì˜ Pod CIDR ëŒ€ì—­ì´ **non-masquerade ì²˜ë¦¬** ë˜ì–´ ìˆìŒ

### **6. CoreDNS ì„¤ì • í™•ì¸**

```bash
kubectl describe cm -n kube-system coredns --context kind-west | grep kubernetes
```

âœ…Â **ì¶œë ¥**

```bash
    kubernetes cluster.local in-addr.arpa ip6.arpa {
```

```bash
kubectl describe cm -n kube-system coredns --context kind-west | grep kubernetes
```

âœ…Â **ì¶œë ¥**

```bash
    kubernetes cluster.local in-addr.arpa ip6.arpa {
```

- `cluster.local` ë„ë©”ì¸ ì‚¬ìš©
- ConfigMap ì¶œë ¥ì—ì„œ `kubernetes cluster.local in-addr.arpa ip6.arpa` í™•ì¸

---

## **ğŸš€ Setting up Cluster Mesh**
- [https://docs.cilium.io/en/stable/network/clustermesh/clustermesh/](https://docs.cilium.io/en/stable/network/clustermesh/clustermesh/)

### **1. ì´ˆê¸° ìƒíƒœ: ìƒí˜¸ Pod CIDR ì•Œ ìˆ˜ ì—†ìŒ**

```bash
docker exec -it west-control-plane ip -c route
docker exec -it west-worker ip -c route
docker exec -it east-control-plane ip -c route
docker exec -it east-worker ip -c route
```

âœ…Â **ì¶œë ¥**

```bash
default via 172.18.0.1 dev eth0 
10.0.0.0/24 via 10.0.0.19 dev cilium_host proto kernel src 10.0.0.19 
10.0.0.19 dev cilium_host proto kernel scope link 
10.0.1.0/24 via 172.18.0.3 dev eth0 proto kernel 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.2

default via 172.18.0.1 dev eth0 
10.0.0.0/24 via 172.18.0.2 dev eth0 proto kernel 
10.0.1.0/24 via 10.0.1.99 dev cilium_host proto kernel src 10.0.1.99 
10.0.1.99 dev cilium_host proto kernel scope link 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.3

default via 172.18.0.1 dev eth0 
10.1.0.0/24 via 10.1.0.165 dev cilium_host proto kernel src 10.1.0.165 
10.1.0.165 dev cilium_host proto kernel scope link 
10.1.1.0/24 via 172.18.0.5 dev eth0 proto kernel 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.4

default via 172.18.0.1 dev eth0
10.1.0.0/24 via 172.18.0.4 dev eth0 proto kernel 
10.1.1.0/24 via 10.1.1.122 dev cilium_host proto kernel src 10.1.1.122 
10.1.1.122 dev cilium_host proto kernel scope link 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.5
```

- `kind` í™˜ê²½ì€ Docker ë„¤íŠ¸ì›Œí¬ ìœ„ì—ì„œ ë™ì‘í•˜ê³  BGPë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- ë”°ë¼ì„œ **West â†” East í´ëŸ¬ìŠ¤í„° ê°„ Pod CIDR ë¼ìš°íŒ… ì •ë³´ê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŒ**
- `ip route` í™•ì¸ ê²°ê³¼, ê° í´ëŸ¬ìŠ¤í„°ëŠ” ìê¸° Pod CIDRë§Œ ì•Œê³  ìˆê³  ìƒëŒ€ë°©ì˜ Pod CIDRì€ ì•Œ ìˆ˜ ì—†ìŒ

### **2. CA Secret ë™ê¸°í™”**

```bash
keast get secret -n kube-system cilium-ca
NAME        TYPE     DATA   AGE
cilium-ca   Opaque   2      14m

keast delete secret -n kube-system cilium-ca 
secret "cilium-ca" deleted

keast get secret -n kube-system cilium-ca
Error from server (NotFound): secrets "cilium-ca" not found
```

- `east` í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ë³¸ `cilium-ca` Secret ì‚­ì œ

```bash
kubectl --context kind-west get secret -n kube-system cilium-ca -o yaml | \
kubectl --context kind-east create -f -

# ê²°ê³¼
secret/cilium-ca created
```

- `west` í´ëŸ¬ìŠ¤í„°ì˜ `cilium-ca` Secretì„ `east` í´ëŸ¬ìŠ¤í„°ë¡œ ê°€ì ¸ì™€ ìƒì„±

```bash
keast get secret -n kube-system cilium-ca
NAME        TYPE     DATA   AGE
cilium-ca   Opaque   2      62s
```

- ì–‘ìª½ í´ëŸ¬ìŠ¤í„°ê°€ ë™ì¼í•œ CAë¥¼ ê³µìœ í•˜ì—¬ **ìƒí˜¸ TLS ì¸ì¦ ê¸°ë°˜ í†µì‹  ê°€ëŠ¥**

### **3. ClusterMesh ìƒíƒœ ëª¨ë‹ˆí„°ë§**

```bash
cilium clustermesh status --context kind-west --wait  
cilium clustermesh status --context kind-east --wait
```

### **4. ClusterMesh í™œì„±í™”**

```bash
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-west
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/c37c4609-950e-4b2c-af07-b98e63f449d0/image.png)

```bash
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-east
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/3aa92f7e-32fd-4df2-a179-2330ab6424aa/image.png)
- ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ `cilium clustermesh enable` ì‹¤í–‰

### **5. West í´ëŸ¬ìŠ¤í„° Service/Endpoint í™•ì¸**

```bash
kwest get svc,ep -n kube-system clustermesh-apiserver --context kind-west
```

âœ…Â **ì¶œë ¥**

```bash
Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
NAME                            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/clustermesh-apiserver   NodePort   10.2.126.217   <none>        2379:32379/TCP   2m57s

NAME                              ENDPOINTS       AGE
endpoints/clustermesh-apiserver   10.0.1.8:2379   2m57s
```

- `clustermesh-apiserver` Service ìƒì„±, íƒ€ì…ì€ **NodePort**
- ClusterIP: `10.2.126.217`, Port: `2379:32379/TCP`
- `NodePort (32379)`ë¡œ ì™¸ë¶€ ë…¸ë“œ ê°„ í†µì‹  ì±„ë„ ê°œë°©
- Endpoint: `10.0.1.8:2379`

### **6. West í´ëŸ¬ìŠ¤í„° Pod ìƒíƒœ í™•ì¸**

```bash
kwest get pod -n kube-system -owide | grep clustermesh
```

âœ…Â **ì¶œë ¥**

```bash
clustermesh-apiserver-5cf45db9cc-2g847       2/2     Running     0          4m29s   10.0.1.8     west-worker          <none>           <none>
clustermesh-apiserver-generate-certs-pl6ws   0/1     Completed   0          4m29s   172.18.0.3   west-worker          <none>           <none>
```

### **7. East í´ëŸ¬ìŠ¤í„° Service/Endpoint í™•ì¸**

```bash
keast get svc,ep -n kube-system clustermesh-apiserver --context kind-east
```

âœ…Â **ì¶œë ¥**

```bash
Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
NAME                            TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
service/clustermesh-apiserver   NodePort   10.3.173.28   <none>        2379:32379/TCP   3m47s

NAME                              ENDPOINTS        AGE
endpoints/clustermesh-apiserver   10.1.1.62:2379   3m47s
```

- ClusterIP: `10.3.173.28`, Port: `2379:32379/TCP`
- Endpoint: `10.1.1.62:2379`

### **8. ClusterMesh ìƒíƒœ ëª¨ë‹ˆí„°ë§**

```bash
watch -d "cilium clustermesh status --context kind-west --wait"
watch -d "cilium clustermesh status --context kind-east --wait"
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/537aa8ea-e494-4208-a937-866f0b056234/image.png)

### **9. ClusterMesh ì—°ê²° ì‹¤í–‰ (West â†’ East)**

```bash
cilium clustermesh connect --context kind-west --destination-context kind-east
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/f4142327-5f1f-4a52-b1c5-e040cb1dc8f0/image.png)

### **10. ClusterMesh ì •ìƒ ì—°ê²° í™•ì¸ (West/East)**

```bash
cilium status --context kind-west
```

âœ…Â **ì¶œë ¥**

```bash
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    OK
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        OK

DaemonSet              cilium                   Desired: 2, Ready: 2/2, Available: 2/2
DaemonSet              cilium-envoy             Desired: 2, Ready: 2/2, Available: 2/2
Deployment             cilium-operator          Desired: 1, Ready: 1/1, Available: 1/1
Deployment             clustermesh-apiserver    Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium                   Running: 2
                       cilium-envoy             Running: 2
                       cilium-operator          Running: 1
                       clustermesh-apiserver    Running: 1
                       hubble-relay             
Cluster Pods:          4/4 managed by Cilium
Helm chart version:    1.17.6
Image versions         cilium                   quay.io/cilium/cilium:v1.17.6@sha256:544de3d4fed7acba72758413812780a4972d47c39035f2a06d6145d8644a3353: 2
                       cilium-envoy             quay.io/cilium/cilium-envoy:v1.33.4-1752151664-7c2edb0b44cf95f326d628b837fcdd845102ba68@sha256:318eff387835ca2717baab42a84f35a83a5f9e7d519253df87269f80b9ff0171: 2
                       cilium-operator          quay.io/cilium/operator-generic:v1.17.6@sha256:91ac3bf7be7bed30e90218f219d4f3062a63377689ee7246062fa0cc3839d096: 1
                       clustermesh-apiserver    quay.io/cilium/clustermesh-apiserver:v1.17.6@sha256:f619e97432db427e1511bf91af3be8ded418c53a353a09629e04c5880659d1df: 2
```

```bash
cilium status --context kind-east
```

âœ…Â **ì¶œë ¥**

```bash
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    OK
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        OK

DaemonSet              cilium                   Desired: 2, Ready: 2/2, Available: 2/2
DaemonSet              cilium-envoy             Desired: 2, Ready: 2/2, Available: 2/2
Deployment             cilium-operator          Desired: 1, Ready: 1/1, Available: 1/1
Deployment             clustermesh-apiserver    Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium                   Running: 2
                       cilium-envoy             Running: 2
                       cilium-operator          Running: 1
                       clustermesh-apiserver    Running: 1
                       hubble-relay             
Cluster Pods:          4/4 managed by Cilium
Helm chart version:    1.17.6
Image versions         cilium                   quay.io/cilium/cilium:v1.17.6@sha256:544de3d4fed7acba72758413812780a4972d47c39035f2a06d6145d8644a3353: 2
                       cilium-envoy             quay.io/cilium/cilium-envoy:v1.33.4-1752151664-7c2edb0b44cf95f326d628b837fcdd845102ba68@sha256:318eff387835ca2717baab42a84f35a83a5f9e7d519253df87269f80b9ff0171: 2
                       cilium-operator          quay.io/cilium/operator-generic:v1.17.6@sha256:91ac3bf7be7bed30e90218f219d4f3062a63377689ee7246062fa0cc3839d096: 1
                       clustermesh-apiserver    quay.io/cilium/clustermesh-apiserver:v1.17.6@sha256:f619e97432db427e1511bf91af3be8ded418c53a353a09629e04c5880659d1df: 2
```

- `cilium status` í™•ì¸ ê²°ê³¼, ì–‘ìª½ í´ëŸ¬ìŠ¤í„° ëª¨ë‘ **ClusterMesh: OK** ì¶œë ¥
- DaemonSet, Envoy, Operator, clustermesh-apiserver ëª¨ë‘ Running ìƒíƒœì´ë©°, ì—°ê²° ì•ˆì •ì 

### **11. ìƒì„¸ ìƒíƒœ í™•ì¸ (Verbose Mode)**

```bash
kwest exec -it -n kube-system ds/cilium -- cilium status --verbose
```

âœ…Â **ì¶œë ¥**

```bash
...
ClusterMesh:   1/1 remote clusters ready, 0 global-services
   east: ready, 2 nodes, 4 endpoints, 3 identities, 0 services, 0 MCS-API service exports, 0 reconnections (last: never)
   â””  etcd: 1/1 connected, leases=0, lock leases=0, has-quorum=true: endpoint status checks are disabled, ID: b88364e6e9ad8658
   â””  remote configuration: expected=true, retrieved=true, cluster-id=2, kvstoremesh=false, sync-canaries=true, service-exports=disabled
   â””  synchronization status: nodes=true, endpoints=true, identities=true, services=true
...   
```

```bash
keast exec -it -n kube-system ds/cilium -- cilium status --verbose
```

âœ…Â **ì¶œë ¥**

```bash
...
ClusterMesh:   1/1 remote clusters ready, 0 global-services
   west: ready, 2 nodes, 4 endpoints, 3 identities, 0 services, 0 MCS-API service exports, 0 reconnections (last: never)
   â””  etcd: 1/1 connected, leases=0, lock leases=0, has-quorum=true: endpoint status checks are disabled, ID: 700452e5b45c47e8
   â””  remote configuration: expected=true, retrieved=true, cluster-id=1, kvstoremesh=false, sync-canaries=true, service-exports=disabled
   â””  synchronization status: nodes=true, endpoints=true, identities=true, services=true
...
```

- west í´ëŸ¬ìŠ¤í„°ì—ì„œ east, east í´ëŸ¬ìŠ¤í„°ì—ì„œ west ìƒíƒœë¥¼ ìƒì„¸ ì¡°íšŒ
- ë™ê¸°í™” í•­ëª©: `nodes=true`, `endpoints=true`, `identities=true`, `services=true` â†’ ë™ê¸°í™” ì •ìƒ
- etcd ì—°ê²°ë„ 1/1 connected, quorum í™•ë³´ë¨

### **12. Pod CIDR ë¼ìš°íŒ… ìë™ ì£¼ì… í™•ì¸**

```bash
docker exec -it west-control-plane ip -c route
docker exec -it west-worker ip -c route
docker exec -it east-control-plane ip -c route
docker exec -it east-worker ip -c route
```

âœ…Â **ì¶œë ¥**

```bash
default via 172.18.0.1 dev eth0 
10.0.0.0/24 via 10.0.0.19 dev cilium_host proto kernel src 10.0.0.19 
10.0.0.19 dev cilium_host proto kernel scope link 
10.0.1.0/24 via 172.18.0.3 dev eth0 proto kernel 
10.1.0.0/24 via 172.18.0.4 dev eth0 proto kernel 
10.1.1.0/24 via 172.18.0.5 dev eth0 proto kernel 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.2

default via 172.18.0.1 dev eth0 
10.0.0.0/24 via 172.18.0.2 dev eth0 proto kernel 
10.0.1.0/24 via 10.0.1.99 dev cilium_host proto kernel src 10.0.1.99 
10.0.1.99 dev cilium_host proto kernel scope link 
10.1.0.0/24 via 172.18.0.4 dev eth0 proto kernel 
10.1.1.0/24 via 172.18.0.5 dev eth0 proto kernel 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.3 

default via 172.18.0.1 dev eth0 
10.0.0.0/24 via 172.18.0.2 dev eth0 proto kernel 
10.0.1.0/24 via 172.18.0.3 dev eth0 proto kernel 
10.1.0.0/24 via 10.1.0.165 dev cilium_host proto kernel src 10.1.0.165 
10.1.0.165 dev cilium_host proto kernel scope link 
10.1.1.0/24 via 172.18.0.5 dev eth0 proto kernel 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.4 

default via 172.18.0.1 dev eth0 
10.0.0.0/24 via 172.18.0.2 dev eth0 proto kernel 
10.0.1.0/24 via 172.18.0.3 dev eth0 proto kernel 
10.1.0.0/24 via 172.18.0.4 dev eth0 proto kernel 
10.1.1.0/24 via 10.1.1.122 dev cilium_host proto kernel src 10.1.1.122 
10.1.1.122 dev cilium_host proto kernel scope link 
172.18.0.0/16 dev eth0 proto kernel scope link src 172.18.0.5 
```

- ClusterMesh ì—°ê²° í›„, ê° í´ëŸ¬ìŠ¤í„°ì˜ ë…¸ë“œ ë¼ìš°íŒ… í…Œì´ë¸”ì— ìƒëŒ€ í´ëŸ¬ìŠ¤í„°ì˜ PodCIDR ìë™ ì£¼ì… í™•ì¸
- ê²°ê³¼ì ìœ¼ë¡œ, **ì–‘ìª½ í´ëŸ¬ìŠ¤í„° Pod ê°„ ì§ì ‘ í†µì‹  ê°€ëŠ¥** ìƒíƒœë¡œ ì „í™˜ë¨

---

## **ğŸ‘ï¸ Hubble enable**

### **1. Cilium Helm ì €ì¥ì†Œ ì¶”ê°€ ë° ì—…ë°ì´íŠ¸**

```bash
helm repo add cilium https://helm.cilium.io/
helm repo update
```

âœ…Â **ì¶œë ¥**

```bash
"cilium" has been added to your repositories

Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "cilium" chart repository
Update Complete. âˆHappy Helming!âˆ
```

### **2. West í´ëŸ¬ìŠ¤í„°ì— Hubble í™œì„±í™”**

```bash
helm upgrade cilium cilium/cilium --version 1.17.6 --namespace kube-system --reuse-values \
--set hubble.enabled=true --set hubble.relay.enabled=true --set hubble.ui.enabled=true \
--set hubble.ui.service.type=NodePort --set hubble.ui.service.nodePort=30001 --kube-context kind-west
```

âœ…Â **ì¶œë ¥**

```bash
Release "cilium" has been upgraded. Happy Helming!
NAME: cilium
LAST DEPLOYED: Sat Aug 16 19:29:04 2025
NAMESPACE: kube-system
STATUS: deployed
REVISION: 4
TEST SUITE: None
NOTES:
You have successfully installed Cilium with Hubble Relay and Hubble UI.

Your release version is 1.17.6.

For any further help, visit https://docs.cilium.io/en/v1.17/gettinghelp
```

### **3. West í´ëŸ¬ìŠ¤í„° Cilium DaemonSet ì¬ì‹œì‘**

```bash
kwest -n kube-system rollout restart ds/cilium

# ê²°ê³¼
daemonset.apps/cilium restarted
```

### **4. East í´ëŸ¬ìŠ¤í„°ì— Hubble í™œì„±í™”**

```bash
helm upgrade cilium cilium/cilium --version 1.17.6 --namespace kube-system --reuse-values \
--set hubble.enabled=true --set hubble.relay.enabled=true --set hubble.ui.enabled=true \
--set hubble.ui.service.type=NodePort --set hubble.ui.service.nodePort=31001 --kube-context kind-east
```

âœ…Â **ì¶œë ¥**

```bash
Release "cilium" has been upgraded. Happy Helming!
NAME: cilium
LAST DEPLOYED: Sat Aug 16 19:30:52 2025
NAMESPACE: kube-system
STATUS: deployed
REVISION: 4
TEST SUITE: None
NOTES:
You have successfully installed Cilium with Hubble Relay and Hubble UI.

Your release version is 1.17.6.

For any further help, visit https://docs.cilium.io/en/v1.17/gettinghelp
```

### **5. East í´ëŸ¬ìŠ¤í„° Cilium DaemonSet ì¬ì‹œì‘**

```bash
kwest -n kube-system rollout restart ds/cilium

# ê²°ê³¼
daemonset.apps/cilium restarted
```

### **6. Hubble UI ì ‘ì† í™•ì¸**

`http://localhost:30001`
![](https://velog.velcdn.com/images/tlsalswls123/post/61cdf60c-d19d-4f65-927e-7dbcde90f25d/image.png)

`http://localhost:31001`
![](https://velog.velcdn.com/images/tlsalswls123/post/687a71b5-d3e6-4247-9d7e-45749c320764/image.png)

---

## **â†”ï¸ west â†” east íŒŒë“œê°„ ì§ì ‘ í†µì‹ (tcpdump ê²€ì¦)**

### **1. í…ŒìŠ¤íŠ¸ìš© íŒŒë“œ ë°°í¬ (west/east ê°ê°)**

```bash
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

cat << EOF | kubectl apply --context kind-east -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# ê²°ê³¼
pod/curl-pod created
pod/curl-pod created
```

### **2. íŒŒë“œ ìƒíƒœ ë° IP í™•ì¸**

```bash
kwest get pod -A && keast get pod -A
```

âœ…Â **ì¶œë ¥**

```bash
NAMESPACE            NAME                                         READY   STATUS      RESTARTS   AGE
default              curl-pod                                     1/1     Running     0          55s
kube-system          cilium-6l82v                                 1/1     Running     0          5m16s
kube-system          cilium-envoy-5gpxx                           1/1     Running     0          54m
kube-system          cilium-envoy-skv7b                           1/1     Running     0          54m
kube-system          cilium-lrpcr                                 1/1     Running     0          5m16s
kube-system          cilium-operator-7dbb574d5b-drtg2             1/1     Running     0          54m
kube-system          clustermesh-apiserver-5cf45db9cc-2g847       2/2     Running     0          32m
kube-system          clustermesh-apiserver-generate-certs-xvddz   0/1     Completed   0          7m53s
kube-system          coredns-674b8bbfcf-kwxv5                     1/1     Running     0          86m
kube-system          coredns-674b8bbfcf-nb96t                     1/1     Running     0          86m
kube-system          etcd-west-control-plane                      1/1     Running     0          86m
kube-system          hubble-relay-5dcd46f5c-rqrvl                 1/1     Running     0          7m54s
kube-system          hubble-ui-76d4965bb6-xkn8k                   2/2     Running     0          7m54s
kube-system          kube-apiserver-west-control-plane            1/1     Running     0          86m
kube-system          kube-controller-manager-west-control-plane   1/1     Running     0          86m
kube-system          kube-scheduler-west-control-plane            1/1     Running     0          86m
local-path-storage   local-path-provisioner-7dc846544d-jrdw8      1/1     Running     0          86m

NAMESPACE            NAME                                         READY   STATUS      RESTARTS   AGE
default              curl-pod                                     1/1     Running     0          55s
kube-system          cilium-7z2kz                                 1/1     Running     0          24m
kube-system          cilium-envoy-mrzw8                           1/1     Running     0          53m
kube-system          cilium-envoy-vq5r7                           1/1     Running     0          53m
kube-system          cilium-operator-867f8dc978-44zqb             1/1     Running     0          53m
kube-system          cilium-thtxk                                 1/1     Running     0          24m
kube-system          clustermesh-apiserver-5cf45db9cc-7wfwz       2/2     Running     0          31m
kube-system          clustermesh-apiserver-generate-certs-5csbq   0/1     Completed   0          6m4s
kube-system          coredns-674b8bbfcf-2tm2l                     1/1     Running     0          85m
kube-system          coredns-674b8bbfcf-c9qsg                     1/1     Running     0          85m
kube-system          etcd-east-control-plane                      1/1     Running     0          85m
kube-system          hubble-relay-5dcd46f5c-6qzn7                 1/1     Running     0          6m5s
kube-system          hubble-ui-76d4965bb6-jg78b                   2/2     Running     0          6m5s
kube-system          kube-apiserver-east-control-plane            1/1     Running     0          85m
kube-system          kube-controller-manager-east-control-plane   1/1     Running     0          85m
kube-system          kube-scheduler-east-control-plane            1/1     Running     0          85m
local-path-storage   local-path-provisioner-7dc846544d-mwfdc      1/1     Running     0          85m
```

```bash
kwest get pod -owide && keast get pod -owide 
```

âœ…Â **ì¶œë ¥**

```bash
NAME       READY   STATUS    RESTARTS   AGE    IP          NODE          NOMINATED NODE   READINESS GATES
curl-pod   1/1     Running   0          114s   10.0.1.12   west-worker   <none>           <none>

NAME       READY   STATUS    RESTARTS   AGE    IP          NODE          NOMINATED NODE   READINESS GATES
curl-pod   1/1     Running   0          114s   10.1.1.67   east-worker   <none>           <none>
```

### **3. west â†’ east íŒŒë“œê°„ Ping í…ŒìŠ¤íŠ¸**

```bash
kubectl exec -it curl-pod --context kind-west -- ping 10.1.1.67
```

âœ…Â **ì¶œë ¥**

```bash
PING 10.1.1.67 (10.1.1.67) 56(84) bytes of data.
64 bytes from 10.1.1.67: icmp_seq=1 ttl=62 time=0.070 ms
64 bytes from 10.1.1.67: icmp_seq=2 ttl=62 time=0.188 ms
64 bytes from 10.1.1.67: icmp_seq=3 ttl=62 time=0.093 ms
64 bytes from 10.1.1.67: icmp_seq=4 ttl=62 time=0.120 ms
64 bytes from 10.1.1.67: icmp_seq=5 ttl=62 time=0.153 ms
....
```
- ì •ìƒì ìœ¼ë¡œ ì‘ë‹µ(Reply) ìˆ˜ì‹  â†’ **Pod ê°„ ì§ì ‘ í†µì‹  ê°€ëŠ¥ í™•ì¸**

![](https://velog.velcdn.com/images/tlsalswls123/post/be80a0fd-25e3-4878-957a-13204361d779/image.png)

### **4. ëª©ì ì§€ íŒŒë“œì—ì„œ tcpdump í™•ì¸**

east íŒŒë“œ ë‚´ë¶€ì—ì„œ `tcpdump` ì‹¤í–‰

```bash
kubectl exec -it curl-pod --context kind-east -- tcpdump -i eth0 -nn
```

âœ…Â **ì¶œë ¥**

```bash
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes
10:43:50.833580 IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 199, length 64
10:43:50.833627 IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 199, length 64
10:43:51.857541 IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 200, length 64
10:43:51.857578 IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 200, length 64
10:43:52.880956 IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 201, length 64
10:43:52.881075 IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 201, length 64
10:43:53.904522 IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 202, length 64
10:43:53.904555 IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 202, length 64
10:43:54.928512 IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 203, length 64
10:43:54.928540 IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 203, length 64
10:43:55.952560 IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 204, length 64
10:43:55.952593 IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 204, length 64
10:43:56.976694 IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 205, length 64
10:43:56.976763 IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 205, length 64
...
```

- west íŒŒë“œì˜ IP(`10.0.1.12`)ì—ì„œ ì§ì ‘ ë“¤ì–´ì˜¤ëŠ” ICMP ìš”ì²­ ë° ì‘ë‹µ í™•ì¸
- NAT ë³€í™˜ ì—†ì´ **Pod â†” Pod ë‹¤ì´ë ‰íŠ¸ ë¼ìš°íŒ…** ê²€ì¦

### **5. ëª©ì ì§€ í´ëŸ¬ìŠ¤í„° ë…¸ë“œì—ì„œ tcpdump í™•ì¸**

east-control-plane, east-worker ë…¸ë“œì—ì„œ `tcpdump` ì‹¤í–‰

```bash
docker exec -it east-control-plane tcpdump -i any icmp -nn
```

âœ…Â **ì¶œë ¥**

```bash
tcpdump: data link type LINUX_SLL2
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes
```

```bash
docker exec -it east-worker tcpdump -i any icmp -nn
```

âœ…Â **ì¶œë ¥**

```bash
tcpdump: data link type LINUX_SLL2
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes
10:46:31.473504 eth0  In  IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 356, length 64
10:46:31.473530 lxccd8da9e761ca In  IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 356, length 64
10:46:31.473540 eth0  Out IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 356, length 64
10:46:31.988151 eth0  In  IP 172.18.0.1 > 172.18.0.5: ICMP 172.18.0.1 udp port 53 unreachable, length 53
10:46:32.496507 eth0  In  IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 357, length 64
10:46:32.496535 lxccd8da9e761ca In  IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 357, length 64
10:46:32.496545 eth0  Out IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 357, length 64
10:46:33.488946 eth0  In  IP 172.18.0.1 > 172.18.0.5: ICMP 172.18.0.1 udp port 53 unreachable, length 53
10:46:33.520513 eth0  In  IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 358, length 64
10:46:33.520542 lxccd8da9e761ca In  IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 358, length 64
10:46:33.520554 eth0  Out IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 358, length 64
10:46:33.569979 eth0  In  IP 172.18.0.1 > 172.18.0.5: ICMP 172.18.0.1 udp port 53 unreachable, length 53
10:46:34.544531 eth0  In  IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 359, length 64
10:46:34.544557 lxccd8da9e761ca In  IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 359, length 64
10:46:34.544569 eth0  Out IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 359, length 64
10:46:34.990071 eth0  In  IP 172.18.0.1 > 172.18.0.5: ICMP 172.18.0.1 udp port 53 unreachable, length 53
10:46:35.568525 eth0  In  IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 360, length 64
10:46:35.568555 lxccd8da9e761ca In  IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 360, length 64
10:46:35.568573 eth0  Out IP 10.1.1.67 > 10.0.1.12: ICMP echo reply, id 2, seq 360, length 64
10:46:36.492093 eth0  In  IP 172.18.0.1 > 172.18.0.5: ICMP 172.18.0.1 udp port 53 unreachable, length 53
10:46:36.572589 eth0  In  IP 172.18.0.1 > 172.18.0.5: ICMP 172.18.0.1 udp port 53 unreachable, length 53
10:46:36.593535 eth0  In  IP 10.0.1.12 > 10.1.1.67: ICMP echo request, id 2, seq 361, length 64
...
```

- west íŒŒë“œì—ì„œ ë“¤ì–´ì˜¨ íŒ¨í‚·ì´ ë…¸ë“œì—ì„œ ì§ì ‘ íŒŒë“œë¡œ ì „ë‹¬ë˜ëŠ” ê³¼ì • í™•ì¸
- **ì¤‘ê°„ NAT ê²Œì´íŠ¸ì›¨ì´ë‚˜ í„°ë„ë§ ì—†ì´ direct routing** ë™ì‘ ê²€ì¦

---

## **âš–ï¸ Load-balancing & Service Discovery**
- [https://docs.cilium.io/en/stable/network/clustermesh/services/](https://docs.cilium.io/en/stable/network/clustermesh/services/)

### **1. ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ë°°í¬ (west / east í´ëŸ¬ìŠ¤í„°)**

```bash
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
  annotations:
    service.cilium.io/global: "true"
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF

# ê²°ê³¼
deployment.apps/webpod created
service/webpod created
```

```bash
cat << EOF | kubectl apply --context kind-east -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
  annotations:
    service.cilium.io/global: "true"
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF

# ê²°ê³¼
deployment.apps/webpod created
service/webpod created
```

- `webpod` Deploymentê³¼ Serviceë¥¼ **ì–‘ìª½ í´ëŸ¬ìŠ¤í„°**ì— ë™ì¼í•˜ê²Œ ë°°í¬
- `service.cilium.io/global: "true"` ì–´ë…¸í…Œì´ì…˜ì„ ì¶”ê°€í•˜ì—¬ **ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤**ë¡œ ë“±ë¡
- ClusterIP íƒ€ì… ì„œë¹„ìŠ¤ ìƒì„±, í¬íŠ¸ 80 ë…¸ì¶œ

### **2. ì—”ë“œí¬ì¸íŠ¸ í™•ì¸**

```bash
kwest get svc,ep webpod && keast get svc,ep webpod
```

âœ…Â **ì¶œë ¥**

```bash
Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
service/webpod   ClusterIP   10.2.167.94   <none>        80/TCP    118s

NAME               ENDPOINTS                    AGE
endpoints/webpod   10.0.1.136:80,10.0.1.69:80   118s

Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
service/webpod   ClusterIP   10.3.128.46   <none>        80/TCP    56s

NAME               ENDPOINTS                  AGE
endpoints/webpod   10.1.1.6:80,10.1.1.95:80   56s
```

- west â†’ `10.0.1.136`, `10.0.1.69`
- east â†’ `10.1.1.6`, `10.1.1.95`
- ë‘ í´ëŸ¬ìŠ¤í„° ëª¨ë‘ `webpod` ì„œë¹„ìŠ¤ê°€ ì¡´ì¬í•˜ê³ , **ê°ê°ì˜ Pod IPë“¤ì´ ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ì— ë“±ë¡ë¨**

### **3. ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ë§¤í•‘ í™•ì¸ (west)**

```bash
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend                Service Type   Backend                             
1    10.2.0.1:443/TCP        ClusterIP      1 => 172.18.0.2:6443/TCP (active)   
2    10.2.182.189:443/TCP    ClusterIP      1 => 172.18.0.3:4244/TCP (active)   
3    10.2.0.10:53/UDP        ClusterIP      1 => 10.0.1.177:53/UDP (active)     
                                            2 => 10.0.1.115:53/UDP (active)     
4    10.2.0.10:53/TCP        ClusterIP      1 => 10.0.1.177:53/TCP (active)     
                                            2 => 10.0.1.115:53/TCP (active)     
5    10.2.0.10:9153/TCP      ClusterIP      1 => 10.0.1.177:9153/TCP (active)   
                                            2 => 10.0.1.115:9153/TCP (active)   
6    10.2.126.217:2379/TCP   ClusterIP      1 => 10.0.1.8:2379/TCP (active)     
7    172.18.0.3:32379/TCP    NodePort       1 => 10.0.1.8:2379/TCP (active)     
8    0.0.0.0:32379/TCP       NodePort       1 => 10.0.1.8:2379/TCP (active)     
9    10.2.1.9:80/TCP         ClusterIP      1 => 10.0.1.120:4245/TCP (active)   
10   10.2.233.237:80/TCP     ClusterIP      1 => 10.0.1.158:8081/TCP (active)   
11   172.18.0.3:30001/TCP    NodePort       1 => 10.0.1.158:8081/TCP (active)   
12   0.0.0.0:30001/TCP       NodePort       1 => 10.0.1.158:8081/TCP (active)   
13   10.2.167.94:80/TCP      ClusterIP      1 => 10.0.1.69:80/TCP (active)      
                                            2 => 10.0.1.136:80/TCP (active)     
                                            3 => 10.1.1.6:80/TCP (active)       
                                            4 => 10.1.1.95:80/TCP (active) 
```

- `webpod (10.2.167.94:80/TCP)` â†’ west, east í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  Pod IPë¡œ ë¶„ì‚°
    - west : `10.0.1.69:80`, `10.0.1.136:80`
    - east : `10.1.1.6:80`, `10.1.1.95:80`

### **4. ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ë§¤í•‘ í™•ì¸ (east)**

```bash
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend               Service Type   Backend                             
1    10.3.0.1:443/TCP       ClusterIP      1 => 172.18.0.4:6443/TCP (active)   
2    10.3.107.176:443/TCP   ClusterIP      1 => 172.18.0.5:4244/TCP (active)   
3    10.3.0.10:9153/TCP     ClusterIP      1 => 10.1.1.45:9153/TCP (active)    
                                           2 => 10.1.1.21:9153/TCP (active)    
4    10.3.0.10:53/UDP       ClusterIP      1 => 10.1.1.45:53/UDP (active)      
                                           2 => 10.1.1.21:53/UDP (active)      
5    10.3.0.10:53/TCP       ClusterIP      1 => 10.1.1.45:53/TCP (active)      
                                           2 => 10.1.1.21:53/TCP (active)      
6    10.3.173.28:2379/TCP   ClusterIP      1 => 10.1.1.62:2379/TCP (active)    
7    172.18.0.5:32379/TCP   NodePort       1 => 10.1.1.62:2379/TCP (active)    
8    0.0.0.0:32379/TCP      NodePort       1 => 10.1.1.62:2379/TCP (active)    
9    10.3.54.60:80/TCP      ClusterIP      1 => 10.1.1.198:4245/TCP (active)   
10   10.3.1.28:80/TCP       ClusterIP      1 => 10.1.1.236:8081/TCP (active)   
11   172.18.0.5:31001/TCP   NodePort       1 => 10.1.1.236:8081/TCP (active)   
12   0.0.0.0:31001/TCP      NodePort       1 => 10.1.1.236:8081/TCP (active)   
13   10.3.128.46:80/TCP     ClusterIP      1 => 10.0.1.69:80/TCP (active)      
                                           2 => 10.0.1.136:80/TCP (active)     
                                           3 => 10.1.1.6:80/TCP (active)       
                                           4 => 10.1.1.95:80/TCP (active)
```

- ë™ì¼í•˜ê²Œ `cilium service list --clustermesh-affinity` ì‹¤í–‰
- `webpod (10.3.128.46:80/TCP)` â†’ ë™ì¼í•œ 4ê°œ ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶„ì‚°

### **5. í¬ë¡œìŠ¤ í´ëŸ¬ìŠ¤í„° í˜¸ì¶œ**

```bash
kubectl exec -it curl-pod --context kind-west -- sh -c 'while true; do curl -s --connect-timeout 1 webpod ; sleep 1; echo "---"; done;'
kubectl exec -it curl-pod --context kind-east -- sh -c 'while true; do curl -s --connect-timeout 1 webpod ; sleep 1; echo "---"; done;'
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/18b8ba7e-e934-4e51-98a6-605b301fe224/image.png)
- westì˜ curl-pod â†’ webpod í˜¸ì¶œ ì‹œ **west + east ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¡œë“œë°¸ëŸ°ì‹±**
- eastì˜ curl-pod â†’ webpod í˜¸ì¶œ ì‹œ **east + west ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¡œë“œë°¸ëŸ°ì‹±**
- ì¦‰, **ì„œë¹„ìŠ¤ VIP í˜¸ì¶œë§Œìœ¼ë¡œ ë‘ í´ëŸ¬ìŠ¤í„°ì˜ Pod ëª¨ë‘ ëŒ€ìƒì´ ë¨**

![](https://velog.velcdn.com/images/tlsalswls123/post/b0cba316-18bb-4063-8032-1f73935c3f95/image.png)

### **6. ë ˆí”Œë¦¬ì¹´ ìˆ˜ ì¶•ì†Œ (west 2 â†’ 1)**

```bash
kwest scale deployment webpod --replicas 1

# ê²°ê³¼
deployment.apps/webpod scaled
```

```bash
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend                Service Type   Backend                             
1    10.2.0.1:443/TCP        ClusterIP      1 => 172.18.0.2:6443/TCP (active)   
2    10.2.182.189:443/TCP    ClusterIP      1 => 172.18.0.3:4244/TCP (active)   
3    10.2.0.10:53/UDP        ClusterIP      1 => 10.0.1.177:53/UDP (active)     
                                            2 => 10.0.1.115:53/UDP (active)     
4    10.2.0.10:53/TCP        ClusterIP      1 => 10.0.1.177:53/TCP (active)     
                                            2 => 10.0.1.115:53/TCP (active)     
5    10.2.0.10:9153/TCP      ClusterIP      1 => 10.0.1.177:9153/TCP (active)   
                                            2 => 10.0.1.115:9153/TCP (active)   
6    10.2.126.217:2379/TCP   ClusterIP      1 => 10.0.1.8:2379/TCP (active)     
7    172.18.0.3:32379/TCP    NodePort       1 => 10.0.1.8:2379/TCP (active)     
8    0.0.0.0:32379/TCP       NodePort       1 => 10.0.1.8:2379/TCP (active)     
9    10.2.1.9:80/TCP         ClusterIP      1 => 10.0.1.120:4245/TCP (active)   
10   10.2.233.237:80/TCP     ClusterIP      1 => 10.0.1.158:8081/TCP (active)   
11   172.18.0.3:30001/TCP    NodePort       1 => 10.0.1.158:8081/TCP (active)   
12   0.0.0.0:30001/TCP       NodePort       1 => 10.0.1.158:8081/TCP (active)   
13   10.2.167.94:80/TCP      ClusterIP      1 => 10.0.1.69:80/TCP (active)      
                                            2 => 10.1.1.6:80/TCP (active)       
                                            3 => 10.1.1.95:80/TCP (active)   
```

- westì— ë‚¨ì€ Pod 1ê°œ + eastì˜ Pod 2ê°œë¡œ ì„œë¹„ìŠ¤ íŠ¸ë˜í”½ì´ ë¶„ì‚°ë¨
- ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ëŠ” Pod ê°œìˆ˜ì— ë”°ë¼ ìë™ ë°˜ì˜

```bash
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend               Service Type   Backend                             
1    10.3.0.1:443/TCP       ClusterIP      1 => 172.18.0.4:6443/TCP (active)   
2    10.3.107.176:443/TCP   ClusterIP      1 => 172.18.0.5:4244/TCP (active)   
3    10.3.0.10:9153/TCP     ClusterIP      1 => 10.1.1.45:9153/TCP (active)    
                                           2 => 10.1.1.21:9153/TCP (active)    
4    10.3.0.10:53/UDP       ClusterIP      1 => 10.1.1.45:53/UDP (active)      
                                           2 => 10.1.1.21:53/UDP (active)      
5    10.3.0.10:53/TCP       ClusterIP      1 => 10.1.1.45:53/TCP (active)      
                                           2 => 10.1.1.21:53/TCP (active)      
6    10.3.173.28:2379/TCP   ClusterIP      1 => 10.1.1.62:2379/TCP (active)    
7    172.18.0.5:32379/TCP   NodePort       1 => 10.1.1.62:2379/TCP (active)    
8    0.0.0.0:32379/TCP      NodePort       1 => 10.1.1.62:2379/TCP (active)    
9    10.3.54.60:80/TCP      ClusterIP      1 => 10.1.1.198:4245/TCP (active)   
10   10.3.1.28:80/TCP       ClusterIP      1 => 10.1.1.236:8081/TCP (active)   
11   172.18.0.5:31001/TCP   NodePort       1 => 10.1.1.236:8081/TCP (active)   
12   0.0.0.0:31001/TCP      NodePort       1 => 10.1.1.236:8081/TCP (active)   
13   10.3.128.46:80/TCP     ClusterIP      1 => 10.0.1.69:80/TCP (active)      
                                           2 => 10.1.1.6:80/TCP (active)       
                                           3 => 10.1.1.95:80/TCP (active)
```

### **7. ë ˆí”Œë¦¬ì¹´ 0 (west)**

```bash
kwest scale deployment webpod --replicas 0

# ê²°ê³¼
deployment.apps/webpod scaled
```
- westì˜ ëª¨ë“  Pod ì‚­ì œ í›„ì—ë„ ì„œë¹„ìŠ¤ ì •ìƒ ë™ì‘

![](https://velog.velcdn.com/images/tlsalswls123/post/e8cdfc07-359c-4519-a767-57c26beb579e/image.png)


```bash
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend                Service Type   Backend                             
1    10.2.0.1:443/TCP        ClusterIP      1 => 172.18.0.2:6443/TCP (active)   
2    10.2.182.189:443/TCP    ClusterIP      1 => 172.18.0.3:4244/TCP (active)   
3    10.2.0.10:53/UDP        ClusterIP      1 => 10.0.1.177:53/UDP (active)     
                                            2 => 10.0.1.115:53/UDP (active)     
4    10.2.0.10:53/TCP        ClusterIP      1 => 10.0.1.177:53/TCP (active)     
                                            2 => 10.0.1.115:53/TCP (active)     
5    10.2.0.10:9153/TCP      ClusterIP      1 => 10.0.1.177:9153/TCP (active)   
                                            2 => 10.0.1.115:9153/TCP (active)   
6    10.2.126.217:2379/TCP   ClusterIP      1 => 10.0.1.8:2379/TCP (active)     
7    172.18.0.3:32379/TCP    NodePort       1 => 10.0.1.8:2379/TCP (active)     
8    0.0.0.0:32379/TCP       NodePort       1 => 10.0.1.8:2379/TCP (active)     
9    10.2.1.9:80/TCP         ClusterIP      1 => 10.0.1.120:4245/TCP (active)   
10   10.2.233.237:80/TCP     ClusterIP      1 => 10.0.1.158:8081/TCP (active)   
11   172.18.0.3:30001/TCP    NodePort       1 => 10.0.1.158:8081/TCP (active)   
12   0.0.0.0:30001/TCP       NodePort       1 => 10.0.1.158:8081/TCP (active)   
13   10.2.167.94:80/TCP      ClusterIP      1 => 10.1.1.6:80/TCP (active)       
                                            2 => 10.1.1.95:80/TCP (active)
```

```bash
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend               Service Type   Backend                             
1    10.3.0.1:443/TCP       ClusterIP      1 => 172.18.0.4:6443/TCP (active)   
2    10.3.107.176:443/TCP   ClusterIP      1 => 172.18.0.5:4244/TCP (active)   
3    10.3.0.10:9153/TCP     ClusterIP      1 => 10.1.1.45:9153/TCP (active)    
                                           2 => 10.1.1.21:9153/TCP (active)    
4    10.3.0.10:53/UDP       ClusterIP      1 => 10.1.1.45:53/UDP (active)      
                                           2 => 10.1.1.21:53/UDP (active)      
5    10.3.0.10:53/TCP       ClusterIP      1 => 10.1.1.45:53/TCP (active)      
                                           2 => 10.1.1.21:53/TCP (active)      
6    10.3.173.28:2379/TCP   ClusterIP      1 => 10.1.1.62:2379/TCP (active)    
7    172.18.0.5:32379/TCP   NodePort       1 => 10.1.1.62:2379/TCP (active)    
8    0.0.0.0:32379/TCP      NodePort       1 => 10.1.1.62:2379/TCP (active)    
9    10.3.54.60:80/TCP      ClusterIP      1 => 10.1.1.198:4245/TCP (active)   
10   10.3.1.28:80/TCP       ClusterIP      1 => 10.1.1.236:8081/TCP (active)   
11   172.18.0.5:31001/TCP   NodePort       1 => 10.1.1.236:8081/TCP (active)   
12   0.0.0.0:31001/TCP      NodePort       1 => 10.1.1.236:8081/TCP (active)   
13   10.3.128.46:80/TCP     ClusterIP      1 => 10.1.1.6:80/TCP (active)       
                                           2 => 10.1.1.95:80/TCP (active) 
```

- eastì˜ Pod ì—”ë“œí¬ì¸íŠ¸ë§Œ ì‚´ì•„ìˆìœ¼ë¯€ë¡œ, íŠ¸ë˜í”½ì€ ìë™ìœ¼ë¡œ east í´ëŸ¬ìŠ¤í„°ë¡œ ë¼ìš°íŒ…

### **8. ë ˆí”Œë¦¬ì¹´ ì›ë³µ**

```bash
kwest scale deployment webpod --replicas 2
deployment.apps/webpod scaled
```

- westì˜ ë ˆí”Œë¦¬ì¹´ ë‹¤ì‹œ 2ê°œë¡œ ë³µêµ¬
- ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡ì— ë‹¤ì‹œ west Pod IPê°€ ì¶”ê°€

---

## **ğŸ¯ Service Affinity**
- [https://docs.cilium.io/en/stable/network/clustermesh/affinity/](https://docs.cilium.io/en/stable/network/clustermesh/affinity/)

### **1. ì„œë¹„ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ì„¤ì •**

```bash
kwest annotate service webpod service.cilium.io/affinity=local --overwrite
# ê²°ê³¼
service/webpod annotated

keast annotate service webpod service.cilium.io/affinity=local --overwrite
# ê²°ê³¼
service/webpod annotated
```

```bash
kwest describe svc webpod | grep Annotations -A3
Annotations:              service.cilium.io/affinity: local
                          service.cilium.io/global: true
Selector:                 app=webpod
Type:                     ClusterIP

keast describe svc webpod | grep Annotations -A3
Annotations:              service.cilium.io/affinity: local
                          service.cilium.io/global: true
Selector:                 app=webpod
Type:                     ClusterIP
```

- west, east í´ëŸ¬ìŠ¤í„°ì˜ `webpod` ì„œë¹„ìŠ¤ì— `service.cilium.io/affinity=local` ì–´ë…¸í…Œì´ì…˜ ì¶”ê°€

### **2. Affinity ë™ì‘ í™•ì¸ (west)**

```bash
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend                Service Type   Backend                                       
1    10.2.0.1:443/TCP        ClusterIP      1 => 172.18.0.2:6443/TCP (active)             
2    10.2.182.189:443/TCP    ClusterIP      1 => 172.18.0.3:4244/TCP (active)             
3    10.2.0.10:53/UDP        ClusterIP      1 => 10.0.1.177:53/UDP (active)               
                                            2 => 10.0.1.115:53/UDP (active)               
4    10.2.0.10:53/TCP        ClusterIP      1 => 10.0.1.177:53/TCP (active)               
                                            2 => 10.0.1.115:53/TCP (active)               
5    10.2.0.10:9153/TCP      ClusterIP      1 => 10.0.1.177:9153/TCP (active)             
                                            2 => 10.0.1.115:9153/TCP (active)             
6    10.2.126.217:2379/TCP   ClusterIP      1 => 10.0.1.8:2379/TCP (active)               
7    172.18.0.3:32379/TCP    NodePort       1 => 10.0.1.8:2379/TCP (active)               
8    0.0.0.0:32379/TCP       NodePort       1 => 10.0.1.8:2379/TCP (active)               
9    10.2.1.9:80/TCP         ClusterIP      1 => 10.0.1.120:4245/TCP (active)             
10   10.2.233.237:80/TCP     ClusterIP      1 => 10.0.1.158:8081/TCP (active)             
11   172.18.0.3:30001/TCP    NodePort       1 => 10.0.1.158:8081/TCP (active)             
12   0.0.0.0:30001/TCP       NodePort       1 => 10.0.1.158:8081/TCP (active)             
13   10.2.167.94:80/TCP      ClusterIP      1 => 10.1.1.6:80/TCP (active)                 
                                            2 => 10.1.1.95:80/TCP (active)                
                                            3 => 10.0.1.159:80/TCP (active) (preferred)   
                                            4 => 10.0.1.107:80/TCP (active) (preferred)
```

- `webpod` ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ **ë¡œì»¬ Pod IP**(`10.0.1.x`)ê°€ `(preferred)` ë¡œ í‘œì‹œ
- ì¦‰, í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì€ ê¸°ë³¸ì ìœ¼ë¡œ **ë¡œì»¬ ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš°ì„  ë¼ìš°íŒ…**ë¨

### **3. Affinity ë™ì‘ í™•ì¸ (east)**

```bash
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend               Service Type   Backend                                      
1    10.3.0.1:443/TCP       ClusterIP      1 => 172.18.0.4:6443/TCP (active)            
2    10.3.107.176:443/TCP   ClusterIP      1 => 172.18.0.5:4244/TCP (active)            
3    10.3.0.10:9153/TCP     ClusterIP      1 => 10.1.1.45:9153/TCP (active)             
                                           2 => 10.1.1.21:9153/TCP (active)             
4    10.3.0.10:53/UDP       ClusterIP      1 => 10.1.1.45:53/UDP (active)               
                                           2 => 10.1.1.21:53/UDP (active)               
5    10.3.0.10:53/TCP       ClusterIP      1 => 10.1.1.45:53/TCP (active)               
                                           2 => 10.1.1.21:53/TCP (active)               
6    10.3.173.28:2379/TCP   ClusterIP      1 => 10.1.1.62:2379/TCP (active)             
7    172.18.0.5:32379/TCP   NodePort       1 => 10.1.1.62:2379/TCP (active)             
8    0.0.0.0:32379/TCP      NodePort       1 => 10.1.1.62:2379/TCP (active)             
9    10.3.54.60:80/TCP      ClusterIP      1 => 10.1.1.198:4245/TCP (active)            
10   10.3.1.28:80/TCP       ClusterIP      1 => 10.1.1.236:8081/TCP (active)            
11   172.18.0.5:31001/TCP   NodePort       1 => 10.1.1.236:8081/TCP (active)            
12   0.0.0.0:31001/TCP      NodePort       1 => 10.1.1.236:8081/TCP (active)            
13   10.3.128.46:80/TCP     ClusterIP      1 => 10.1.1.6:80/TCP (active) (preferred)    
                                           2 => 10.1.1.95:80/TCP (active) (preferred)   
                                           3 => 10.0.1.159:80/TCP (active)              
                                           4 => 10.0.1.107:80/TCP (active) 
```

- east í´ëŸ¬ìŠ¤í„°ì—ì„œë„ ë™ì¼í•˜ê²Œ ë¡œì»¬ Pod IP(`10.1.1.x`)ê°€ `(preferred)` ë¡œ í‘œì‹œ
- ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ì§€ë§Œ, **ìì‹ ì˜ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” Pod â†’ ìµœìš°ì„  ì²˜ë¦¬**

### **4. ì„œë¹„ìŠ¤ í˜¸ì¶œ í…ŒìŠ¤íŠ¸**

```bash
kubectl exec -it curl-pod --context kind-west -- sh -c 'while true; do curl -s --connect-timeout 1 webpod ; sleep 1; echo "---"; done;'
kubectl exec -it curl-pod --context kind-east -- sh -c 'while true; do curl -s --connect-timeout 1 webpod ; sleep 1; echo "---"; done;'
```

âœ…Â **ì¶œë ¥**
![](https://velog.velcdn.com/images/tlsalswls123/post/fe544ba3-3e90-4ff6-bb2f-4f0071901064/image.png)
- ì„œë¹„ìŠ¤ í˜¸ì¶œì´ **ë™ì¼ í´ëŸ¬ìŠ¤í„° ë‚´ ì—”ë“œí¬ì¸íŠ¸ë¡œ ì§‘ì¤‘**ë¨

### **5. ë ˆí”Œë¦¬ì¹´ ìˆ˜ ì¶•ì†Œ (west 2 â†’ 0)**

```bash
kwest scale deployment webpod --replicas 0
deployment.apps/webpod scaled
```

- west í´ëŸ¬ìŠ¤í„°ì—ì„œ ëª¨ë“  `webpod` Pod ì‚­ì œ

```bash
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
```

âœ…Â **ì¶œë ¥**

```bash
ID   Frontend                Service Type   Backend                             
1    10.2.0.1:443/TCP        ClusterIP      1 => 172.18.0.2:6443/TCP (active)   
2    10.2.182.189:443/TCP    ClusterIP      1 => 172.18.0.3:4244/TCP (active)   
3    10.2.0.10:53/UDP        ClusterIP      1 => 10.0.1.177:53/UDP (active)     
                                            2 => 10.0.1.115:53/UDP (active)     
4    10.2.0.10:53/TCP        ClusterIP      1 => 10.0.1.177:53/TCP (active)     
                                            2 => 10.0.1.115:53/TCP (active)     
5    10.2.0.10:9153/TCP      ClusterIP      1 => 10.0.1.177:9153/TCP (active)   
                                            2 => 10.0.1.115:9153/TCP (active)   
6    10.2.126.217:2379/TCP   ClusterIP      1 => 10.0.1.8:2379/TCP (active)     
7    172.18.0.3:32379/TCP    NodePort       1 => 10.0.1.8:2379/TCP (active)     
8    0.0.0.0:32379/TCP       NodePort       1 => 10.0.1.8:2379/TCP (active)     
9    10.2.1.9:80/TCP         ClusterIP      1 => 10.0.1.120:4245/TCP (active)   
10   10.2.233.237:80/TCP     ClusterIP      1 => 10.0.1.158:8081/TCP (active)   
11   172.18.0.3:30001/TCP    NodePort       1 => 10.0.1.158:8081/TCP (active)   
12   0.0.0.0:30001/TCP       NodePort       1 => 10.0.1.158:8081/TCP (active)   
13   10.2.167.94:80/TCP      ClusterIP      1 => 10.1.1.6:80/TCP (active)       
                                            2 => 10.1.1.95:80/TCP (active)
```
- affinity ì •ì±…ìƒ ë¡œì»¬ Podë¥¼ ì„ í˜¸í•˜ì§€ë§Œ, ë¡œì»¬ì— Podê°€ ì—†ìœ¼ë¯€ë¡œ **ì›ê²© east Podë¡œ ìš”ì²­ ì „ë‹¬**

![](https://velog.velcdn.com/images/tlsalswls123/post/2bb998fa-0167-4886-b31b-94971c2a6dbc/image.png)

```bash
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
ID   Frontend               Service Type   Backend                                      
1    10.3.0.1:443/TCP       ClusterIP      1 => 172.18.0.4:6443/TCP (active)            
2    10.3.107.176:443/TCP   ClusterIP      1 => 172.18.0.5:4244/TCP (active)            
3    10.3.0.10:9153/TCP     ClusterIP      1 => 10.1.1.45:9153/TCP (active)             
                                           2 => 10.1.1.21:9153/TCP (active)             
4    10.3.0.10:53/UDP       ClusterIP      1 => 10.1.1.45:53/UDP (active)               
                                           2 => 10.1.1.21:53/UDP (active)               
5    10.3.0.10:53/TCP       ClusterIP      1 => 10.1.1.45:53/TCP (active)               
                                           2 => 10.1.1.21:53/TCP (active)               
6    10.3.173.28:2379/TCP   ClusterIP      1 => 10.1.1.62:2379/TCP (active)             
7    172.18.0.5:32379/TCP   NodePort       1 => 10.1.1.62:2379/TCP (active)             
8    0.0.0.0:32379/TCP      NodePort       1 => 10.1.1.62:2379/TCP (active)             
9    10.3.54.60:80/TCP      ClusterIP      1 => 10.1.1.198:4245/TCP (active)            
10   10.3.1.28:80/TCP       ClusterIP      1 => 10.1.1.236:8081/TCP (active)            
11   172.18.0.5:31001/TCP   NodePort       1 => 10.1.1.236:8081/TCP (active)            
12   0.0.0.0:31001/TCP      NodePort       1 => 10.1.1.236:8081/TCP (active)            
13   10.3.128.46:80/TCP     ClusterIP      1 => 10.1.1.6:80/TCP (active) (preferred)    
                                           2 => 10.1.1.95:80/TCP (active) (preferred)
```

- east í´ëŸ¬ìŠ¤í„°ëŠ” ì—¬ì „íˆ ë¡œì»¬ Podê°€ ì‚´ì•„ìˆì–´ **ìì‹ ì˜ Podë¡œ ìš°ì„  ì‘ë‹µ**

### **6. ì›ë³µ (west replicas=2)**

```bash
kwest scale deployment webpod --replicas 2
deployment.apps/webpod scaled
```

- westì— ë‹¤ì‹œ 2ê°œì˜ Pod ìƒì„± â†’ ë¡œì»¬ Pod IPê°€ ë‹¤ì‹œ `(preferred)` ë¡œ í‘œì‹œë¨
- ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹œ ë¡œì»¬ ìš°ì„  ì •ì±…ì´ ë‹¤ì‹œ ì ìš©ë¨
