---
title: AEWS 11Ï£ºÏ∞® Ï†ïÎ¶¨
date: 2025-04-19 17:30:00 +0900
categories: [EKS]
tags: [AEWS]
---

## **üèÅ Getting Started: GenAI with Inferentia & FSx Workshop**
![](https://velog.velcdn.com/images/tlsalswls123/post/01db1c5e-0bd5-4e9f-ba27-437106ece3b7/image.png)

### **1. AWS Cloud9 Ïù¥Îèô**

`genaifsxworkshoponeks` Cloud9 IDE Open ÌÅ¥Î¶≠
![](https://velog.velcdn.com/images/tlsalswls123/post/41b2c695-a0fd-401d-a6a4-baab084e9946/image.png)

### **2. New Terminal ÏÉùÏÑ±**

Cloud9 IDE ÌôîÎ©¥ Î°úÎìú ÌõÑ ÏÉÅÎã® ÌÉ≠Ïùò¬†(+)¬†Î≤ÑÌäº¬†>¬†ÏÉà ÌÑ∞ÎØ∏ÎÑê ÌÅ¥Î¶≠ÌïòÏó¨ ÌÑ∞ÎØ∏ÎÑê ÏÉùÏÑ±
![](https://velog.velcdn.com/images/tlsalswls123/post/107aeb5a-184c-4be2-8688-cfee4dcf1d41/image.png)

### **3. ÏûêÍ≤© Ï¶ùÎ™Ö ÎπÑÌôúÏÑ±Ìôî**

```bash
WSParticipantRole:~/environment $ aws cloud9 update-environment --environment-id ${C9_PID} --managed-credentials-action DISABLE
WSParticipantRole:~/environment $ rm -vf ${HOME}/.aws/credentials

# Í≤∞Í≥º
removed '/home/ec2-user/.aws/credentials'
```

### **4. IAM Ïó≠Ìï† ÌôïÏù∏**

```bash
WSParticipantRole:~/environment $ aws sts get-caller-identity

{
    "UserId": "XXXXXXXXXXXXXXXXXXXXXX:x-xxxxxxxxxxxxxxxxxx",
    "Account": "xxxxxxxxxxxxx",
    "Arn": "arn:aws:sts::xxxxxxxxxxxxx:assumed-role/genaifsxworkshoponeks-C9Role-NsrVsrgsvUf3/x-xxxxxxxxxxxxxxxxxx"
}
```

### **5. Î¶¨Ï†Ñ Î∞è EKS ÌÅ¥Îü¨Ïä§ÌÑ∞Î™Ö ÏÑ§Ï†ï**

```bash
WSParticipantRole:~/environment $ TOKEN=`curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"`
WSParticipantRole:~/environment $ export AWS_REGION=$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/placement/region)
```

```bash
WSParticipantRole:~/environment $ export CLUSTER_NAME=eksworkshop
```

### **6. Î¶¨Ï†Ñ Î∞è EKS ÌÅ¥Îü¨Ïä§ÌÑ∞Î™Ö ÌôïÏù∏**

```bash
WSParticipantRole:~/environment $ echo $AWS_REGION
us-west-2

WSParticipantRole:~/environment $ echo $CLUSTER_NAME
eksworkshop
```

### **7. kube-config ÏóÖÎç∞Ïù¥Ìä∏**

```bash
WSParticipantRole:~/environment $ aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION

# Í≤∞Í≥º
Added new context arn:aws:eks:us-west-2:xxxxxxxxxxxxx:cluster/eksworkshop to /home/ec2-user/.kube/config
```

### **8. EKS ÎÖ∏Îìú ÏÉÅÌÉú ÌôïÏù∏**

```bash
WSParticipantRole:~/environment $ kubectl get nodes
NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-108-71.us-west-2.compute.internal   Ready    <none>   47h   v1.30.9-eks-5d632ec
ip-10-0-67-165.us-west-2.compute.internal   Ready    <none>   47h   v1.30.9-eks-5d632ec
```

### **9. Karpenter Deployment Ï°∞Ìöå**

```bash
WSParticipantRole:~/environment $ kubectl -n karpenter get deploy/karpenter -o yaml
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    meta.helm.sh/release-name: karpenter
    meta.helm.sh/release-namespace: karpenter
  creationTimestamp: "2025-04-17T02:22:13Z"
  generation: 1
  labels:
    app.kubernetes.io/instance: karpenter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: karpenter
    app.kubernetes.io/version: 1.0.1
    helm.sh/chart: karpenter-1.0.1
  name: karpenter
  namespace: karpenter
  resourceVersion: "3161"
  uid: 51294e16-448c-48f8-93ce-179331c7e2ca
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: karpenter
      app.kubernetes.io/name: karpenter
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: karpenter
        app.kubernetes.io/name: karpenter
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: karpenter.sh/nodepool
                operator: DoesNotExist
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/instance: karpenter
                app.kubernetes.io/name: karpenter
            topologyKey: kubernetes.io/hostname
      containers:
      - env:
        - name: KUBERNETES_MIN_VERSION
          value: 1.19.0-0
        - name: KARPENTER_SERVICE
          value: karpenter
        - name: WEBHOOK_PORT
          value: "8443"
        - name: WEBHOOK_METRICS_PORT
          value: "8001"
        - name: DISABLE_WEBHOOK
          value: "false"
        - name: LOG_LEVEL
          value: info
        - name: METRICS_PORT
          value: "8080"
        - name: HEALTH_PROBE_PORT
          value: "8081"
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: MEMORY_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: controller
              divisor: "0"
              resource: limits.memory
        - name: FEATURE_GATES
          value: SpotToSpotConsolidation=false
        - name: BATCH_MAX_DURATION
          value: 10s
        - name: BATCH_IDLE_DURATION
          value: 1s
        - name: CLUSTER_NAME
          value: eksworkshop
        - name: CLUSTER_ENDPOINT
          value: https://53E5113441C691170249AE781B50CCEE.gr7.us-west-2.eks.amazonaws.com
        - name: VM_MEMORY_OVERHEAD_PERCENT
          value: "0.075"
        - name: INTERRUPTION_QUEUE
          value: karpenter-eksworkshop
        - name: RESERVED_ENIS
          value: "0"
        image: public.ecr.aws/karpenter/controller:1.0.1@sha256:fc54495b35dfeac6459ead173dd8452ca5d572d90e559f09536a494d2795abe6
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: http
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        name: controller
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 8001
          name: webhook-metrics
          protocol: TCP
        - containerPort: 8443
          name: https-webhook
          protocol: TCP
        - containerPort: 8081
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /readyz
            port: http
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
          seccompProfile:
            type: RuntimeDefault
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 65532
      serviceAccount: karpenter
      serviceAccountName: karpenter
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/instance: karpenter
            app.kubernetes.io/name: karpenter
        maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2025-04-17T02:22:23Z"
    lastUpdateTime: "2025-04-17T02:22:23Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2025-04-17T02:22:13Z"
    lastUpdateTime: "2025-04-17T02:22:33Z"
    message: ReplicaSet "karpenter-86d7868f9f" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 2
  replicas: 2
  updatedReplicas: 2
```

- https://karpenter.sh/docs/reference/settings/

### **10. Karpenter Pod ÏÉÅÌÉú ÌôïÏù∏**

```bash
WSParticipantRole:~/environment $ kubectl get pods --namespace karpenter
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                         READY   STATUS    RESTARTS   AGE
karpenter-86d7868f9f-2dmfd   1/1     Running   0          47h
karpenter-86d7868f9f-zjbl8   1/1     Running   0          47h
```

### **11. Karpenter Î°úÍ∑∏ Ïä§Ìä∏Î¶¨Î∞ç**

ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Î≥ÑÏπ≠ ÏÑ§Ï†ï ÌõÑ, Karpenter Ïª®Ìä∏Î°§Îü¨ Î°úÍ∑∏ Ïä§Ìä∏Î¶¨Î∞ç

```bash
WSParticipantRole:~/environment $ alias kl='kubectl -n karpenter logs -l app.kubernetes.io/name=karpenter --all-containers=true -f --tail=20'
WSParticipantRole:~/environment $ kl
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
{"level":"INFO","time":"2025-04-17T02:22:21.024Z","logger":"controller.controller-runtime.metrics","message":"Starting metrics server","commit":"62a726c"}
{"level":"INFO","time":"2025-04-17T02:22:21.025Z","logger":"controller","message":"starting server","commit":"62a726c","name":"health probe","addr":"[::]:8081"}
{"level":"INFO","time":"2025-04-17T02:22:21.025Z","logger":"controller.controller-runtime.metrics","message":"Serving metrics server","commit":"62a726c","bindAddress":":8080","secure":false}
{"level":"INFO","time":"2025-04-17T02:22:21.126Z","logger":"controller","message":"attempting to acquire leader lease karpenter/karpenter-leader-election...","commit":"62a726c"}
{"level":"INFO","time":"2025-04-17T02:22:19.397Z","logger":"controller","message":"Starting workers","commit":"62a726c","controller":"nodeclaim.consistency","controllerGroup":"karpenter.sh","controllerKind":"NodeClaim","worker count":10}
{"level":"INFO","time":"2025-04-17T02:22:19.397Z","logger":"controller","message":"Starting workers","commit":"62a726c","controller":"nodeclaim.disruption","controllerGroup":"karpenter.sh","controllerKind":"NodeClaim","worker count":10}
{"level":"ERROR","time":"2025-04-17T02:22:19.663Z","logger":"webhook.ConversionWebhook","message":"Reconcile error","commit":"62a726c","knative.dev/traceid":"85caea6f-8f7a-4cd3-9dcc-642a21a15959","knative.dev/key":"nodeclaims.karpenter.sh","duration":"154.257731ms","error":"failed to update webhook: Operation cannot be fulfilled on customresourcedefinitions.apiextensions.k8s.io \"nodeclaims.karpenter.sh\": the object has been modified; please apply your changes to the latest version and try again"}
{"level":"ERROR","time":"2025-04-17T02:22:19.686Z","logger":"webhook.ConversionWebhook","message":"Reconcile error","commit":"62a726c","knative.dev/traceid":"72b3cf48-22e3-410a-9667-962c8534b26d","knative.dev/key":"nodepools.karpenter.sh","duration":"97.713552ms","error":"failed to update webhook: Operation cannot be fulfilled on customresourcedefinitions.apiextensions.k8s.io \"nodepools.karpenter.sh\": the object has been modified; please apply your changes to the latest version and try again"}
{"level":"INFO","time":"2025-04-17T02:23:05.822Z","logger":"controller","message":"discovered ssm parameter","commit":"62a726c","controller":"nodeclass.status","controllerGroup":"karpenter.k8s.aws","controllerKind":"EC2NodeClass","EC2NodeClass":{"name":"sysprep"},"namespace":"","name":"sysprep","reconcileID":"859590e8-76f0-4884-9c38-ac6378cc40c3","parameter":"/aws/service/eks/optimized-ami/1.30/amazon-linux-2/amazon-eks-node-1.30-v20240917/image_id","value":"ami-05f7e80c30f28d8b9"}
{"level":"INFO","time":"2025-04-17T02:23:05.852Z","logger":"controller","message":"discovered ssm parameter","commit":"62a726c","controller":"nodeclass.status","controllerGroup":"karpenter.k8s.aws","controllerKind":"EC2NodeClass","EC2NodeClass":{"name":"sysprep"},"namespace":"","name":"sysprep","reconcileID":"859590e8-76f0-4884-9c38-ac6378cc40c3","parameter":"/aws/service/eks/optimized-ami/1.30/amazon-linux-2-arm64/amazon-eks-arm64-node-1.30-v20240917/image_id","value":"ami-0b402b9a4c1bacaa5"}
{"level":"INFO","time":"2025-04-17T02:23:05.881Z","logger":"controller","message":"discovered ssm parameter","commit":"62a726c","controller":"nodeclass.status","controllerGroup":"karpenter.k8s.aws","controllerKind":"EC2NodeClass","EC2NodeClass":{"name":"sysprep"},"namespace":"","name":"sysprep","reconcileID":"859590e8-76f0-4884-9c38-ac6378cc40c3","parameter":"/aws/service/eks/optimized-ami/1.30/amazon-linux-2-gpu/amazon-eks-gpu-node-1.30-v20240917/image_id","value":"ami-0356f40aea17e9b9e"}
{"level":"INFO","time":"2025-04-17T02:23:07.175Z","logger":"controller","message":"found provisionable pod(s)","commit":"62a726c","controller":"provisioner","namespace":"","name":"","reconcileID":"869e7086-1212-4127-b4b3-94480c1680cf","Pods":"default/sysprep-l4wpv","duration":"350.444011ms"}
{"level":"INFO","time":"2025-04-17T02:23:07.175Z","logger":"controller","message":"computed new nodeclaim(s) to fit pod(s)","commit":"62a726c","controller":"provisioner","namespace":"","name":"","reconcileID":"869e7086-1212-4127-b4b3-94480c1680cf","nodeclaims":1,"pods":1}
{"level":"INFO","time":"2025-04-17T02:23:07.186Z","logger":"controller","message":"created nodeclaim","commit":"62a726c","controller":"provisioner","namespace":"","name":"","reconcileID":"869e7086-1212-4127-b4b3-94480c1680cf","NodePool":{"name":"sysprep"},"NodeClaim":{"name":"sysprep-r6fgf"},"requests":{"cpu":"180m","memory":"120Mi","pods":"9"},"instance-types":"c5.large, c5.xlarge, c5a.large, c5a.xlarge, c5ad.large and 55 other(s)"}
{"level":"INFO","time":"2025-04-17T02:23:13.205Z","logger":"controller","message":"launched nodeclaim","commit":"62a726c","controller":"nodeclaim.lifecycle","controllerGroup":"karpenter.sh","controllerKind":"NodeClaim","NodeClaim":{"name":"sysprep-r6fgf"},"namespace":"","name":"sysprep-r6fgf","reconcileID":"f2b2ec5c-bcf7-4402-a919-a8280a1e9cac","provider-id":"aws:///us-west-2a/i-0dd7235d205d3547d","instance-type":"c6i.large","zone":"us-west-2a","capacity-type":"on-demand","allocatable":{"cpu":"1930m","ephemeral-storage":"89Gi","memory":"3114Mi","pods":"29","vpc.amazonaws.com/pod-eni":"9"}}
{"level":"INFO","time":"2025-04-17T02:23:30.109Z","logger":"controller","message":"registered nodeclaim","commit":"62a726c","controller":"nodeclaim.lifecycle","controllerGroup":"karpenter.sh","controllerKind":"NodeClaim","NodeClaim":{"name":"sysprep-r6fgf"},"namespace":"","name":"sysprep-r6fgf","reconcileID":"8f45ee82-92e7-47b2-92e7-43449a500f45","provider-id":"aws:///us-west-2a/i-0dd7235d205d3547d","Node":{"name":"ip-10-0-43-213.us-west-2.compute.internal"}}
{"level":"INFO","time":"2025-04-17T02:23:41.271Z","logger":"controller","message":"initialized nodeclaim","commit":"62a726c","controller":"nodeclaim.lifecycle","controllerGroup":"karpenter.sh","controllerKind":"NodeClaim","NodeClaim":{"name":"sysprep-r6fgf"},"namespace":"","name":"sysprep-r6fgf","reconcileID":"5a63fb88-bcb2-4e50-87b2-30ffbd131697","provider-id":"aws:///us-west-2a/i-0dd7235d205d3547d","Node":{"name":"ip-10-0-43-213.us-west-2.compute.internal"},"allocatable":{"cpu":"1930m","ephemeral-storage":"95551679124","hugepages-1Gi":"0","hugepages-2Mi":"0","memory":"3232656Ki","pods":"29"}}
{"level":"INFO","time":"2025-04-17T02:33:56.588Z","logger":"controller","message":"tainted node","commit":"62a726c","controller":"node.termination","controllerGroup":"","controllerKind":"Node","Node":{"name":"ip-10-0-43-213.us-west-2.compute.internal"},"namespace":"","name":"ip-10-0-43-213.us-west-2.compute.internal","reconcileID":"e8b2aa8b-bb75-4566-9aa1-8c787ae1b7bf","taint.Key":"karpenter.sh/disrupted","taint.Value":"","taint.Effect":"NoSchedule"}
{"level":"INFO","time":"2025-04-17T02:34:23.083Z","logger":"controller","message":"deleted node","commit":"62a726c","controller":"node.termination","controllerGroup":"","controllerKind":"Node","Node":{"name":"ip-10-0-43-213.us-west-2.compute.internal"},"namespace":"","name":"ip-10-0-43-213.us-west-2.compute.internal","reconcileID":"90b9486f-add1-444b-872c-1fc3d748f6a8"}
{"level":"INFO","time":"2025-04-17T02:34:23.320Z","logger":"controller","message":"deleted nodeclaim","commit":"62a726c","controller":"nodeclaim.termination","controllerGroup":"karpenter.sh","controllerKind":"NodeClaim","NodeClaim":{"name":"sysprep-r6fgf"},"namespace":"","name":"sysprep-r6fgf","reconcileID":"0260c674-f67d-4d57-84e0-b1c9db150229","Node":{"name":"ip-10-0-43-213.us-west-2.compute.internal"},"provider-id":"aws:///us-west-2a/i-0dd7235d205d3547d"}
{"level":"ERROR","time":"2025-04-17T14:35:52.758Z","logger":"controller","message":"Failed to update lock optimitically: etcdserver: request timed out, falling back to slow path","commit":"62a726c"}
{"level":"INFO","time":"2025-04-18T02:23:23.974Z","logger":"controller","message":"discovered ssm parameter","commit":"62a726c","controller":"nodeclass.status","controllerGroup":"karpenter.k8s.aws","controllerKind":"EC2NodeClass","EC2NodeClass":{"name":"default"},"namespace":"","name":"default","reconcileID":"fc30a0be-2ef1-47bd-a48b-b6d491061241","parameter":"/aws/service/eks/optimized-ami/1.30/amazon-linux-2/amazon-eks-node-1.30-v20240917/image_id","value":"ami-05f7e80c30f28d8b9"}
{"level":"INFO","time":"2025-04-18T02:23:24.000Z","logger":"controller","message":"discovered ssm parameter","commit":"62a726c","controller":"nodeclass.status","controllerGroup":"karpenter.k8s.aws","controllerKind":"EC2NodeClass","EC2NodeClass":{"name":"default"},"namespace":"","name":"default","reconcileID":"fc30a0be-2ef1-47bd-a48b-b6d491061241","parameter":"/aws/service/eks/optimized-ami/1.30/amazon-linux-2-arm64/amazon-eks-arm64-node-1.30-v20240917/image_id","value":"ami-0b402b9a4c1bacaa5"}
{"level":"INFO","time":"2025-04-18T02:23:24.016Z","logger":"controller","message":"discovered ssm parameter","commit":"62a726c","controller":"nodeclass.status","controllerGroup":"karpenter.k8s.aws","controllerKind":"EC2NodeClass","EC2NodeClass":{"name":"default"},"namespace":"","name":"default","reconcileID":"fc30a0be-2ef1-47bd-a48b-b6d491061241","parameter":"/aws/service/eks/optimized-ami/1.30/amazon-linux-2-gpu/amazon-eks-gpu-node-1.30-v20240917/image_id","value":"ami-0356f40aea17e9b9e"}
```

`control + c` : Ï¢ÖÎ£å

```bash
^C
```

---

## **üóÑÔ∏è Configure storage - Host model data on Amazon FSx for Lustre**

![](https://velog.velcdn.com/images/tlsalswls123/post/31999270-acfe-4245-8822-2f65c72f4925/image.png)

## **Amazon FSx for Lustre**

- https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html
- **ÏôÑÏ†ÑÍ¥ÄÎ¶¨Ìòï Í≥†ÏÑ±Îä• ÌååÏùº ÏãúÏä§ÌÖú Ï†úÍ≥µ:** Machine Learning, Î∂ÑÏÑù, HPC Îì± ÏÜçÎèÑÍ∞Ä Ï§ëÏöîÌïú ÏõåÌÅ¨Î°úÎìúÏóê Ï†ÅÌï©
- **Î∞ÄÎ¶¨Ï¥à Ïù¥Ìïò ÏßÄÏó∞ ÏãúÍ∞Ñ Î∞è ÎåÄÍ∑úÎ™® ÏÑ±Îä• ÌôïÏû•:**¬†TB/s Ï≤òÎ¶¨ÎüâÍ≥º ÏàòÎ∞±Îßå IOPSÎ°ú ÌôïÏû• ÏßÄÏõê
- **Amazon S3 Ïó∞Îèô:** S3 Í∞ùÏ≤¥Î•º ÌååÏùºÎ°ú Ï†úÍ≥µ, Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ ÏûêÎèô ÎèôÍ∏∞Ìôî

## CSI ÎìúÎùºÏù¥Î≤Ñ Î∞∞Ìè¨

### **1. account-id ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ï**

```bash
WSParticipantRole:~/environment $ ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
```

### **2. IAM ÏÑúÎπÑÏä§ Í≥ÑÏ†ï ÏÉùÏÑ±**

AWS API Ìò∏Ï∂ú Í∂åÌïúÏùÑ Í∞ÄÏßÑ IAM Ï†ïÏ±Ö ÏÉùÏÑ± ÌõÑ ÏÑúÎπÑÏä§ Í≥ÑÏ†ïÏóê Ïó∞Í≤∞

```bash
WSParticipantRole:~/environment $ cat << EOF >  fsx-csi-driver.json
> {
>     "Version":"2012-10-17",
>     "Statement":[
>         {
>             "Effect":"Allow",
>             "Action":[
>                 "iam:CreateServiceLinkedRole",
>                 "iam:AttachRolePolicy",
>                 "iam:PutRolePolicy"
>             ],
>             "Resource":"arn:aws:iam::*:role/aws-service-role/s3.data-source.lustre.fsx.amazonaws.com/*"
>         },
>         {
>             "Action":"iam:CreateServiceLinkedRole",
>             "Effect":"Allow",
>             "Resource":"*",
>             "Condition":{
>                 "StringLike":{
>                     "iam:AWSServiceName":[
>                         "fsx.amazonaws.com"
>                     ]
>                 }
>             }
>         },
>         {
>             "Effect":"Allow",
>             "Action":[
>                 "s3:ListBucket",
>                 "fsx:CreateFileSystem",
>                 "fsx:DeleteFileSystem",
>                 "fsx:DescribeFileSystems",
>                 "fsx:TagResource"
>             ],
>             "Resource":[
>                 "*"
>             ]
>         }
>     ]
> }
> EOF
```

### **3. IAM Ï†ïÏ±Ö ÏÉùÏÑ±**

```bash
WSParticipantRole:~/environment $ aws iam create-policy \
>         --policy-name Amazon_FSx_Lustre_CSI_Driver \
>         --policy-document file://fsx-csi-driver.json
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
{
    "Policy": {
        "PolicyName": "Amazon_FSx_Lustre_CSI_Driver",
        "PolicyId": "ANPATNWURYOGXHAZW6A6U",
        "Arn": "arn:aws:iam::xxxxxxxxxxxxx:policy/Amazon_FSx_Lustre_CSI_Driver",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2025-04-19T02:18:02+00:00",
        "UpdateDate": "2025-04-19T02:18:02+00:00"
    }
}
```

### **4. ÏÑúÎπÑÏä§ Í≥ÑÏ†ï ÏÉùÏÑ± Î∞è Ï†ïÏ±Ö Ïó∞Í≤∞**

ÏïÑÎûò Î™ÖÎ†π Ïã§ÌñâÌï¥ ÎìúÎùºÏù¥Î≤ÑÏö© ÏÑúÎπÑÏä§ Í≥ÑÏ†ï ÏÉùÏÑ± ÌõÑ **3Îã®Í≥Ñ IAM Ï†ïÏ±Ö** Ïó∞Í≤∞

```bash
WSParticipantRole:~/environment $ eksctl create iamserviceaccount \
>     --region $AWS_REGION \
>     --name fsx-csi-controller-sa \
>     --namespace kube-system \
>     --cluster $CLUSTER_NAME \
>     --attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/Amazon_FSx_Lustre_CSI_Driver \
>     --approve
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
2025-04-19 02:19:20 [‚Ñπ]  1 iamserviceaccount (kube-system/fsx-csi-controller-sa) was included (based on the include/exclude rules)
2025-04-19 02:19:20 [!]  serviceaccounts that exist in Kubernetes will be excluded, use --override-existing-serviceaccounts to override
2025-04-19 02:19:20 [‚Ñπ]  1 task: { 
    2 sequential sub-tasks: { 
        create IAM role for serviceaccount "kube-system/fsx-csi-controller-sa",
        create serviceaccount "kube-system/fsx-csi-controller-sa",
    } }2025-04-19 02:19:20 [‚Ñπ]  building iamserviceaccount stack "eksctl-eksworkshop-addon-iamserviceaccount-kube-system-fsx-csi-controller-sa"
2025-04-19 02:19:20 [‚Ñπ]  deploying stack "eksctl-eksworkshop-addon-iamserviceaccount-kube-system-fsx-csi-controller-sa"
2025-04-19 02:19:20 [‚Ñπ]  waiting for CloudFormation stack "eksctl-eksworkshop-addon-iamserviceaccount-kube-system-fsx-csi-controller-sa"
2025-04-19 02:19:50 [‚Ñπ]  waiting for CloudFormation stack "eksctl-eksworkshop-addon-iamserviceaccount-kube-system-fsx-csi-controller-sa"
2025-04-19 02:19:50 [‚Ñπ]  created serviceaccount "kube-system/fsx-csi-controller-sa"
```

### **5. Ïó≠Ìï†(Role) ARN Ï†ÄÏû•**

```bash
WSParticipantRole:~/environment $ export ROLE_ARN=$(aws cloudformation describe-stacks --stack-name "eksctl-${CLUSTER_NAME}-addon-iamserviceaccount-kube-system-fsx-csi-controller-sa" --query "Stacks[0].Outputs[0].OutputValue"  --region $AWS_REGION --output text)
WSParticipantRole:~/environment $ echo $ROLE_ARN
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
arn:aws:iam::xxxxxxxxxxxxx:role/eksctl-eksworkshop-addon-iamserviceaccount-ku-Role1-eALfDkUyCxWi
```

### **6. LustreÏö© FSx CSI ÎìúÎùºÏù¥Î≤Ñ Î∞∞Ìè¨**

```bash
WSParticipantRole:~/environment $ kubectl apply -k "github.com/kubernetes-sigs/aws-fsx-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.2"
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
# Warning: 'bases' is deprecated. Please use 'resources' instead. Run 'kustomize edit fix' to update your Kustomization automatically.
Warning: resource serviceaccounts/fsx-csi-controller-sa is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
serviceaccount/fsx-csi-controller-sa configured
serviceaccount/fsx-csi-node-sa created
clusterrole.rbac.authorization.k8s.io/fsx-csi-external-provisioner-role created
clusterrole.rbac.authorization.k8s.io/fsx-csi-node-role created
clusterrole.rbac.authorization.k8s.io/fsx-external-resizer-role created
clusterrolebinding.rbac.authorization.k8s.io/fsx-csi-external-provisioner-binding created
clusterrolebinding.rbac.authorization.k8s.io/fsx-csi-node-getter-binding created
clusterrolebinding.rbac.authorization.k8s.io/fsx-csi-resizer-binding created
deployment.apps/fsx-csi-controller created
daemonset.apps/fsx-csi-node created
csidriver.storage.k8s.io/fsx.csi.aws.com created
```

CSI ÎìúÎùºÏù¥Î≤Ñ ÏÑ§Ïπò ÌôïÏù∏

```bash
WSParticipantRole:~/environment $ kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-fsx-csi-driver
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                                  READY   STATUS    RESTARTS   AGE
fsx-csi-controller-6f4c577bd4-4wxrs   4/4     Running   0          38s
fsx-csi-controller-6f4c577bd4-r6kgd   4/4     Running   0          38s
fsx-csi-node-c4spk                    3/3     Running   0          38s
fsx-csi-node-k9dd6                    3/3     Running   0          38s
```

### **7. ÏÑúÎπÑÏä§ Í≥ÑÏ†ïÏóê Ï£ºÏÑù Îã¨Í∏∞**

```bash
WSParticipantRole:~/environment $ kubectl annotate serviceaccount -n kube-system fsx-csi-controller-sa \
>  eks.amazonaws.com/role-arn=$ROLE_ARN --overwrite=true

# Í≤∞Í≥º
serviceaccount/fsx-csi-controller-sa annotated
```

ÏÑúÎπÑÏä§ Í≥ÑÏ†ï ÏÑ∏Î∂Ä Ï†ïÎ≥¥ Ï°∞Ìöå

```bash
WSParticipantRole:~/environment $ kubectl get sa/fsx-csi-controller-sa -n kube-system -o yaml
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::xxxxxxxxxxxxx:role/eksctl-eksworkshop-addon-iamserviceaccount-ku-Role1-eALfDkUyCxWi
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"ServiceAccount","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"aws-fsx-csi-driver"},"name":"fsx-csi-controller-sa","namespace":"kube-system"}}
  creationTimestamp: "2025-04-19T02:19:50Z"
  labels:
    app.kubernetes.io/managed-by: eksctl
    app.kubernetes.io/name: aws-fsx-csi-driver
  name: fsx-csi-controller-sa
  namespace: kube-system
  resourceVersion: "873913"
  uid: 7abe6847-4225-4740-9835-acd4ef985cd6
```

---

## **üì¶ Create Persistent Volume on EKS Cluster**

- **Ï†ïÏ†Å ÌîÑÎ°úÎπÑÏ†ÄÎãù:** Í¥ÄÎ¶¨ÏûêÍ∞Ä FSx Ïù∏Ïä§ÌÑ¥Ïä§ÏôÄ PV Ï†ïÏùòÎ•º ÎØ∏Î¶¨ ÏÉùÏÑ±ÌïòÍ≥† PVC ÏöîÏ≤≠ Ïãú ÌôúÏö©Ìï®
- **ÎèôÏ†Å ÌîÑÎ°úÎπÑÏ†ÄÎãù:** PVC ÏöîÏ≤≠ Ïãú CSI ÎìúÎùºÏù¥Î≤ÑÍ∞Ä PVÏôÄ FSx Ïù∏Ïä§ÌÑ¥Ïä§ÍπåÏßÄ ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±Ìï®
- Ïù¥ Îû©ÏóêÏÑúÎäî **Ï†ïÏ†Å ÌîÑÎ°úÎπÑÏ†ÄÎãù** Î∞©ÏãùÏùÑ ÏÇ¨Ïö©Ìï®

### **1. ÏûëÏóÖ ÎîîÎ†âÌÑ∞Î¶¨ Ïù¥Îèô**

```bash
WSParticipantRole:~/environment $ cd /home/ec2-user/environment/eks/FSxL
WSParticipantRole:~/environment/eks/FSxL $ 
```

### **2. FSx Lustre Ïù∏Ïä§ÌÑ¥Ïä§ Î≥ÄÏàò ÏÑ§Ï†ï**

```bash
WSParticipantRole:~/environment/eks/FSxL $ FSXL_VOLUME_ID=$(aws fsx describe-file-systems --query 'FileSystems[].FileSystemId' --output text)
WSParticipantRole:~/environment/eks/FSxL $ DNS_NAME=$(aws fsx describe-file-systems --query 'FileSystems[].DNSName' --output text)
WSParticipantRole:~/environment/eks/FSxL $ MOUNT_NAME=$(aws fsx describe-file-systems --query 'FileSystems[].LustreConfiguration.MountName' --output text)
```

### **3. Persistent Volume ÌååÏùº ÌôïÏù∏**

```bash
# fsxL-persistent-volume.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fsx-pv
spec:
  persistentVolumeReclaimPolicy: Retain
  capacity:
    storage: 1200Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  mountOptions:
    - flock
  csi:
    driver: fsx.csi.aws.com
    volumeHandle: FSXL_VOLUME_ID
    volumeAttributes:
      dnsname: DNS_NAME
      mountname: MOUNT_NAME
```

`FSXL_VOLUME_ID`,¬†`DNS_NAME`, `MOUNT_NAME` FSx Lustre Ïù∏Ïä§ÌÑ¥Ïä§Ïùò Ïã§Ï†ú Í∞íÏúºÎ°ú ÏπòÌôò

```bash
WSParticipantRole:~/environment/eks/FSxL $ sed -i'' -e "s/FSXL_VOLUME_ID/$FSXL_VOLUME_ID/g" fsxL-persistent-volume.yaml
WSParticipantRole:~/environment/eks/FSxL $ sed -i'' -e "s/DNS_NAME/$DNS_NAME/g" fsxL-persistent-volume.yaml
WSParticipantRole:~/environment/eks/FSxL $ sed -i'' -e "s/MOUNT_NAME/$MOUNT_NAME/g" fsxL-persistent-volume.yaml

WSParticipantRole:~/environment/eks/FSxL $ cat fsxL-persistent-volume.yaml
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fsx-pv
spec:
  persistentVolumeReclaimPolicy: Retain
  capacity:
    storage: 1200Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  mountOptions:
    - flock
  csi:
    driver: fsx.csi.aws.com
    volumeHandle: fs-032691accd783ccb1
    volumeAttributes:
      dnsname: fs-032691accd783ccb1.fsx.us-west-2.amazonaws.com
      mountname: u42dbb4v
```

### **4. PersistentVolume(PV) Î∞∞Ìè¨**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl apply -f fsxL-persistent-volume.yaml

# Í≤∞Í≥º
persistentvolume/fsx-pv created
```

**PersistentVolume ÏÉùÏÑ± ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl get pv
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                      STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
fsx-pv                                     1200Gi     RWX            Retain           Available                                                                                             <unset>                          24s
pvc-47fa9ff1-7e9f-435d-9bfb-513f0df382e9   50Gi       RWO            Delete           Bound       kube-prometheus-stack/data-prometheus-kube-prometheus-stack-prometheus-0   gp3            <unset>                          2d
```

### **5. PersistentVolumeClaim ÌååÏùº ÌôïÏù∏**

```bash
# fsxL-claim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fsx-lustre-claim
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 1200Gi
  volumeName: fsx-pv
```

### **6. PersistentVolumeClaim Î∞∞Ìè¨**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl apply -f fsxL-claim.yaml

# Í≤∞Í≥º
persistentvolumeclaim/fsx-lustre-claim created
```

### **7. PV/PVC Î∞îÏù∏Îî© ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl get pv,pvc
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                                      STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
persistentvolume/fsx-pv                                     1200Gi     RWX            Retain           Bound    default/fsx-lustre-claim                                                                  <unset>                          2m29s
persistentvolume/pvc-47fa9ff1-7e9f-435d-9bfb-513f0df382e9   50Gi       RWO            Delete           Bound    kube-prometheus-stack/data-prometheus-kube-prometheus-stack-prometheus-0   gp3            <unset>                          2d

NAME                                     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/fsx-lustre-claim   Bound    fsx-pv   1200Gi     RWX                           <unset>                 28s
```

---

## **üîç Amazon¬†FSx¬†ÏΩòÏÜîÏóêÏÑú¬†ÏòµÏÖò¬†Î∞è¬†ÏÑ±Îä•¬†ÏÑ∏Î∂ÄÏ†ïÎ≥¥¬†ÌôïÏù∏**

### **1. FSx ÏΩòÏÜî Ïù¥Îèô ÌõÑ, Create file system ÏÑ†ÌÉù**

![](https://velog.velcdn.com/images/tlsalswls123/post/4c78c341-cb6a-4744-aa04-bd67f281cd55/image.png)

### **2. Amazon FSx for Lustre¬†ÏÑ†ÌÉù ÌõÑ,¬†Next¬†ÌÅ¥Î¶≠**

![](https://velog.velcdn.com/images/tlsalswls123/post/7618ca4e-5df7-47c2-b414-1da078b74be3/image.png)

### **3. FSx for Lustre Î∞∞Ìè¨ ÏòµÏÖò ÏÑ∏Î∂ÄÏÇ¨Ìï≠ ÌôïÏù∏**

![](https://velog.velcdn.com/images/tlsalswls123/post/1b7d5c18-e30c-4f1b-8188-07b9804fbdb8/image.png)

### **4. Cancel ÌÅ¥Î¶≠Ìï¥ ÏÑ§Ï†ï ÌôîÎ©¥ Ï¢ÖÎ£å ÌõÑ, FSx Î™©Î°ùÏúºÎ°ú Î≥µÍ∑Ä**

1200GiB FSx for Lustre Ïù∏Ïä§ÌÑ¥Ïä§Í∞Ä Ïù¥ÎØ∏ ÌîÑÎ°úÎπÑÏ†ÄÎãù ÎêòÏñ¥ ÏûàÏùå

![](https://velog.velcdn.com/images/tlsalswls123/post/b4db1027-268d-487f-9244-f178185e00c1/image.png)


### **5. Î™®ÎãàÌÑ∞ÎßÅ Î∞è ÏÑ±Îä• ÌÉ≠ ÌôïÏù∏**

Ïö©Îüâ, Ï≤òÎ¶¨Îüâ, IOPS ÏöîÏïΩ ÏßÄÌëúÎ∂ÄÌÑ∞ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÑ±Îä•, ÎÑ§Ìä∏ÏõåÌÅ¨ ÏßÄÌëú ÌôïÏù∏
![](https://velog.velcdn.com/images/tlsalswls123/post/efaf019d-4342-4c56-a2e2-bec54fd9e521/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/98a0e155-7cd7-48d0-bf23-f4bbbb725899/image.png)

---

## **ü§ñ ÏÉùÏÑ±Ìòï AI Ï±ÑÌåÖ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Î∞∞Ìè¨**

### **1. AWS Inferentia AcceleratorsÏö© Karpenter NodePool Î∞è EC2NodeClass ÏÉùÏÑ±**

**(1) Cloud9 ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨Î°ú Ïù¥Îèô**

```bash
WSParticipantRole:~/environment/eks/FSxL $ cd /home/ec2-user/environment/eks/genai
WSParticipantRole:~/environment/eks/genai $ 
```

**(2) NodePool Ï†ïÏùò ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/genai $ cat inferentia_nodepool.yaml
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: inferentia
  labels:
    intent: genai-apps
    NodeGroupType: inf2-neuron-karpenter
spec:
  template:
    spec:
      taints:
        - key: aws.amazon.com/neuron
          value: "true"
          effect: "NoSchedule"
      requirements:
        - key: "karpenter.k8s.aws/instance-family"
          operator: In
          values: ["inf2"]
        - key: "karpenter.k8s.aws/instance-size"
          operator: In
          values: [ "xlarge", "2xlarge", "8xlarge", "24xlarge", "48xlarge"]
        - key: "kubernetes.io/arch"
          operator: In
          values: ["amd64"]
        - key: "karpenter.sh/capacity-type"
          operator: In
          values: ["spot", "on-demand"]
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: inferentia
  limits:
    cpu: 1000
    memory: 1000Gi
  disruption:
    consolidationPolicy: WhenEmpty
    # expireAfter: 720h # 30 * 24h = 720h
    consolidateAfter: 180s
  weight: 100
---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: inferentia
spec:
  amiFamily: AL2
  amiSelectorTerms:
  - alias: al2@v20240917
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        deleteOnTermination: true
        volumeSize: 100Gi
        volumeType: gp3
  role: "Karpenter-eksworkshop" 
  subnetSelectorTerms:          
    - tags:
        karpenter.sh/discovery: "eksworkshop"
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: "eksworkshop"
  tags:
    intent: apps
    managed-by: karpenter
```

**(3) NodePool Î∞è EC2NodeClass Î∞∞Ìè¨**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl apply -f inferentia_nodepool.yaml

# Í≤∞Í≥º
nodepool.karpenter.sh/inferentia created
ec2nodeclass.karpenter.k8s.aws/inferentia created
```

**(4) ÏÉùÏÑ±Îêú Î¶¨ÏÜåÏä§ ÏÉÅÌÉú ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl get nodepool,ec2nodeclass inferentia
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                               NODECLASS    NODES   READY   AGE
nodepool.karpenter.sh/inferentia   inferentia   0       True    35s

NAME                                        READY   AGE
ec2nodeclass.karpenter.k8s.aws/inferentia   True    35s
```

### **2. Îâ¥Îü∞ ÎîîÎ∞îÏù¥Ïä§ ÌîåÎü¨Í∑∏Ïù∏ Î∞è Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ïπò**

**(1) Îâ¥Îü∞ ÎîîÎ∞îÏù¥Ïä§ ÌîåÎü¨Í∑∏Ïù∏ ÏÑ§Ïπò**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/k8s-neuron-device-plugin-rbac.yml

# Í≤∞Í≥º
Warning: resource clusterroles/neuron-device-plugin is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/neuron-device-plugin configured
Warning: resource serviceaccounts/neuron-device-plugin is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
serviceaccount/neuron-device-plugin configured
Warning: resource clusterrolebindings/neuron-device-plugin is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrolebinding.rbac.authorization.k8s.io/neuron-device-plugin configured
```

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/k8s-neuron-device-plugin.yml

# Í≤∞Í≥º
daemonset.apps/neuron-device-plugin-daemonset created
```

**(2) Îâ¥Îü∞ Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ïπò**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/k8s-neuron-scheduler-eks.yml

# Í≤∞Í≥º
clusterrole.rbac.authorization.k8s.io/k8s-neuron-scheduler created
serviceaccount/k8s-neuron-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/k8s-neuron-scheduler created
Warning: spec.template.metadata.annotations[scheduler.alpha.kubernetes.io/critical-pod]: non-functional in v1.16+; use the "priorityClassName" field instead
deployment.apps/k8s-neuron-scheduler created
service/k8s-neuron-scheduler created
```

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/my-scheduler.yml

# Í≤∞Í≥º
serviceaccount/my-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/my-scheduler-as-kube-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/my-scheduler-as-volume-scheduler created
clusterrole.rbac.authorization.k8s.io/my-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/my-scheduler created
configmap/my-scheduler-config created
deployment.apps/my-scheduler created
```

### **3. vLLM Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÌååÎìú Î∞∞Ìè¨**

**(1) vLLM ÌååÎìú Î∞è ÏÑúÎπÑÏä§ Î∞∞Ìè¨ - ÎåÄÎûµ 7~8Î∂Ñ ÏÜåÏöî**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl apply -f mistral-fsxl.yaml

# Í≤∞Í≥º
deployment.apps/vllm-mistral-inf2-deployment created
service/vllm-mistral7b-service created
```

**(2) `mistral-fsxl.yaml` ÌååÏùº ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/genai $ cat mistral-fsxl.yaml
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-mistral-inf2-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-mistral-inf2-server
  template:
    metadata:
      labels:
        app: vllm-mistral-inf2-server
    spec:
      tolerations:
      - key: "aws.amazon.com/neuron"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: inference-server
        image: public.ecr.aws/u3r1l1j7/eks-genai:neuronrayvllm-100G-root
        resources:
          requests:
            aws.amazon.com/neuron: 1
          limits:
            aws.amazon.com/neuron: 1
        args:
        - --model=$(MODEL_ID)
        - --enforce-eager
        - --gpu-memory-utilization=0.96
        - --device=neuron
        - --max-num-seqs=4
        - --tensor-parallel-size=2
        - --max-model-len=10240
        - --served-model-name=mistralai/Mistral-7B-Instruct-v0.2-neuron
        env:
        - name: MODEL_ID
          value: /work-dir/Mistral-7B-Instruct-v0.2/
        - name: NEURON_COMPILE_CACHE_URL
          value: /work-dir/Mistral-7B-Instruct-v0.2/neuron-cache/
        - name: PORT
          value: "8000"
        volumeMounts:
        - name: persistent-storage
          mountPath: "/work-dir"
      volumes:
      - name: persistent-storage
        persistentVolumeClaim:
          claimName: fsx-lustre-claim
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-mistral7b-service
spec:
  selector:
    app: vllm-mistral-inf2-server
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
```

**(3) Î∞∞Ìè¨ ÏÉÅÌÉú Î™®ÎãàÌÑ∞ÎßÅ**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl get pod
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                                            READY   STATUS              RESTARTS   AGE
kube-ops-view-5d9d967b77-w75cm                  1/1     Running             0          2d
vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb   0/1     ContainerCreating   0          87s
```

### **4. EKS ÏΩòÏÜîÏóêÏÑú Inferentia ÎÖ∏Îìú ÌôïÏù∏**

**(1) eksworkshop ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÑ†ÌÉù**
![](https://velog.velcdn.com/images/tlsalswls123/post/e78216c1-7fb7-4074-aa90-8ad89466122e/image.png)

**(2) Compute ÌÉ≠ÏóêÏÑú `inf2.xlarge`¬†ÎÖ∏Îìú ÌôïÏù∏**
![](https://velog.velcdn.com/images/tlsalswls123/post/31f22ef8-10d5-4f0e-bd86-458a2d81d729/image.png)

**(3) ÎÖ∏Îìú Ïù¥Î¶Ñ¬†ÌÅ¥Î¶≠ ÌõÑ, Ïö©Îüâ Ìï†Îãπ Î∞è Pod ÏÑ∏Î∂Ä Ï†ïÎ≥¥ ÌôïÏù∏**
![](https://velog.velcdn.com/images/tlsalswls123/post/0e0bfd8a-a99e-44cc-97b0-e7f827ef2692/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/d4e54937-2a85-4cef-a31d-dbd6e06e2138/image.png)

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl get pod
NAME                                            READY   STATUS    RESTARTS   AGE
kube-ops-view-5d9d967b77-w75cm                  1/1     Running   0          2d
vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb   1/1     Running   0          8m2s
```

---

## **üí¨¬†Î™®Îç∏Í≥º ÏÉÅÌò∏ ÏûëÏö©ÌïòÍ∏∞ ÏúÑÌïú WebUI Ï±ÑÌåÖ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Î∞∞Ìè¨**

### **1. WebUI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Î∞∞Ìè¨**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl apply -f open-webui.yaml

# Í≤∞Í≥º
deployment.apps/open-webui-deployment created
service/open-webui-service created
ingress.networking.k8s.io/open-webui-ingress created
```

### **2. WebUI Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ URL ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/genai $ kubectl get ing
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                 CLASS   HOSTS   ADDRESS                                                    PORTS   AGE
open-webui-ingress   alb     *       open-webui-ingress-628580148.us-west-2.elb.amazonaws.com   80      32s
```

### **3. WebUI Ï†ëÏÜç**
[open-webui-ingress-628580148.us-west-2.elb.amazonaws.com](http://open-webui-ingress-628580148.us-west-2.elb.amazonaws.com/)

![](https://velog.velcdn.com/images/tlsalswls123/post/75de7ae9-e540-43d9-b0c3-5855319dc971/image.png)

### **4. Î™®Îç∏ ÏÑ†ÌÉù Î∞è Ï±ÑÌåÖ ÏãúÏûë**
![](https://velog.velcdn.com/images/tlsalswls123/post/fbb0c06b-7291-4846-98fb-04a817e4e34c/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/5a94381c-8284-44af-8e4b-19f12cd4b767/image.png)

---

## **Mistral‚Äë7B Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏ Î∞è ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞ ÏûêÏÇ∞ Í≥µÏú† ¬∑ Î≥µÏ†ú**
![](https://velog.velcdn.com/images/tlsalswls123/post/fce67f3e-40b8-4c5c-bef4-0b8cb407ae1e/image.png)

---

## **üîÑ S3ÏôÄ Ïó∞ÎèôÎêú FSx for Lustre Ïù∏Ïä§ÌÑ¥Ïä§Ïóê ÎåÄÌïú ÌÅ¨Î°úÏä§ Î¶¨Ï†Ñ Î≥µÏ†ú ÏÑ§Ï†ïÌïòÍ∏∞**

### **1. Amazon S3 ÏΩòÏÜî Ï†ëÏÜç**

### **2. Ìï¥Îãπ ÏßÄÏó≠Ïóê ÏÉùÏÑ±Îêú S3 Î≤ÑÌÇ∑ ÏÑ†ÌÉù (2ndregionÏù¥ ÏûàÎäî S3 Î≤ÑÌÇ∑ x)**
![](https://velog.velcdn.com/images/tlsalswls123/post/240e3df5-081f-44bb-881a-a93b17fd3edf/image.png)

### **3. Replication rules ÏÉùÏÑ±**

Management ‚Üí Create replication rules ÏÑ†ÌÉù
![](https://velog.velcdn.com/images/tlsalswls123/post/5a912c34-4059-4ef6-b617-cdfbe8a9b696/image.png)

### **4. Enable Bucket Versioning ÏÑ†ÌÉù**
![](https://velog.velcdn.com/images/tlsalswls123/post/d6221817-6bd2-40bb-9cd7-5384057a0448/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/be4cdfd5-f6bf-45b2-a759-dd2abd9d9602/image.png)

### **5. Source bucket ÏÑ§Ï†ï**

Limit the scope of this rule using one or more filters ÏÑ†ÌÉù ÌõÑ, ÌïÑÌÑ∞Í∞íÏúºÎ°ú `test/` ÏûÖÎ†•
![](https://velog.velcdn.com/images/tlsalswls123/post/7fce472a-2024-419d-a0fc-fc2932c1cb16/image.png)

### **6. Destination**

Browse S3 ‚Üí `fsx-lustre-bucket-2ndregion-xxxx` ‚Üí Choose path ÏÑ†ÌÉù
![](https://velog.velcdn.com/images/tlsalswls123/post/684fb119-18b2-4592-8e67-45a2de320828/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/a912491e-1df7-4965-be56-8097aecf8795/image.png)

Enable Versioning ÏÑ†ÌÉù
![](https://velog.velcdn.com/images/tlsalswls123/post/1611e1f8-080c-4ad2-a9d2-3153bffc4222/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/170dc6cf-fe05-4407-b8e0-064785fa68f9/image.png)

### **7. IAM Ïó≠Ìï† ÏßÄÏ†ï**

s3-cross-region-replication-role* ÏÑ†ÌÉù
![](https://velog.velcdn.com/images/tlsalswls123/post/236bee7c-4bc0-4732-93f9-f0871eee74e8/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/dd5f01e6-0e32-4d75-a805-d47f73a49fd4/image.png)

### **8. Encryption**

Choose from your AWS KSM keys
![](https://velog.velcdn.com/images/tlsalswls123/post/150b3db3-3eb6-431f-b088-a7fd230070aa/image.png)

### **9. Save**

Replicate existing objects? ‚Üí No, do not replicate existing objects ‚Üí Submit
![](https://velog.velcdn.com/images/tlsalswls123/post/b75adf02-fbc7-44bc-a964-0b9e930bda24/image.png)

### **10. Replication rules ÏÉùÏÑ± ÌôïÏù∏**
![](https://velog.velcdn.com/images/tlsalswls123/post/ce14d8b2-279a-4ab8-a04b-d3952f187673/image.png)

---

## **üß™ Mistral-7B Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÇ¨ Î∞è ÌÖåÏä§Ìä∏ ÌååÏùº ÏÉùÏÑ±**

### **1. ÏûëÏóÖ ÎîîÎ†âÌÑ∞Î¶¨ Ïù¥Îèô**

```bash
WSParticipantRole:~/environment/eks/genai $ cd /home/ec2-user/environment/eks/FSxL
WSParticipantRole:~/environment/eks/FSxL $ 
```

### **2. ÌååÎìú Î™©Î°ù Ï°∞Ìöå**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl get pods
NAME                                            READY   STATUS    RESTARTS   AGE
kube-ops-view-5d9d967b77-w75cm                  1/1     Running   0          2d1h
open-webui-deployment-5d7ff94bc9-wfv9v          1/1     Running   0          26m
vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb   1/1     Running   0          35m
```

`vllm-`ÏúºÎ°ú ÏãúÏûëÌïòÎäî ÌååÎìú Ïù¥Î¶Ñ Î≥µÏÇ¨

```bash
vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb
```

### **3. vLLM ÌååÎìú Î°úÍ∑∏Ïù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl exec -it vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb -- bash
bash: /home/ray/anaconda3/lib/libtinfo.so.6: no version information available (required by bash)
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:~# 
```

### **4. PV ÎßàÏö¥Ìä∏ ÏúÑÏπò ÌôïÏù∏**

```bash
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:~# df -h
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
Filesystem                 Size  Used Avail Use% Mounted on
overlay                    100G   23G   78G  23% /
tmpfs                       64M     0   64M   0% /dev
tmpfs                      7.7G     0  7.7G   0% /sys/fs/cgroup
10.0.34.188@tcp:/u42dbb4v  1.2T   28G  1.1T   3% /work-dir
/dev/nvme0n1p1             100G   23G   78G  23% /etc/hosts
shm                         64M     0   64M   0% /dev/shm
tmpfs                       15G   12K   15G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                      7.7G     0  7.7G   0% /proc/acpi
tmpfs                      7.7G     0  7.7G   0% /sys/firmware
```

- `work¬†-dir`ÏùÄ  FSx for Lustre Í∏∞Î∞ò PVÏûÑ

### **5. PVÏóê Ï†ÄÏû•Îêú ÎÇ¥Ïö© ÌôïÏù∏**

```bash
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:~# cd /work-dir/
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir# ls -ll
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
total 297
drwxr-xr-x 5 root root  33280 Apr 16 19:30 Mistral-7B-Instruct-v0.2
-rw-r--r-- 1 root root 151289 Apr 16 19:32 sysprep
```

Mistral-7B Î™®Îç∏ Ìè¥Îçî ÌôïÏù∏

```bash
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir# cd Mistral-7B-Instruct-v0.2/
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir/Mistral-7B-Instruct-v0.2# ls -ll
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
total 2550
drwxr-xr-x 3 root root   33280 Apr 16 19:30 -split
-rwxr-xr-x 1 root root    5471 Apr 16 19:05 README.md
-rwxr-xr-x 1 root root     596 Apr 16 19:05 config.json
-rwxr-xr-x 1 root root     111 Apr 16 19:05 generation_config.json
-rwxr-xr-x 1 root root   25125 Apr 16 19:05 model.safetensors.index.json
drwxr-xr-x 3 root root   33280 Apr 16 19:30 neuron-cache
-rwxr-xr-x 1 root root   23950 Apr 16 19:05 pytorch_model.bin.index.json
-rwxr-xr-x 1 root root     414 Apr 16 19:05 special_tokens_map.json
-rwxr-xr-x 1 root root 1795188 Apr 16 19:05 tokenizer.json
-rwxr-xr-x 1 root root  493443 Apr 16 19:05 tokenizer.model
-rwxr-xr-x 1 root root    2103 Apr 16 19:05 tokenizer_config.json
```

### **6. ÌÖåÏä§Ìä∏ Ìè¥Îçî Î∞è ÌååÏùº ÏÉùÏÑ±**

```bash
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir/Mistral-7B-Instruct-v0.2# cd /work-dir
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir# mkdir test
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir# cd test
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir/test# cp /work-dir/Mistral-7B-Instruct-v0.2/README.md /work-dir/test/testfile
(base) root@vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb:/work-dir/test# ls -ll /work-dir/test
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
total 1
-rwxr-xr-x 1 root root 5471 Apr 18 20:33 testfile
```

### **9. S3 Î≤ÑÌÇ∑ Î≥µÏ†ú Í≤∞Í≥º ÌôïÏù∏**

**(1) S3 ÏΩòÏÜî Ï†ëÏÜç**
![](https://velog.velcdn.com/images/tlsalswls123/post/301b770e-4420-4e1e-8eab-f17d87106b1c/image.png)

**(2) ÏõêÎ≥∏ Î≤ÑÌÇ∑ÏóêÏÑú Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏**
![](https://velog.velcdn.com/images/tlsalswls123/post/69661e63-c870-407e-88ab-ed6ee2ba1f8c/image.png)

**(3) ÎåÄÏÉÅ Î≤ÑÌÇ∑(2ndregion)ÏóêÏÑú Î≥µÏ†úÎêú Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏**
![](https://velog.velcdn.com/images/tlsalswls123/post/763c905d-3e2a-4873-ab47-f3c00113810f/image.png)

---

## **‚öôÔ∏è¬†Îç∞Ïù¥ÌÑ∞ Í≥ÑÏ∏µ ÌÖåÏä§Ìä∏Î•º ÏúÑÌïú ÏûêÏ≤¥ ÌôòÍ≤Ω ÏÉùÏÑ±**

### **1. ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï**

```bash
WSParticipantRole:~/environment/eks/FSxL $ VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.resourcesVpcConfig.vpcId" --output text)
WSParticipantRole:~/environment/eks/FSxL $ SUBNET_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.resourcesVpcConfig.subnetIds[0]" --output text)
WSParticipantRole:~/environment/eks/FSxL $ SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=${VPC_ID} Name=group-name,Values="FSxLSecurityGroup01"  --query "SecurityGroups[*].GroupId" --output text) 
```

```bash
WSParticipantRole:~/environment/eks/FSxL $ echo $SUBNET_ID
subnet-047814202808347bc
WSParticipantRole:~/environment/eks/FSxL $ echo $SECURITY_GROUP_ID
sg-00e9611cf170ffb25
```

### **2. ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ Ïù¥Îèô**

```bash
WSParticipantRole:~/environment/eks/FSxL $ cd /home/ec2-user/environment/eks/FSxL
WSParticipantRole:~/environment/eks/FSxL $ 
```

### **3. StorageClass ÏÉùÏÑ±**

**(1) `fsxL-storage-class.yaml` ÌôïÏù∏**

```bash
# fsxL-storage-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
    name: fsx-lustre-sc
provisioner: fsx.csi.aws.com
parameters:
  subnetId: SUBNET_ID
  securityGroupIds: SECURITY_GROUP_ID
  deploymentType: SCRATCH_2
  fileSystemTypeVersion: "2.15"
mountOptions:
  - flock
```

**(2) `SUBNET_ID`, `SECURITY_GROUP_ID` ÏπòÌôò**

```bash
WSParticipantRole:~/environment/eks/FSxL $ sed -i'' -e "s/SUBNET_ID/$SUBNET_ID/g" fsxL-storage-class.yaml
WSParticipantRole:~/environment/eks/FSxL $ sed -i'' -e "s/SECURITY_GROUP_ID/$SECURITY_GROUP_ID/g" fsxL-storage-class.yaml
```

**(3) `fsxL-storage-class.yaml` Ïû¨ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ cat fsxL-storage-class.yaml
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
    name: fsx-lustre-sc
provisioner: fsx.csi.aws.com
parameters:
  subnetId: subnet-047814202808347bc
  securityGroupIds: sg-00e9611cf170ffb25
  deploymentType: SCRATCH_2
  fileSystemTypeVersion: "2.15"
mountOptions:
  - flock
```

**(4) StorageClass Î∞∞Ìè¨**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl apply -f fsxL-storage-class.yaml

# Í≤∞Í≥º
storageclass.storage.k8s.io/fsx-lustre-sc created
```

**(5) StorageClass ÏÉùÏÑ± ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl get sc
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
fsx-lustre-sc   fsx.csi.aws.com         Delete          Immediate              false                  28s
gp2             kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  2d1h
gp3 (default)   ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   2d1h
```

### **5. PVC ÏÉùÏÑ±**

**(1) `fsxL-dynamic-claim.yaml` ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ cat fsxL-dynamic-claim.yaml
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fsx-lustre-dynamic-claim
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: fsx-lustre-sc
  resources:
    requests:
      storage: 1200Gi
```

**(2) PersistentVolumeClaim(PVC) ÏÉùÏÑ±**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl apply -f fsxL-dynamic-claim.yaml

# Í≤∞Í≥º
persistentvolumeclaim/fsx-lustre-dynamic-claim created
```

**(3) PVC ÏÉÅÌÉú ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl describe pvc/fsx-lustre-dynamic-claim
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
Name:          fsx-lustre-dynamic-claim
Namespace:     default
StorageClass:  fsx-lustre-sc
Status:        Pending
Volume:        
Labels:        <none>
Annotations:   volume.beta.kubernetes.io/storage-provisioner: fsx.csi.aws.com
               volume.kubernetes.io/storage-provisioner: fsx.csi.aws.com
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      
Access Modes:  
VolumeMode:    Filesystem
Used By:       <none>
Events:
  Type    Reason                Age                From                                                                                      Message
  ----    ------                ----               ----                                                                                      -------
  Normal  Provisioning          28s                fsx.csi.aws.com_fsx-csi-controller-6f4c577bd4-4wxrs_3244bfde-da2e-414b-913b-9a55b306d020  External provisioner is provisioning volume for claim "default/fsx-lustre-dynamic-claim"
  Normal  ExternalProvisioning  11s (x4 over 28s)  persistentvolume-controller                                                               Waiting for a volume to be created either by the external provisioner 'fsx.csi.aws.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
```

Î∞îÏù∏Îî© ÎåÄÍ∏∞ ÏÉÅÌÉú ÌôïÏù∏

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl get pvc
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                       STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    VOLUMEATTRIBUTESCLASS   AGE
fsx-lustre-claim           Bound     fsx-pv   1200Gi     RWX                            <unset>                 73m
fsx-lustre-dynamic-claim   Pending                                      fsx-lustre-sc   <unset>                 77s
```

### **6. PVC Î∞îÏù∏Îî© ÏôÑÎ£å ÌôïÏù∏**

ÏïΩ 15Î∂Ñ Ï†ïÎèÑ Í∏∞Îã§Î¶∞ ÌõÑ, PVC ÏÉÅÌÉú Bound ÌôïÏù∏

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl get pvc
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    VOLUMEATTRIBUTESCLASS   AGE
fsx-lustre-claim           Bound    fsx-pv                                     1200Gi     RWX                            <unset>                 96m
fsx-lustre-dynamic-claim   Bound    pvc-1c121fa2-7362-4c0b-a2bf-dfe4d0a24b11   1200Gi     RWX            fsx-lustre-sc   <unset>                 24m
```

FSx ÏΩòÏÜîÏóêÏÑú Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉÅÌÉú ÌôïÏù∏
![](https://velog.velcdn.com/images/tlsalswls123/post/3644e64b-fa73-4959-be31-f44d3f2c21a8/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/6bdfe8fd-d25b-4133-b1ca-0ce4927101dd/image.png)

---

## **üìà ÏÑ±Îä• ÌÖåÏä§Ìä∏**

### **1. ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ Ïù¥Îèô**

```bash
WSParticipantRole:~/environment/eks/FSxL $ cd /home/ec2-user/environment/eks/FSxL
WSParticipantRole:~/environment/eks/FSxL $ 
```

### **2. Í∞ÄÏö©ÏÑ± ÏòÅÏó≠ ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ aws ec2 describe-subnets --subnet-id $SUBNET_ID --region $AWS_REGION | jq .Subnets[0].AvailabilityZone
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
"us-west-2b"
```

### **3. ÌååÎìú Íµ¨ÏÑ± ÏàòÏ†ï**
![](https://velog.velcdn.com/images/tlsalswls123/post/d90e53c1-9f9c-496e-b930-b047da494308/image.png)
![](https://velog.velcdn.com/images/tlsalswls123/post/d4b76ce2-8c4e-4f7e-8d7e-4669a8af04ca/image.png)

- `nodeSelector`,¬†`topology.kubernetes.io/zone`¬†Ï£ºÏÑù Ï≤òÎ¶¨ Ï†úÍ±∞
- Í∞ÄÏö©ÏÑ± ÏòÅÏó≠ Î≥ÄÍ≤Ω (`us-west-2b`)

### **4. ÌååÎìú ÏÉùÏÑ±**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl apply -f pod_performance.yaml

# Í≤∞Í≥º
pod/fsxl-performance created
```

### **5. ÌååÎìú ÏÉÅÌÉú ÌôïÏù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl get pods
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
NAME                                            READY   STATUS    RESTARTS   AGE
fsxl-performance                                1/1     Running   0          12s
kube-ops-view-5d9d967b77-w75cm                  1/1     Running   0          2d1h
open-webui-deployment-5d7ff94bc9-wfv9v          1/1     Running   0          75m
vllm-mistral-inf2-deployment-7d886c8cc8-n6sbb   1/1     Running   0          84m
```

### **6. Ïª®ÌÖåÏù¥ÎÑà Î°úÍ∑∏Ïù∏**

```bash
WSParticipantRole:~/environment/eks/FSxL $ kubectl exec -it fsxl-performance  -- bash

root@fsxl-performance:/# 
```

### **7. FIO Î∞è IOping ÏÑ§Ïπò**

```bash
root@fsxl-performance:/# apt-get update

# Í≤∞Í≥º
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
Get:3 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
Get:4 http://deb.debian.org/debian bookworm/main amd64 Packages [8792 kB]
Get:5 http://deb.debian.org/debian bookworm-updates/main amd64 Packages [512 B]
Get:6 http://deb.debian.org/debian-security bookworm-security/main amd64 Packages [255 kB]
Fetched 9303 kB in 1s (6521 kB/s)                         
Reading package lists... Done
```

```bash
root@fsxl-performance:/# apt-get install fio ioping -y

# Í≤∞Í≥º
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
...
Processing triggers for libc-bin (2.36-9+deb12u10) ...
```

### **8. ÏßÄÏó∞ ÏãúÍ∞Ñ ÌÖåÏä§Ìä∏ (IOping)**

```bash
root@fsxl-performance:/# ioping -c 20 .
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
4 KiB <<< . (overlay overlay 19.9 GiB): request=1 time=459.7 us (warmup)
4 KiB <<< . (overlay overlay 19.9 GiB): request=2 time=1.07 ms
4 KiB <<< . (overlay overlay 19.9 GiB): request=3 time=693.1 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=4 time=720.3 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=5 time=634.4 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=6 time=661.2 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=7 time=581.8 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=8 time=742.6 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=9 time=647.6 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=10 time=730.4 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=11 time=809.5 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=12 time=713.6 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=13 time=604.9 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=14 time=1.34 ms
4 KiB <<< . (overlay overlay 19.9 GiB): request=15 time=700.8 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=16 time=591.3 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=17 time=698.9 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=18 time=928.0 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=19 time=638.9 us
4 KiB <<< . (overlay overlay 19.9 GiB): request=20 time=632.7 us

--- . (overlay overlay 19.9 GiB) ioping statistics ---
19 requests completed in 14.1 ms, 76 KiB read, 1.34 k iops, 5.25 MiB/s
generated 20 requests in 19.0 s, 80 KiB, 1 iops, 4.21 KiB/s
min/avg/max/mdev = 581.8 us / 744.1 us / 1.34 ms / 181.8 us
```

### **9. Î∂ÄÌïò ÌÖåÏä§Ìä∏**

```bash
root@fsxl-performance:/# mkdir -p /data/performance
root@fsxl-performance:/# cd /data/performance
root@fsxl-performance:/data/performance# 
```

```bash
root@fsxl-performance:/data/performance# fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio8gb --bs=1MB --iodepth=64 --size=8G --readwrite=randrw --rwmixread=50 --numjobs=8 --group_reporting --runtime=10
```

‚úÖ¬†**Ï∂úÎ†•**

```bash
fiotest: (g=0): rw=randrw, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=libaio, iodepth=64
...
fio-3.33
Starting 8 processes
fiotest: Laying out IO file (1 file / 8192MiB)
Jobs: 6 (f=5): [f(1),_(1),m(3),_(1),m(2)][38.9%][r=89.0MiB/s,w=100MiB/s][r=89,w=100 IOPS][eta 00m:22s]
fiotest: (groupid=0, jobs=8): err= 0: pid=723: Sat Apr 19 04:24:09 2025
  read: IOPS=96, BW=96.4MiB/s (101MB/s)(1394MiB/14460msec)
   bw (  KiB/s): min=20461, max=390963, per=100.00%, avg=120798.95, stdev=11723.27, samples=156
   iops        : min=   17, max=  380, avg=116.50, stdev=11.43, samples=156
  write: IOPS=97, BW=97.2MiB/s (102MB/s)(1406MiB/14460msec); 0 zone resets
   bw (  KiB/s): min=18403, max=391048, per=100.00%, avg=116698.51, stdev=11301.16, samples=161
   iops        : min=   13, max=  380, avg=112.42, stdev=11.04, samples=161
  cpu          : usr=0.09%, sys=0.76%, ctx=5389, majf=0, minf=53
  IO depths    : 1=0.3%, 2=0.6%, 4=1.1%, 8=2.3%, 16=4.6%, 32=9.1%, >=64=82.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=99.7%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.3%, >=64=0.0%
     issued rwts: total=1394,1406,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=96.4MiB/s (101MB/s), 96.4MiB/s-96.4MiB/s (101MB/s-101MB/s), io=1394MiB (1462MB), run=14460-14460msec
  WRITE: bw=97.2MiB/s (102MB/s), 97.2MiB/s-97.2MiB/s (102MB/s-102MB/s), io=1406MiB (1474MB), run=14460-14460msec
```

### **10. ÌÖåÏä§Ìä∏ Ï¢ÖÎ£å**

```bash
root@fsxl-performance:/data/performance# exit
exit
```

